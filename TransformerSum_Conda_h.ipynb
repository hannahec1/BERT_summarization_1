{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TransformerSum_Conda_h.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vuHv1Lhfx548",
        "WMiKwUS9CuDR",
        "JGTxMW5Sy8NU",
        "vNpLZuJgZXg1",
        "6_IFO9hvRPWo",
        "Oz6oDXN7fp89",
        "y8lT3M0Zjuew",
        "Q2bYxC7Sl4Xo",
        "m_HZ3oF_nnHe",
        "BgydICM3o9Pq",
        "Q0RQaEud7aR8",
        "8vA0utyj9dfn",
        "33hYe2NYQNsY",
        "NEF6cn8QquVY"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannahec1/BERT_summarization_1/blob/master/TransformerSum_Conda_h.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfU65CW0UCpL",
        "outputId": "0c93b43a-e78d-4071-b387-96c0e38f9226"
      },
      "source": [
        "!echo $PYTHONPATH"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/env/python\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HhGy7jVdH7s",
        "outputId": "81552d8f-d4b5-4a63-ead7-730569a69432"
      },
      "source": [
        "!pip install rouge\n",
        "!pip install summa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n",
            "Collecting summa\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/3b/1c7dc435d05aef474c4137328400f1e11787b9bffab1f87a3f160c1fef54/summa-1.2.0.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.19.5)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-cp37-none-any.whl size=54411 sha256=dc9563eb193fa9bee59df0ee25b9fe457d92eee330c5f27cfe431e5efe89383b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/09/68/e2f2861c01d86407c3fa5220826ed7eed2abaa56b001be5970\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzVp54DNUG1f",
        "outputId": "efa366af-d316-4591-da07-fa5339d696ee"
      },
      "source": [
        "%env PYTHONPATH="
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: PYTHONPATH=\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIpMD3RBUMqB",
        "outputId": "168c5eae-0ba0-4be3-a396-812a0aae1135"
      },
      "source": [
        "\n",
        "%%bash\n",
        "\n",
        "MINICONDA_INSTALLER_SCRIPT=Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "MINICONDA_PREFIX=/usr/local\n",
        "wget https://repo.continuum.io/miniconda/$MINICONDA_INSTALLER_SCRIPT\n",
        "chmod +x $MINICONDA_INSTALLER_SCRIPT\n",
        "./$MINICONDA_INSTALLER_SCRIPT -b -f -p $MINICONDA_PREFIX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREFIX=/usr/local\n",
            "installing: python-3.6.5-hc3d631a_2 ...\n",
            "installing: ca-certificates-2018.03.07-0 ...\n",
            "installing: conda-env-2.6.0-h36134e3_1 ...\n",
            "installing: libgcc-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libstdcxx-ng-7.2.0-hdf63c60_3 ...\n",
            "installing: libffi-3.2.1-hd88cf55_4 ...\n",
            "installing: ncurses-6.1-hf484d3e_0 ...\n",
            "installing: openssl-1.0.2o-h20670df_0 ...\n",
            "installing: tk-8.6.7-hc745277_3 ...\n",
            "installing: xz-5.2.4-h14c3975_4 ...\n",
            "installing: yaml-0.1.7-had09818_2 ...\n",
            "installing: zlib-1.2.11-ha838bed_2 ...\n",
            "installing: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "installing: readline-7.0-ha6073c6_4 ...\n",
            "installing: sqlite-3.23.1-he433501_0 ...\n",
            "installing: asn1crypto-0.24.0-py36_0 ...\n",
            "installing: certifi-2018.4.16-py36_0 ...\n",
            "installing: chardet-3.0.4-py36h0f667ec_1 ...\n",
            "installing: idna-2.6-py36h82fb2a8_1 ...\n",
            "installing: pycosat-0.6.3-py36h0a5515d_0 ...\n",
            "installing: pycparser-2.18-py36hf9f622e_1 ...\n",
            "installing: pysocks-1.6.8-py36_0 ...\n",
            "installing: ruamel_yaml-0.15.37-py36h14c3975_2 ...\n",
            "installing: six-1.11.0-py36h372c433_1 ...\n",
            "installing: cffi-1.11.5-py36h9745a5d_0 ...\n",
            "installing: setuptools-39.2.0-py36_0 ...\n",
            "installing: cryptography-2.2.2-py36h14c3975_0 ...\n",
            "installing: wheel-0.31.1-py36_0 ...\n",
            "installing: pip-10.0.1-py36_0 ...\n",
            "installing: pyopenssl-18.0.0-py36_0 ...\n",
            "installing: urllib3-1.22-py36hbe7ace6_0 ...\n",
            "installing: requests-2.18.4-py36he2e5f8d_1 ...\n",
            "installing: conda-4.5.4-py36_0 ...\n",
            "installation finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "--2021-03-11 11:27:02--  https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.200.79, 104.18.201.79, 2606:4700::6812:c94f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.200.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh [following]\n",
            "--2021-03-11 11:27:02--  https://repo.anaconda.com/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58468498 (56M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-4.5.4-Linux-x86_64.sh’\n",
            "\n",
            "     0K .......... .......... .......... .......... ..........  0% 4.10M 14s\n",
            "    50K .......... .......... .......... .......... ..........  0% 9.61M 10s\n",
            "   100K .......... .......... .......... .......... ..........  0% 4.47M 11s\n",
            "   150K .......... .......... .......... .......... ..........  0% 11.4M 9s\n",
            "   200K .......... .......... .......... .......... ..........  0% 11.3M 8s\n",
            "   250K .......... .......... .......... .......... ..........  0% 15.0M 8s\n",
            "   300K .......... .......... .......... .......... ..........  0% 20.2M 7s\n",
            "   350K .......... .......... .......... .......... ..........  0% 13.8M 6s\n",
            "   400K .......... .......... .......... .......... ..........  0% 19.4M 6s\n",
            "   450K .......... .......... .......... .......... ..........  0% 23.2M 6s\n",
            "   500K .......... .......... .......... .......... ..........  0% 23.5M 5s\n",
            "   550K .......... .......... .......... .......... ..........  1% 49.4M 5s\n",
            "   600K .......... .......... .......... .......... ..........  1% 54.9M 5s\n",
            "   650K .......... .......... .......... .......... ..........  1% 27.5M 5s\n",
            "   700K .......... .......... .......... .......... ..........  1% 33.2M 4s\n",
            "   750K .......... .......... .......... .......... ..........  1% 30.5M 4s\n",
            "   800K .......... .......... .......... .......... ..........  1% 51.0M 4s\n",
            "   850K .......... .......... .......... .......... ..........  1% 64.7M 4s\n",
            "   900K .......... .......... .......... .......... ..........  1% 37.1M 4s\n",
            "   950K .......... .......... .......... .......... ..........  1% 49.3M 4s\n",
            "  1000K .......... .......... .......... .......... ..........  1% 38.4M 3s\n",
            "  1050K .......... .......... .......... .......... ..........  1%  183M 3s\n",
            "  1100K .......... .......... .......... .......... ..........  2% 43.7M 3s\n",
            "  1150K .......... .......... .......... .......... ..........  2% 51.0M 3s\n",
            "  1200K .......... .......... .......... .......... ..........  2% 51.2M 3s\n",
            "  1250K .......... .......... .......... .......... ..........  2% 58.1M 3s\n",
            "  1300K .......... .......... .......... .......... ..........  2%  191M 3s\n",
            "  1350K .......... .......... .......... .......... ..........  2% 51.9M 3s\n",
            "  1400K .......... .......... .......... .......... ..........  2% 68.6M 3s\n",
            "  1450K .......... .......... .......... .......... ..........  2% 71.2M 3s\n",
            "  1500K .......... .......... .......... .......... ..........  2%  190M 3s\n",
            "  1550K .......... .......... .......... .......... ..........  2% 65.6M 3s\n",
            "  1600K .......... .......... .......... .......... ..........  2% 77.8M 2s\n",
            "  1650K .......... .......... .......... .......... ..........  2% 72.2M 2s\n",
            "  1700K .......... .......... .......... .......... ..........  3% 46.4M 2s\n",
            "  1750K .......... .......... .......... .......... ..........  3%  107M 2s\n",
            "  1800K .......... .......... .......... .......... ..........  3%  125M 2s\n",
            "  1850K .......... .......... .......... .......... ..........  3% 65.6M 2s\n",
            "  1900K .......... .......... .......... .......... ..........  3%  157M 2s\n",
            "  1950K .......... .......... .......... .......... ..........  3%  116M 2s\n",
            "  2000K .......... .......... .......... .......... ..........  3%  101M 2s\n",
            "  2050K .......... .......... .......... .......... ..........  3%  104M 2s\n",
            "  2100K .......... .......... .......... .......... ..........  3%  117M 2s\n",
            "  2150K .......... .......... .......... .......... ..........  3%  167M 2s\n",
            "  2200K .......... .......... .......... .......... ..........  3%  122M 2s\n",
            "  2250K .......... .......... .......... .......... ..........  4%  108M 2s\n",
            "  2300K .......... .......... .......... .......... ..........  4%  121M 2s\n",
            "  2350K .......... .......... .......... .......... ..........  4%  100M 2s\n",
            "  2400K .......... .......... .......... .......... ..........  4%  160M 2s\n",
            "  2450K .......... .......... .......... .......... ..........  4%  135M 2s\n",
            "  2500K .......... .......... .......... .......... ..........  4%  134M 2s\n",
            "  2550K .......... .......... .......... .......... ..........  4%  116M 2s\n",
            "  2600K .......... .......... .......... .......... ..........  4%  101M 2s\n",
            "  2650K .......... .......... .......... .......... ..........  4%  127M 2s\n",
            "  2700K .......... .......... .......... .......... ..........  4%  142M 2s\n",
            "  2750K .......... .......... .......... .......... ..........  4%  107M 2s\n",
            "  2800K .......... .......... .......... .......... ..........  4%  139M 2s\n",
            "  2850K .......... .......... .......... .......... ..........  5%  133M 2s\n",
            "  2900K .......... .......... .......... .......... ..........  5%  132M 2s\n",
            "  2950K .......... .......... .......... .......... ..........  5%  130M 2s\n",
            "  3000K .......... .......... .......... .......... ..........  5%  128M 2s\n",
            "  3050K .......... .......... .......... .......... ..........  5%  179M 1s\n",
            "  3100K .......... .......... .......... .......... ..........  5% 10.9M 2s\n",
            "  3150K .......... .......... .......... .......... ..........  5% 99.2M 2s\n",
            "  3200K .......... .......... .......... .......... ..........  5%  121M 2s\n",
            "  3250K .......... .......... .......... .......... ..........  5%  121M 1s\n",
            "  3300K .......... .......... .......... .......... ..........  5%  137M 1s\n",
            "  3350K .......... .......... .......... .......... ..........  5%  120M 1s\n",
            "  3400K .......... .......... .......... .......... ..........  6%  125M 1s\n",
            "  3450K .......... .......... .......... .......... ..........  6%  145M 1s\n",
            "  3500K .......... .......... .......... .......... ..........  6%  110M 1s\n",
            "  3550K .......... .......... .......... .......... ..........  6%  102M 1s\n",
            "  3600K .......... .......... .......... .......... ..........  6%  113M 1s\n",
            "  3650K .......... .......... .......... .......... ..........  6%  116M 1s\n",
            "  3700K .......... .......... .......... .......... ..........  6%  134M 1s\n",
            "  3750K .......... .......... .......... .......... ..........  6%  100M 1s\n",
            "  3800K .......... .......... .......... .......... ..........  6%  135M 1s\n",
            "  3850K .......... .......... .......... .......... ..........  6%  121M 1s\n",
            "  3900K .......... .......... .......... .......... ..........  6% 78.7M 1s\n",
            "  3950K .......... .......... .......... .......... ..........  7% 88.2M 1s\n",
            "  4000K .......... .......... .......... .......... ..........  7%  131M 1s\n",
            "  4050K .......... .......... .......... .......... ..........  7% 97.2M 1s\n",
            "  4100K .......... .......... .......... .......... ..........  7%  140M 1s\n",
            "  4150K .......... .......... .......... .......... ..........  7%  136M 1s\n",
            "  4200K .......... .......... .......... .......... ..........  7%  128M 1s\n",
            "  4250K .......... .......... .......... .......... ..........  7%  117M 1s\n",
            "  4300K .......... .......... .......... .......... ..........  7%  163M 1s\n",
            "  4350K .......... .......... .......... .......... ..........  7%  125M 1s\n",
            "  4400K .......... .......... .......... .......... ..........  7%  171M 1s\n",
            "  4450K .......... .......... .......... .......... ..........  7%  100M 1s\n",
            "  4500K .......... .......... .......... .......... ..........  7%  131M 1s\n",
            "  4550K .......... .......... .......... .......... ..........  8%  155M 1s\n",
            "  4600K .......... .......... .......... .......... ..........  8%  166M 1s\n",
            "  4650K .......... .......... .......... .......... ..........  8%  155M 1s\n",
            "  4700K .......... .......... .......... .......... ..........  8%  122M 1s\n",
            "  4750K .......... .......... .......... .......... ..........  8% 80.7M 1s\n",
            "  4800K .......... .......... .......... .......... ..........  8%  146M 1s\n",
            "  4850K .......... .......... .......... .......... ..........  8%  149M 1s\n",
            "  4900K .......... .......... .......... .......... ..........  8%  121M 1s\n",
            "  4950K .......... .......... .......... .......... ..........  8% 88.0M 1s\n",
            "  5000K .......... .......... .......... .......... ..........  8%  174M 1s\n",
            "  5050K .......... .......... .......... .......... ..........  8%  143M 1s\n",
            "  5100K .......... .......... .......... .......... ..........  9% 98.1M 1s\n",
            "  5150K .......... .......... .......... .......... ..........  9%  102M 1s\n",
            "  5200K .......... .......... .......... .......... ..........  9%  108M 1s\n",
            "  5250K .......... .......... .......... .......... ..........  9%  146M 1s\n",
            "  5300K .......... .......... .......... .......... ..........  9% 89.8M 1s\n",
            "  5350K .......... .......... .......... .......... ..........  9% 83.3M 1s\n",
            "  5400K .......... .......... .......... .......... ..........  9%  146M 1s\n",
            "  5450K .......... .......... .......... .......... ..........  9%  151M 1s\n",
            "  5500K .......... .......... .......... .......... ..........  9%  165M 1s\n",
            "  5550K .......... .......... .......... .......... ..........  9% 82.2M 1s\n",
            "  5600K .......... .......... .......... .......... ..........  9%  140M 1s\n",
            "  5650K .......... .......... .......... .......... ..........  9%  101M 1s\n",
            "  5700K .......... .......... .......... .......... .......... 10%  161M 1s\n",
            "  5750K .......... .......... .......... .......... .......... 10%  158M 1s\n",
            "  5800K .......... .......... .......... .......... .......... 10%  175M 1s\n",
            "  5850K .......... .......... .......... .......... .......... 10%  140M 1s\n",
            "  5900K .......... .......... .......... .......... .......... 10%  175M 1s\n",
            "  5950K .......... .......... .......... .......... .......... 10%  129M 1s\n",
            "  6000K .......... .......... .......... .......... .......... 10%  130M 1s\n",
            "  6050K .......... .......... .......... .......... .......... 10%  182M 1s\n",
            "  6100K .......... .......... .......... .......... .......... 10%  171M 1s\n",
            "  6150K .......... .......... .......... .......... .......... 10%  104M 1s\n",
            "  6200K .......... .......... .......... .......... .......... 10%  177M 1s\n",
            "  6250K .......... .......... .......... .......... .......... 11%  142M 1s\n",
            "  6300K .......... .......... .......... .......... .......... 11%  137M 1s\n",
            "  6350K .......... .......... .......... .......... .......... 11% 92.8M 1s\n",
            "  6400K .......... .......... .......... .......... .......... 11%  147M 1s\n",
            "  6450K .......... .......... .......... .......... .......... 11%  163M 1s\n",
            "  6500K .......... .......... .......... .......... .......... 11%  135M 1s\n",
            "  6550K .......... .......... .......... .......... .......... 11%  119M 1s\n",
            "  6600K .......... .......... .......... .......... .......... 11%  206M 1s\n",
            "  6650K .......... .......... .......... .......... .......... 11%  345M 1s\n",
            "  6700K .......... .......... .......... .......... .......... 11%  334M 1s\n",
            "  6750K .......... .......... .......... .......... .......... 11%  297M 1s\n",
            "  6800K .......... .......... .......... .......... .......... 11%  338M 1s\n",
            "  6850K .......... .......... .......... .......... .......... 12%  168M 1s\n",
            "  6900K .......... .......... .......... .......... .......... 12%  124M 1s\n",
            "  6950K .......... .......... .......... .......... .......... 12% 85.8M 1s\n",
            "  7000K .......... .......... .......... .......... .......... 12%  172M 1s\n",
            "  7050K .......... .......... .......... .......... .......... 12%  172M 1s\n",
            "  7100K .......... .......... .......... .......... .......... 12%  134M 1s\n",
            "  7150K .......... .......... .......... .......... .......... 12% 69.0M 1s\n",
            "  7200K .......... .......... .......... .......... .......... 12%  140M 1s\n",
            "  7250K .......... .......... .......... .......... .......... 12%  187M 1s\n",
            "  7300K .......... .......... .......... .......... .......... 12%  224M 1s\n",
            "  7350K .......... .......... .......... .......... .......... 12%  185M 1s\n",
            "  7400K .......... .......... .......... .......... .......... 13%  121M 1s\n",
            "  7450K .......... .......... .......... .......... .......... 13%  156M 1s\n",
            "  7500K .......... .......... .......... .......... .......... 13%  166M 1s\n",
            "  7550K .......... .......... .......... .......... .......... 13%  137M 1s\n",
            "  7600K .......... .......... .......... .......... .......... 13%  171M 1s\n",
            "  7650K .......... .......... .......... .......... .......... 13%  187M 1s\n",
            "  7700K .......... .......... .......... .......... .......... 13%  137M 1s\n",
            "  7750K .......... .......... .......... .......... .......... 13%  148M 1s\n",
            "  7800K .......... .......... .......... .......... .......... 13%  197M 1s\n",
            "  7850K .......... .......... .......... .......... .......... 13%  144M 1s\n",
            "  7900K .......... .......... .......... .......... .......... 13%  135M 1s\n",
            "  7950K .......... .......... .......... .......... .......... 14% 67.7M 1s\n",
            "  8000K .......... .......... .......... .......... .......... 14% 97.1M 1s\n",
            "  8050K .......... .......... .......... .......... .......... 14% 87.1M 1s\n",
            "  8100K .......... .......... .......... .......... .......... 14%  115M 1s\n",
            "  8150K .......... .......... .......... .......... .......... 14%  168M 1s\n",
            "  8200K .......... .......... .......... .......... .......... 14%  234M 1s\n",
            "  8250K .......... .......... .......... .......... .......... 14%  303M 1s\n",
            "  8300K .......... .......... .......... .......... .......... 14%  100M 1s\n",
            "  8350K .......... .......... .......... .......... .......... 14%  132M 1s\n",
            "  8400K .......... .......... .......... .......... .......... 14%  174M 1s\n",
            "  8450K .......... .......... .......... .......... .......... 14%  183M 1s\n",
            "  8500K .......... .......... .......... .......... .......... 14%  288M 1s\n",
            "  8550K .......... .......... .......... .......... .......... 15%  248M 1s\n",
            "  8600K .......... .......... .......... .......... .......... 15%  367M 1s\n",
            "  8650K .......... .......... .......... .......... .......... 15%  184M 1s\n",
            "  8700K .......... .......... .......... .......... .......... 15%  257M 1s\n",
            "  8750K .......... .......... .......... .......... .......... 15%  201M 1s\n",
            "  8800K .......... .......... .......... .......... .......... 15%  122M 1s\n",
            "  8850K .......... .......... .......... .......... .......... 15%  152M 1s\n",
            "  8900K .......... .......... .......... .......... .......... 15%  202M 1s\n",
            "  8950K .......... .......... .......... .......... .......... 15%  139M 1s\n",
            "  9000K .......... .......... .......... .......... .......... 15%  119M 1s\n",
            "  9050K .......... .......... .......... .......... .......... 15%  142M 1s\n",
            "  9100K .......... .......... .......... .......... .......... 16%  134M 1s\n",
            "  9150K .......... .......... .......... .......... .......... 16%  136M 1s\n",
            "  9200K .......... .......... .......... .......... .......... 16%  149M 1s\n",
            "  9250K .......... .......... .......... .......... .......... 16%  154M 1s\n",
            "  9300K .......... .......... .......... .......... .......... 16%  147M 1s\n",
            "  9350K .......... .......... .......... .......... .......... 16%  146M 1s\n",
            "  9400K .......... .......... .......... .......... .......... 16%  198M 1s\n",
            "  9450K .......... .......... .......... .......... .......... 16%  204M 1s\n",
            "  9500K .......... .......... .......... .......... .......... 16%  187M 1s\n",
            "  9550K .......... .......... .......... .......... .......... 16%  147M 1s\n",
            "  9600K .......... .......... .......... .......... .......... 16%  165M 1s\n",
            "  9650K .......... .......... .......... .......... .......... 16%  136M 1s\n",
            "  9700K .......... .......... .......... .......... .......... 17%  156M 1s\n",
            "  9750K .......... .......... .......... .......... .......... 17%  154M 1s\n",
            "  9800K .......... .......... .......... .......... .......... 17%  186M 1s\n",
            "  9850K .......... .......... .......... .......... .......... 17% 93.9M 1s\n",
            "  9900K .......... .......... .......... .......... .......... 17%  111M 1s\n",
            "  9950K .......... .......... .......... .......... .......... 17% 88.7M 1s\n",
            " 10000K .......... .......... .......... .......... .......... 17%  177M 1s\n",
            " 10050K .......... .......... .......... .......... .......... 17%  201M 1s\n",
            " 10100K .......... .......... .......... .......... .......... 17%  161M 1s\n",
            " 10150K .......... .......... .......... .......... .......... 17%  124M 1s\n",
            " 10200K .......... .......... .......... .......... .......... 17%  180M 1s\n",
            " 10250K .......... .......... .......... .......... .......... 18%  187M 1s\n",
            " 10300K .......... .......... .......... .......... .......... 18%  173M 1s\n",
            " 10350K .......... .......... .......... .......... .......... 18%  106M 1s\n",
            " 10400K .......... .......... .......... .......... .......... 18%  182M 1s\n",
            " 10450K .......... .......... .......... .......... .......... 18% 97.0M 1s\n",
            " 10500K .......... .......... .......... .......... .......... 18%  127M 1s\n",
            " 10550K .......... .......... .......... .......... .......... 18%  150M 1s\n",
            " 10600K .......... .......... .......... .......... .......... 18%  187M 1s\n",
            " 10650K .......... .......... .......... .......... .......... 18%  184M 1s\n",
            " 10700K .......... .......... .......... .......... .......... 18%  168M 1s\n",
            " 10750K .......... .......... .......... .......... .......... 18% 65.8M 1s\n",
            " 10800K .......... .......... .......... .......... .......... 19%  116M 1s\n",
            " 10850K .......... .......... .......... .......... .......... 19%  131M 1s\n",
            " 10900K .......... .......... .......... .......... .......... 19%  183M 1s\n",
            " 10950K .......... .......... .......... .......... .......... 19%  151M 1s\n",
            " 11000K .......... .......... .......... .......... .......... 19%  166M 1s\n",
            " 11050K .......... .......... .......... .......... .......... 19%  184M 1s\n",
            " 11100K .......... .......... .......... .......... .......... 19%  165M 1s\n",
            " 11150K .......... .......... .......... .......... .......... 19%  136M 1s\n",
            " 11200K .......... .......... .......... .......... .......... 19%  174M 1s\n",
            " 11250K .......... .......... .......... .......... .......... 19%  174M 1s\n",
            " 11300K .......... .......... .......... .......... .......... 19%  173M 1s\n",
            " 11350K .......... .......... .......... .......... .......... 19%  150M 1s\n",
            " 11400K .......... .......... .......... .......... .......... 20%  153M 1s\n",
            " 11450K .......... .......... .......... .......... .......... 20%  159M 1s\n",
            " 11500K .......... .......... .......... .......... .......... 20%  165M 1s\n",
            " 11550K .......... .......... .......... .......... .......... 20%  153M 1s\n",
            " 11600K .......... .......... .......... .......... .......... 20%  156M 1s\n",
            " 11650K .......... .......... .......... .......... .......... 20% 93.6M 1s\n",
            " 11700K .......... .......... .......... .......... .......... 20%  169M 1s\n",
            " 11750K .......... .......... .......... .......... .......... 20%  144M 1s\n",
            " 11800K .......... .......... .......... .......... .......... 20%  118M 1s\n",
            " 11850K .......... .......... .......... .......... .......... 20% 93.2M 1s\n",
            " 11900K .......... .......... .......... .......... .......... 20%  109M 1s\n",
            " 11950K .......... .......... .......... .......... .......... 21% 81.4M 1s\n",
            " 12000K .......... .......... .......... .......... .......... 21%  166M 1s\n",
            " 12050K .......... .......... .......... .......... .......... 21%  128M 1s\n",
            " 12100K .......... .......... .......... .......... .......... 21%  172M 1s\n",
            " 12150K .......... .......... .......... .......... .......... 21%  107M 1s\n",
            " 12200K .......... .......... .......... .......... .......... 21%  137M 1s\n",
            " 12250K .......... .......... .......... .......... .......... 21%  131M 1s\n",
            " 12300K .......... .......... .......... .......... .......... 21%  185M 1s\n",
            " 12350K .......... .......... .......... .......... .......... 21%  145M 1s\n",
            " 12400K .......... .......... .......... .......... .......... 21%  161M 1s\n",
            " 12450K .......... .......... .......... .......... .......... 21%  189M 1s\n",
            " 12500K .......... .......... .......... .......... .......... 21%  120M 1s\n",
            " 12550K .......... .......... .......... .......... .......... 22%  109M 1s\n",
            " 12600K .......... .......... .......... .......... .......... 22%  152M 1s\n",
            " 12650K .......... .......... .......... .......... .......... 22%  180M 1s\n",
            " 12700K .......... .......... .......... .......... .......... 22%  167M 1s\n",
            " 12750K .......... .......... .......... .......... .......... 22%  149M 1s\n",
            " 12800K .......... .......... .......... .......... .......... 22%  177M 1s\n",
            " 12850K .......... .......... .......... .......... .......... 22%  151M 1s\n",
            " 12900K .......... .......... .......... .......... .......... 22%  164M 1s\n",
            " 12950K .......... .......... .......... .......... .......... 22%  162M 1s\n",
            " 13000K .......... .......... .......... .......... .......... 22%  178M 1s\n",
            " 13050K .......... .......... .......... .......... .......... 22%  193M 1s\n",
            " 13100K .......... .......... .......... .......... .......... 23%  181M 1s\n",
            " 13150K .......... .......... .......... .......... .......... 23%  155M 1s\n",
            " 13200K .......... .......... .......... .......... .......... 23%  159M 1s\n",
            " 13250K .......... .......... .......... .......... .......... 23%  181M 1s\n",
            " 13300K .......... .......... .......... .......... .......... 23%  174M 1s\n",
            " 13350K .......... .......... .......... .......... .......... 23%  149M 1s\n",
            " 13400K .......... .......... .......... .......... .......... 23%  185M 1s\n",
            " 13450K .......... .......... .......... .......... .......... 23%  104M 1s\n",
            " 13500K .......... .......... .......... .......... .......... 23% 96.9M 1s\n",
            " 13550K .......... .......... .......... .......... .......... 23%  120M 1s\n",
            " 13600K .......... .......... .......... .......... .......... 23%  117M 1s\n",
            " 13650K .......... .......... .......... .......... .......... 23%  110M 1s\n",
            " 13700K .......... .......... .......... .......... .......... 24%  142M 1s\n",
            " 13750K .......... .......... .......... .......... .......... 24%  109M 1s\n",
            " 13800K .......... .......... .......... .......... .......... 24%  194M 1s\n",
            " 13850K .......... .......... .......... .......... .......... 24%  189M 1s\n",
            " 13900K .......... .......... .......... .......... .......... 24%  155M 1s\n",
            " 13950K .......... .......... .......... .......... .......... 24%  157M 1s\n",
            " 14000K .......... .......... .......... .......... .......... 24%  129M 1s\n",
            " 14050K .......... .......... .......... .......... .......... 24%  151M 1s\n",
            " 14100K .......... .......... .......... .......... .......... 24%  173M 1s\n",
            " 14150K .......... .......... .......... .......... .......... 24%  169M 1s\n",
            " 14200K .......... .......... .......... .......... .......... 24%  159M 1s\n",
            " 14250K .......... .......... .......... .......... .......... 25%  179M 1s\n",
            " 14300K .......... .......... .......... .......... .......... 25%  188M 1s\n",
            " 14350K .......... .......... .......... .......... .......... 25%  142M 1s\n",
            " 14400K .......... .......... .......... .......... .......... 25%  152M 0s\n",
            " 14450K .......... .......... .......... .......... .......... 25%  179M 0s\n",
            " 14500K .......... .......... .......... .......... .......... 25%  164M 0s\n",
            " 14550K .......... .......... .......... .......... .......... 25%  143M 0s\n",
            " 14600K .......... .......... .......... .......... .......... 25%  170M 0s\n",
            " 14650K .......... .......... .......... .......... .......... 25%  197M 0s\n",
            " 14700K .......... .......... .......... .......... .......... 25%  174M 0s\n",
            " 14750K .......... .......... .......... .......... .......... 25%  146M 0s\n",
            " 14800K .......... .......... .......... .......... .......... 26%  184M 0s\n",
            " 14850K .......... .......... .......... .......... .......... 26%  191M 0s\n",
            " 14900K .......... .......... .......... .......... .......... 26%  161M 0s\n",
            " 14950K .......... .......... .......... .......... .......... 26% 95.4M 0s\n",
            " 15000K .......... .......... .......... .......... .......... 26%  172M 0s\n",
            " 15050K .......... .......... .......... .......... .......... 26%  161M 0s\n",
            " 15100K .......... .......... .......... .......... .......... 26%  185M 0s\n",
            " 15150K .......... .......... .......... .......... .......... 26% 85.4M 0s\n",
            " 15200K .......... .......... .......... .......... .......... 26%  158M 0s\n",
            " 15250K .......... .......... .......... .......... .......... 26%  174M 0s\n",
            " 15300K .......... .......... .......... .......... .......... 26%  166M 0s\n",
            " 15350K .......... .......... .......... .......... .......... 26%  131M 0s\n",
            " 15400K .......... .......... .......... .......... .......... 27%  176M 0s\n",
            " 15450K .......... .......... .......... .......... .......... 27%  171M 0s\n",
            " 15500K .......... .......... .......... .......... .......... 27%  147M 0s\n",
            " 15550K .......... .......... .......... .......... .......... 27%  136M 0s\n",
            " 15600K .......... .......... .......... .......... .......... 27%  177M 0s\n",
            " 15650K .......... .......... .......... .......... .......... 27%  163M 0s\n",
            " 15700K .......... .......... .......... .......... .......... 27%  183M 0s\n",
            " 15750K .......... .......... .......... .......... .......... 27%  162M 0s\n",
            " 15800K .......... .......... .......... .......... .......... 27%  109M 0s\n",
            " 15850K .......... .......... .......... .......... .......... 27%  104M 0s\n",
            " 15900K .......... .......... .......... .......... .......... 27% 83.0M 0s\n",
            " 15950K .......... .......... .......... .......... .......... 28% 69.0M 0s\n",
            " 16000K .......... .......... .......... .......... .......... 28%  104M 0s\n",
            " 16050K .......... .......... .......... .......... .......... 28% 87.4M 0s\n",
            " 16100K .......... .......... .......... .......... .......... 28%  129M 0s\n",
            " 16150K .......... .......... .......... .......... .......... 28%  165M 0s\n",
            " 16200K .......... .......... .......... .......... .......... 28%  180M 0s\n",
            " 16250K .......... .......... .......... .......... .......... 28%  173M 0s\n",
            " 16300K .......... .......... .......... .......... .......... 28%  167M 0s\n",
            " 16350K .......... .......... .......... .......... .......... 28%  148M 0s\n",
            " 16400K .......... .......... .......... .......... .......... 28%  183M 0s\n",
            " 16450K .......... .......... .......... .......... .......... 28%  197M 0s\n",
            " 16500K .......... .......... .......... .......... .......... 28%  197M 0s\n",
            " 16550K .......... .......... .......... .......... .......... 29%  153M 0s\n",
            " 16600K .......... .......... .......... .......... .......... 29%  166M 0s\n",
            " 16650K .......... .......... .......... .......... .......... 29%  175M 0s\n",
            " 16700K .......... .......... .......... .......... .......... 29%  109M 0s\n",
            " 16750K .......... .......... .......... .......... .......... 29%  104M 0s\n",
            " 16800K .......... .......... .......... .......... .......... 29%  187M 0s\n",
            " 16850K .......... .......... .......... .......... .......... 29%  197M 0s\n",
            " 16900K .......... .......... .......... .......... .......... 29%  183M 0s\n",
            " 16950K .......... .......... .......... .......... .......... 29%  150M 0s\n",
            " 17000K .......... .......... .......... .......... .......... 29%  170M 0s\n",
            " 17050K .......... .......... .......... .......... .......... 29%  174M 0s\n",
            " 17100K .......... .......... .......... .......... .......... 30%  193M 0s\n",
            " 17150K .......... .......... .......... .......... .......... 30%  131M 0s\n",
            " 17200K .......... .......... .......... .......... .......... 30%  147M 0s\n",
            " 17250K .......... .......... .......... .......... .......... 30%  178M 0s\n",
            " 17300K .......... .......... .......... .......... .......... 30%  169M 0s\n",
            " 17350K .......... .......... .......... .......... .......... 30%  155M 0s\n",
            " 17400K .......... .......... .......... .......... .......... 30%  198M 0s\n",
            " 17450K .......... .......... .......... .......... .......... 30%  187M 0s\n",
            " 17500K .......... .......... .......... .......... .......... 30%  197M 0s\n",
            " 17550K .......... .......... .......... .......... .......... 30%  148M 0s\n",
            " 17600K .......... .......... .......... .......... .......... 30%  185M 0s\n",
            " 17650K .......... .......... .......... .......... .......... 30%  181M 0s\n",
            " 17700K .......... .......... .......... .......... .......... 31%  116M 0s\n",
            " 17750K .......... .......... .......... .......... .......... 31% 93.8M 0s\n",
            " 17800K .......... .......... .......... .......... .......... 31%  158M 0s\n",
            " 17850K .......... .......... .......... .......... .......... 31%  183M 0s\n",
            " 17900K .......... .......... .......... .......... .......... 31%  167M 0s\n",
            " 17950K .......... .......... .......... .......... .......... 31%  143M 0s\n",
            " 18000K .......... .......... .......... .......... .......... 31%  171M 0s\n",
            " 18050K .......... .......... .......... .......... .......... 31%  142M 0s\n",
            " 18100K .......... .......... .......... .......... .......... 31%  172M 0s\n",
            " 18150K .......... .......... .......... .......... .......... 31%  154M 0s\n",
            " 18200K .......... .......... .......... .......... .......... 31%  163M 0s\n",
            " 18250K .......... .......... .......... .......... .......... 32%  171M 0s\n",
            " 18300K .......... .......... .......... .......... .......... 32%  146M 0s\n",
            " 18350K .......... .......... .......... .......... .......... 32%  126M 0s\n",
            " 18400K .......... .......... .......... .......... .......... 32%  124M 0s\n",
            " 18450K .......... .......... .......... .......... .......... 32% 75.6M 0s\n",
            " 18500K .......... .......... .......... .......... .......... 32% 80.9M 0s\n",
            " 18550K .......... .......... .......... .......... .......... 32% 78.0M 0s\n",
            " 18600K .......... .......... .......... .......... .......... 32% 83.4M 0s\n",
            " 18650K .......... .......... .......... .......... .......... 32% 94.4M 0s\n",
            " 18700K .......... .......... .......... .......... .......... 32%  122M 0s\n",
            " 18750K .......... .......... .......... .......... .......... 32%  129M 0s\n",
            " 18800K .......... .......... .......... .......... .......... 33%  161M 0s\n",
            " 18850K .......... .......... .......... .......... .......... 33%  187M 0s\n",
            " 18900K .......... .......... .......... .......... .......... 33%  180M 0s\n",
            " 18950K .......... .......... .......... .......... .......... 33%  163M 0s\n",
            " 19000K .......... .......... .......... .......... .......... 33%  193M 0s\n",
            " 19050K .......... .......... .......... .......... .......... 33%  170M 0s\n",
            " 19100K .......... .......... .......... .......... .......... 33%  196M 0s\n",
            " 19150K .......... .......... .......... .......... .......... 33%  148M 0s\n",
            " 19200K .......... .......... .......... .......... .......... 33%  193M 0s\n",
            " 19250K .......... .......... .......... .......... .......... 33%  160M 0s\n",
            " 19300K .......... .......... .......... .......... .......... 33%  163M 0s\n",
            " 19350K .......... .......... .......... .......... .......... 33%  154M 0s\n",
            " 19400K .......... .......... .......... .......... .......... 34%  123M 0s\n",
            " 19450K .......... .......... .......... .......... .......... 34% 93.4M 0s\n",
            " 19500K .......... .......... .......... .......... .......... 34%  111M 0s\n",
            " 19550K .......... .......... .......... .......... .......... 34% 92.0M 0s\n",
            " 19600K .......... .......... .......... .......... .......... 34%  148M 0s\n",
            " 19650K .......... .......... .......... .......... .......... 34%  168M 0s\n",
            " 19700K .......... .......... .......... .......... .......... 34%  174M 0s\n",
            " 19750K .......... .......... .......... .......... .......... 34%  136M 0s\n",
            " 19800K .......... .......... .......... .......... .......... 34%  137M 0s\n",
            " 19850K .......... .......... .......... .......... .......... 34%  156M 0s\n",
            " 19900K .......... .......... .......... .......... .......... 34%  185M 0s\n",
            " 19950K .......... .......... .......... .......... .......... 35%  137M 0s\n",
            " 20000K .......... .......... .......... .......... .......... 35%  179M 0s\n",
            " 20050K .......... .......... .......... .......... .......... 35%  120M 0s\n",
            " 20100K .......... .......... .......... .......... .......... 35% 83.6M 0s\n",
            " 20150K .......... .......... .......... .......... .......... 35%  148M 0s\n",
            " 20200K .......... .......... .......... .......... .......... 35%  162M 0s\n",
            " 20250K .......... .......... .......... .......... .......... 35%  175M 0s\n",
            " 20300K .......... .......... .......... .......... .......... 35%  124M 0s\n",
            " 20350K .......... .......... .......... .......... .......... 35% 82.8M 0s\n",
            " 20400K .......... .......... .......... .......... .......... 35% 97.2M 0s\n",
            " 20450K .......... .......... .......... .......... .......... 35% 79.9M 0s\n",
            " 20500K .......... .......... .......... .......... .......... 35%  160M 0s\n",
            " 20550K .......... .......... .......... .......... .......... 36%  139M 0s\n",
            " 20600K .......... .......... .......... .......... .......... 36%  181M 0s\n",
            " 20650K .......... .......... .......... .......... .......... 36%  171M 0s\n",
            " 20700K .......... .......... .......... .......... .......... 36%  178M 0s\n",
            " 20750K .......... .......... .......... .......... .......... 36%  146M 0s\n",
            " 20800K .......... .......... .......... .......... .......... 36%  172M 0s\n",
            " 20850K .......... .......... .......... .......... .......... 36%  181M 0s\n",
            " 20900K .......... .......... .......... .......... .......... 36%  165M 0s\n",
            " 20950K .......... .......... .......... .......... .......... 36%  156M 0s\n",
            " 21000K .......... .......... .......... .......... .......... 36%  174M 0s\n",
            " 21050K .......... .......... .......... .......... .......... 36%  157M 0s\n",
            " 21100K .......... .......... .......... .......... .......... 37%  128M 0s\n",
            " 21150K .......... .......... .......... .......... .......... 37% 81.7M 0s\n",
            " 21200K .......... .......... .......... .......... .......... 37%  185M 0s\n",
            " 21250K .......... .......... .......... .......... .......... 37%  179M 0s\n",
            " 21300K .......... .......... .......... .......... .......... 37%  168M 0s\n",
            " 21350K .......... .......... .......... .......... .......... 37%  144M 0s\n",
            " 21400K .......... .......... .......... .......... .......... 37%  193M 0s\n",
            " 21450K .......... .......... .......... .......... .......... 37%  185M 0s\n",
            " 21500K .......... .......... .......... .......... .......... 37%  143M 0s\n",
            " 21550K .......... .......... .......... .......... .......... 37%  127M 0s\n",
            " 21600K .......... .......... .......... .......... .......... 37%  147M 0s\n",
            " 21650K .......... .......... .......... .......... .......... 38%  167M 0s\n",
            " 21700K .......... .......... .......... .......... .......... 38%  168M 0s\n",
            " 21750K .......... .......... .......... .......... .......... 38%  158M 0s\n",
            " 21800K .......... .......... .......... .......... .......... 38%  164M 0s\n",
            " 21850K .......... .......... .......... .......... .......... 38%  188M 0s\n",
            " 21900K .......... .......... .......... .......... .......... 38%  166M 0s\n",
            " 21950K .......... .......... .......... .......... .......... 38%  131M 0s\n",
            " 22000K .......... .......... .......... .......... .......... 38% 95.2M 0s\n",
            " 22050K .......... .......... .......... .......... .......... 38%  174M 0s\n",
            " 22100K .......... .......... .......... .......... .......... 38%  191M 0s\n",
            " 22150K .......... .......... .......... .......... .......... 38%  141M 0s\n",
            " 22200K .......... .......... .......... .......... .......... 38%  161M 0s\n",
            " 22250K .......... .......... .......... .......... .......... 39%  185M 0s\n",
            " 22300K .......... .......... .......... .......... .......... 39%  168M 0s\n",
            " 22350K .......... .......... .......... .......... .......... 39%  147M 0s\n",
            " 22400K .......... .......... .......... .......... .......... 39%  184M 0s\n",
            " 22450K .......... .......... .......... .......... .......... 39%  184M 0s\n",
            " 22500K .......... .......... .......... .......... .......... 39%  196M 0s\n",
            " 22550K .......... .......... .......... .......... .......... 39%  176M 0s\n",
            " 22600K .......... .......... .......... .......... .......... 39% 96.0M 0s\n",
            " 22650K .......... .......... .......... .......... .......... 39%  181M 0s\n",
            " 22700K .......... .......... .......... .......... .......... 39%  188M 0s\n",
            " 22750K .......... .......... .......... .......... .......... 39%  152M 0s\n",
            " 22800K .......... .......... .......... .......... .......... 40%  174M 0s\n",
            " 22850K .......... .......... .......... .......... .......... 40%  140M 0s\n",
            " 22900K .......... .......... .......... .......... .......... 40% 82.4M 0s\n",
            " 22950K .......... .......... .......... .......... .......... 40%  164M 0s\n",
            " 23000K .......... .......... .......... .......... .......... 40%  184M 0s\n",
            " 23050K .......... .......... .......... .......... .......... 40%  198M 0s\n",
            " 23100K .......... .......... .......... .......... .......... 40%  183M 0s\n",
            " 23150K .......... .......... .......... .......... .......... 40%  161M 0s\n",
            " 23200K .......... .......... .......... .......... .......... 40% 66.9M 0s\n",
            " 23250K .......... .......... .......... .......... .......... 40%  108M 0s\n",
            " 23300K .......... .......... .......... .......... .......... 40%  216M 0s\n",
            " 23350K .......... .......... .......... .......... .......... 40%  157M 0s\n",
            " 23400K .......... .......... .......... .......... .......... 41% 98.2M 0s\n",
            " 23450K .......... .......... .......... .......... .......... 41% 77.7M 0s\n",
            " 23500K .......... .......... .......... .......... .......... 41%  189M 0s\n",
            " 23550K .......... .......... .......... .......... .......... 41% 46.6M 0s\n",
            " 23600K .......... .......... .......... .......... .......... 41%  198M 0s\n",
            " 23650K .......... .......... .......... .......... .......... 41% 90.2M 0s\n",
            " 23700K .......... .......... .......... .......... .......... 41%  111M 0s\n",
            " 23750K .......... .......... .......... .......... .......... 41%  105M 0s\n",
            " 23800K .......... .......... .......... .......... .......... 41%  127M 0s\n",
            " 23850K .......... .......... .......... .......... .......... 41%  159M 0s\n",
            " 23900K .......... .......... .......... .......... .......... 41%  225M 0s\n",
            " 23950K .......... .......... .......... .......... .......... 42%  182M 0s\n",
            " 24000K .......... .......... .......... .......... .......... 42%  216M 0s\n",
            " 24050K .......... .......... .......... .......... .......... 42%  213M 0s\n",
            " 24100K .......... .......... .......... .......... .......... 42%  214M 0s\n",
            " 24150K .......... .......... .......... .......... .......... 42%  201M 0s\n",
            " 24200K .......... .......... .......... .......... .......... 42%  206M 0s\n",
            " 24250K .......... .......... .......... .......... .......... 42%  231M 0s\n",
            " 24300K .......... .......... .......... .......... .......... 42% 32.6M 0s\n",
            " 24350K .......... .......... .......... .......... .......... 42% 34.8M 0s\n",
            " 24400K .......... .......... .......... .......... .......... 42%  155M 0s\n",
            " 24450K .......... .......... .......... .......... .......... 42%  178M 0s\n",
            " 24500K .......... .......... .......... .......... .......... 42%  188M 0s\n",
            " 24550K .......... .......... .......... .......... .......... 43%  165M 0s\n",
            " 24600K .......... .......... .......... .......... .......... 43%  184M 0s\n",
            " 24650K .......... .......... .......... .......... .......... 43%  101M 0s\n",
            " 24700K .......... .......... .......... .......... .......... 43% 83.4M 0s\n",
            " 24750K .......... .......... .......... .......... .......... 43%  144M 0s\n",
            " 24800K .......... .......... .......... .......... .......... 43% 94.1M 0s\n",
            " 24850K .......... .......... .......... .......... .......... 43%  112M 0s\n",
            " 24900K .......... .......... .......... .......... .......... 43%  187M 0s\n",
            " 24950K .......... .......... .......... .......... .......... 43%  157M 0s\n",
            " 25000K .......... .......... .......... .......... .......... 43%  202M 0s\n",
            " 25050K .......... .......... .......... .......... .......... 43%  101M 0s\n",
            " 25100K .......... .......... .......... .......... .......... 44%  160M 0s\n",
            " 25150K .......... .......... .......... .......... .......... 44%  122M 0s\n",
            " 25200K .......... .......... .......... .......... .......... 44%  186M 0s\n",
            " 25250K .......... .......... .......... .......... .......... 44%  166M 0s\n",
            " 25300K .......... .......... .......... .......... .......... 44%  170M 0s\n",
            " 25350K .......... .......... .......... .......... .......... 44%  150M 0s\n",
            " 25400K .......... .......... .......... .......... .......... 44%  175M 0s\n",
            " 25450K .......... .......... .......... .......... .......... 44%  191M 0s\n",
            " 25500K .......... .......... .......... .......... .......... 44%  188M 0s\n",
            " 25550K .......... .......... .......... .......... .......... 44%  153M 0s\n",
            " 25600K .......... .......... .......... .......... .......... 44%  186M 0s\n",
            " 25650K .......... .......... .......... .......... .......... 45%  205M 0s\n",
            " 25700K .......... .......... .......... .......... .......... 45%  172M 0s\n",
            " 25750K .......... .......... .......... .......... .......... 45%  163M 0s\n",
            " 25800K .......... .......... .......... .......... .......... 45%  145M 0s\n",
            " 25850K .......... .......... .......... .......... .......... 45%  193M 0s\n",
            " 25900K .......... .......... .......... .......... .......... 45%  188M 0s\n",
            " 25950K .......... .......... .......... .......... .......... 45%  151M 0s\n",
            " 26000K .......... .......... .......... .......... .......... 45%  186M 0s\n",
            " 26050K .......... .......... .......... .......... .......... 45%  180M 0s\n",
            " 26100K .......... .......... .......... .......... .......... 45%  181M 0s\n",
            " 26150K .......... .......... .......... .......... .......... 45%  153M 0s\n",
            " 26200K .......... .......... .......... .......... .......... 45% 97.8M 0s\n",
            " 26250K .......... .......... .......... .......... .......... 46%  142M 0s\n",
            " 26300K .......... .......... .......... .......... .......... 46%  183M 0s\n",
            " 26350K .......... .......... .......... .......... .......... 46%  154M 0s\n",
            " 26400K .......... .......... .......... .......... .......... 46%  176M 0s\n",
            " 26450K .......... .......... .......... .......... .......... 46%  181M 0s\n",
            " 26500K .......... .......... .......... .......... .......... 46%  188M 0s\n",
            " 26550K .......... .......... .......... .......... .......... 46%  161M 0s\n",
            " 26600K .......... .......... .......... .......... .......... 46%  175M 0s\n",
            " 26650K .......... .......... .......... .......... .......... 46%  204M 0s\n",
            " 26700K .......... .......... .......... .......... .......... 46%  198M 0s\n",
            " 26750K .......... .......... .......... .......... .......... 46%  145M 0s\n",
            " 26800K .......... .......... .......... .......... .......... 47%  170M 0s\n",
            " 26850K .......... .......... .......... .......... .......... 47%  192M 0s\n",
            " 26900K .......... .......... .......... .......... .......... 47%  202M 0s\n",
            " 26950K .......... .......... .......... .......... .......... 47%  164M 0s\n",
            " 27000K .......... .......... .......... .......... .......... 47%  190M 0s\n",
            " 27050K .......... .......... .......... .......... .......... 47%  193M 0s\n",
            " 27100K .......... .......... .......... .......... .......... 47%  181M 0s\n",
            " 27150K .......... .......... .......... .......... .......... 47%  137M 0s\n",
            " 27200K .......... .......... .......... .......... .......... 47%  183M 0s\n",
            " 27250K .......... .......... .......... .......... .......... 47%  171M 0s\n",
            " 27300K .......... .......... .......... .......... .......... 47% 84.9M 0s\n",
            " 27350K .......... .......... .......... .......... .......... 47%  157M 0s\n",
            " 27400K .......... .......... .......... .......... .......... 48%  156M 0s\n",
            " 27450K .......... .......... .......... .......... .......... 48%  166M 0s\n",
            " 27500K .......... .......... .......... .......... .......... 48%  162M 0s\n",
            " 27550K .......... .......... .......... .......... .......... 48% 23.8M 0s\n",
            " 27600K .......... .......... .......... .......... .......... 48%  101M 0s\n",
            " 27650K .......... .......... .......... .......... .......... 48% 96.1M 0s\n",
            " 27700K .......... .......... .......... .......... .......... 48% 77.9M 0s\n",
            " 27750K .......... .......... .......... .......... .......... 48%  161M 0s\n",
            " 27800K .......... .......... .......... .......... .......... 48%  185M 0s\n",
            " 27850K .......... .......... .......... .......... .......... 48%  182M 0s\n",
            " 27900K .......... .......... .......... .......... .......... 48%  147M 0s\n",
            " 27950K .......... .......... .......... .......... .......... 49%  153M 0s\n",
            " 28000K .......... .......... .......... .......... .......... 49%  190M 0s\n",
            " 28050K .......... .......... .......... .......... .......... 49%  145M 0s\n",
            " 28100K .......... .......... .......... .......... .......... 49%  180M 0s\n",
            " 28150K .......... .......... .......... .......... .......... 49%  125M 0s\n",
            " 28200K .......... .......... .......... .......... .......... 49%  114M 0s\n",
            " 28250K .......... .......... .......... .......... .......... 49% 87.9M 0s\n",
            " 28300K .......... .......... .......... .......... .......... 49%  112M 0s\n",
            " 28350K .......... .......... .......... .......... .......... 49% 84.8M 0s\n",
            " 28400K .......... .......... .......... .......... .......... 49%  149M 0s\n",
            " 28450K .......... .......... .......... .......... .......... 49%  142M 0s\n",
            " 28500K .......... .......... .......... .......... .......... 50%  148M 0s\n",
            " 28550K .......... .......... .......... .......... .......... 50%  125M 0s\n",
            " 28600K .......... .......... .......... .......... .......... 50%  163M 0s\n",
            " 28650K .......... .......... .......... .......... .......... 50%  164M 0s\n",
            " 28700K .......... .......... .......... .......... .......... 50%  148M 0s\n",
            " 28750K .......... .......... .......... .......... .......... 50% 89.7M 0s\n",
            " 28800K .......... .......... .......... .......... .......... 50%  123M 0s\n",
            " 28850K .......... .......... .......... .......... .......... 50%  164M 0s\n",
            " 28900K .......... .......... .......... .......... .......... 50%  182M 0s\n",
            " 28950K .......... .......... .......... .......... .......... 50%  160M 0s\n",
            " 29000K .......... .......... .......... .......... .......... 50% 99.8M 0s\n",
            " 29050K .......... .......... .......... .......... .......... 50%  158M 0s\n",
            " 29100K .......... .......... .......... .......... .......... 51%  166M 0s\n",
            " 29150K .......... .......... .......... .......... .......... 51%  130M 0s\n",
            " 29200K .......... .......... .......... .......... .......... 51% 99.9M 0s\n",
            " 29250K .......... .......... .......... .......... .......... 51% 70.8M 0s\n",
            " 29300K .......... .......... .......... .......... .......... 51% 73.2M 0s\n",
            " 29350K .......... .......... .......... .......... .......... 51%  118M 0s\n",
            " 29400K .......... .......... .......... .......... .......... 51%  147M 0s\n",
            " 29450K .......... .......... .......... .......... .......... 51%  173M 0s\n",
            " 29500K .......... .......... .......... .......... .......... 51%  156M 0s\n",
            " 29550K .......... .......... .......... .......... .......... 51%  158M 0s\n",
            " 29600K .......... .......... .......... .......... .......... 51%  163M 0s\n",
            " 29650K .......... .......... .......... .......... .......... 52%  184M 0s\n",
            " 29700K .......... .......... .......... .......... .......... 52%  186M 0s\n",
            " 29750K .......... .......... .......... .......... .......... 52%  149M 0s\n",
            " 29800K .......... .......... .......... .......... .......... 52%  185M 0s\n",
            " 29850K .......... .......... .......... .......... .......... 52%  169M 0s\n",
            " 29900K .......... .......... .......... .......... .......... 52%  140M 0s\n",
            " 29950K .......... .......... .......... .......... .......... 52%  140M 0s\n",
            " 30000K .......... .......... .......... .......... .......... 52%  172M 0s\n",
            " 30050K .......... .......... .......... .......... .......... 52%  189M 0s\n",
            " 30100K .......... .......... .......... .......... .......... 52%  156M 0s\n",
            " 30150K .......... .......... .......... .......... .......... 52%  157M 0s\n",
            " 30200K .......... .......... .......... .......... .......... 52%  164M 0s\n",
            " 30250K .......... .......... .......... .......... .......... 53%  169M 0s\n",
            " 30300K .......... .......... .......... .......... .......... 53%  175M 0s\n",
            " 30350K .......... .......... .......... .......... .......... 53% 79.7M 0s\n",
            " 30400K .......... .......... .......... .......... .......... 53%  195M 0s\n",
            " 30450K .......... .......... .......... .......... .......... 53%  200M 0s\n",
            " 30500K .......... .......... .......... .......... .......... 53%  198M 0s\n",
            " 30550K .......... .......... .......... .......... .......... 53%  154M 0s\n",
            " 30600K .......... .......... .......... .......... .......... 53%  194M 0s\n",
            " 30650K .......... .......... .......... .......... .......... 53% 64.9M 0s\n",
            " 30700K .......... .......... .......... .......... .......... 53%  135M 0s\n",
            " 30750K .......... .......... .......... .......... .......... 53%  106M 0s\n",
            " 30800K .......... .......... .......... .......... .......... 54%  135M 0s\n",
            " 30850K .......... .......... .......... .......... .......... 54%  164M 0s\n",
            " 30900K .......... .......... .......... .......... .......... 54%  191M 0s\n",
            " 30950K .......... .......... .......... .......... .......... 54% 77.3M 0s\n",
            " 31000K .......... .......... .......... .......... .......... 54% 96.6M 0s\n",
            " 31050K .......... .......... .......... .......... .......... 54%  111M 0s\n",
            " 31100K .......... .......... .......... .......... .......... 54% 73.1M 0s\n",
            " 31150K .......... .......... .......... .......... .......... 54%  133M 0s\n",
            " 31200K .......... .......... .......... .......... .......... 54%  268M 0s\n",
            " 31250K .......... .......... .......... .......... .......... 54%  252M 0s\n",
            " 31300K .......... .......... .......... .......... .......... 54%  131M 0s\n",
            " 31350K .......... .......... .......... .......... .......... 54%  201M 0s\n",
            " 31400K .......... .......... .......... .......... .......... 55%  291M 0s\n",
            " 31450K .......... .......... .......... .......... .......... 55%  358M 0s\n",
            " 31500K .......... .......... .......... .......... .......... 55%  367M 0s\n",
            " 31550K .......... .......... .......... .......... .......... 55%  185M 0s\n",
            " 31600K .......... .......... .......... .......... .......... 55%  149M 0s\n",
            " 31650K .......... .......... .......... .......... .......... 55%  180M 0s\n",
            " 31700K .......... .......... .......... .......... .......... 55%  193M 0s\n",
            " 31750K .......... .......... .......... .......... .......... 55%  123M 0s\n",
            " 31800K .......... .......... .......... .......... .......... 55%  163M 0s\n",
            " 31850K .......... .......... .......... .......... .......... 55%  121M 0s\n",
            " 31900K .......... .......... .......... .......... .......... 55%  160M 0s\n",
            " 31950K .......... .......... .......... .......... .......... 56%  135M 0s\n",
            " 32000K .......... .......... .......... .......... .......... 56%  134M 0s\n",
            " 32050K .......... .......... .......... .......... .......... 56%  163M 0s\n",
            " 32100K .......... .......... .......... .......... .......... 56%  173M 0s\n",
            " 32150K .......... .......... .......... .......... .......... 56%  109M 0s\n",
            " 32200K .......... .......... .......... .......... .......... 56%  187M 0s\n",
            " 32250K .......... .......... .......... .......... .......... 56%  139M 0s\n",
            " 32300K .......... .......... .......... .......... .......... 56%  159M 0s\n",
            " 32350K .......... .......... .......... .......... .......... 56%  150M 0s\n",
            " 32400K .......... .......... .......... .......... .......... 56%  171M 0s\n",
            " 32450K .......... .......... .......... .......... .......... 56%  128M 0s\n",
            " 32500K .......... .......... .......... .......... .......... 57% 87.8M 0s\n",
            " 32550K .......... .......... .......... .......... .......... 57%  102M 0s\n",
            " 32600K .......... .......... .......... .......... .......... 57%  178M 0s\n",
            " 32650K .......... .......... .......... .......... .......... 57%  171M 0s\n",
            " 32700K .......... .......... .......... .......... .......... 57%  193M 0s\n",
            " 32750K .......... .......... .......... .......... .......... 57%  152M 0s\n",
            " 32800K .......... .......... .......... .......... .......... 57%  195M 0s\n",
            " 32850K .......... .......... .......... .......... .......... 57%  170M 0s\n",
            " 32900K .......... .......... .......... .......... .......... 57%  135M 0s\n",
            " 32950K .......... .......... .......... .......... .......... 57% 98.5M 0s\n",
            " 33000K .......... .......... .......... .......... .......... 57% 96.5M 0s\n",
            " 33050K .......... .......... .......... .......... .......... 57%  120M 0s\n",
            " 33100K .......... .......... .......... .......... .......... 58% 84.0M 0s\n",
            " 33150K .......... .......... .......... .......... .......... 58% 88.4M 0s\n",
            " 33200K .......... .......... .......... .......... .......... 58% 89.1M 0s\n",
            " 33250K .......... .......... .......... .......... .......... 58%  102M 0s\n",
            " 33300K .......... .......... .......... .......... .......... 58%  100M 0s\n",
            " 33350K .......... .......... .......... .......... .......... 58% 83.5M 0s\n",
            " 33400K .......... .......... .......... .......... .......... 58%  124M 0s\n",
            " 33450K .......... .......... .......... .......... .......... 58%  183M 0s\n",
            " 33500K .......... .......... .......... .......... .......... 58%  178M 0s\n",
            " 33550K .......... .......... .......... .......... .......... 58%  137M 0s\n",
            " 33600K .......... .......... .......... .......... .......... 58%  183M 0s\n",
            " 33650K .......... .......... .......... .......... .......... 59%  198M 0s\n",
            " 33700K .......... .......... .......... .......... .......... 59%  198M 0s\n",
            " 33750K .......... .......... .......... .......... .......... 59%  149M 0s\n",
            " 33800K .......... .......... .......... .......... .......... 59%  112M 0s\n",
            " 33850K .......... .......... .......... .......... .......... 59%  180M 0s\n",
            " 33900K .......... .......... .......... .......... .......... 59%  173M 0s\n",
            " 33950K .......... .......... .......... .......... .......... 59%  163M 0s\n",
            " 34000K .......... .......... .......... .......... .......... 59%  171M 0s\n",
            " 34050K .......... .......... .......... .......... .......... 59%  140M 0s\n",
            " 34100K .......... .......... .......... .......... .......... 59%  172M 0s\n",
            " 34150K .......... .......... .......... .......... .......... 59%  166M 0s\n",
            " 34200K .......... .......... .......... .......... .......... 59%  182M 0s\n",
            " 34250K .......... .......... .......... .......... .......... 60%  162M 0s\n",
            " 34300K .......... .......... .......... .......... .......... 60% 78.7M 0s\n",
            " 34350K .......... .......... .......... .......... .......... 60%  121M 0s\n",
            " 34400K .......... .......... .......... .......... .......... 60%  158M 0s\n",
            " 34450K .......... .......... .......... .......... .......... 60%  172M 0s\n",
            " 34500K .......... .......... .......... .......... .......... 60%  171M 0s\n",
            " 34550K .......... .......... .......... .......... .......... 60%  108M 0s\n",
            " 34600K .......... .......... .......... .......... .......... 60%  138M 0s\n",
            " 34650K .......... .......... .......... .......... .......... 60%  170M 0s\n",
            " 34700K .......... .......... .......... .......... .......... 60%  148M 0s\n",
            " 34750K .......... .......... .......... .......... .......... 60% 73.5M 0s\n",
            " 34800K .......... .......... .......... .......... .......... 61% 91.3M 0s\n",
            " 34850K .......... .......... .......... .......... .......... 61%  151M 0s\n",
            " 34900K .......... .......... .......... .......... .......... 61% 96.3M 0s\n",
            " 34950K .......... .......... .......... .......... .......... 61% 74.5M 0s\n",
            " 35000K .......... .......... .......... .......... .......... 61%  184M 0s\n",
            " 35050K .......... .......... .......... .......... .......... 61%  126M 0s\n",
            " 35100K .......... .......... .......... .......... .......... 61%  125M 0s\n",
            " 35150K .......... .......... .......... .......... .......... 61%  143M 0s\n",
            " 35200K .......... .......... .......... .......... .......... 61%  156M 0s\n",
            " 35250K .......... .......... .......... .......... .......... 61%  139M 0s\n",
            " 35300K .......... .......... .......... .......... .......... 61%  167M 0s\n",
            " 35350K .......... .......... .......... .......... .......... 61%  135M 0s\n",
            " 35400K .......... .......... .......... .......... .......... 62%  176M 0s\n",
            " 35450K .......... .......... .......... .......... .......... 62%  176M 0s\n",
            " 35500K .......... .......... .......... .......... .......... 62%  169M 0s\n",
            " 35550K .......... .......... .......... .......... .......... 62%  117M 0s\n",
            " 35600K .......... .......... .......... .......... .......... 62%  160M 0s\n",
            " 35650K .......... .......... .......... .......... .......... 62%  166M 0s\n",
            " 35700K .......... .......... .......... .......... .......... 62%  170M 0s\n",
            " 35750K .......... .......... .......... .......... .......... 62%  154M 0s\n",
            " 35800K .......... .......... .......... .......... .......... 62%  179M 0s\n",
            " 35850K .......... .......... .......... .......... .......... 62%  132M 0s\n",
            " 35900K .......... .......... .......... .......... .......... 62%  102M 0s\n",
            " 35950K .......... .......... .......... .......... .......... 63% 78.1M 0s\n",
            " 36000K .......... .......... .......... .......... .......... 63%  156M 0s\n",
            " 36050K .......... .......... .......... .......... .......... 63%  192M 0s\n",
            " 36100K .......... .......... .......... .......... .......... 63%  174M 0s\n",
            " 36150K .......... .......... .......... .......... .......... 63%  142M 0s\n",
            " 36200K .......... .......... .......... .......... .......... 63%  193M 0s\n",
            " 36250K .......... .......... .......... .......... .......... 63%  193M 0s\n",
            " 36300K .......... .......... .......... .......... .......... 63%  170M 0s\n",
            " 36350K .......... .......... .......... .......... .......... 63%  149M 0s\n",
            " 36400K .......... .......... .......... .......... .......... 63%  172M 0s\n",
            " 36450K .......... .......... .......... .......... .......... 63%  178M 0s\n",
            " 36500K .......... .......... .......... .......... .......... 64%  159M 0s\n",
            " 36550K .......... .......... .......... .......... .......... 64%  155M 0s\n",
            " 36600K .......... .......... .......... .......... .......... 64%  190M 0s\n",
            " 36650K .......... .......... .......... .......... .......... 64% 89.4M 0s\n",
            " 36700K .......... .......... .......... .......... .......... 64%  152M 0s\n",
            " 36750K .......... .......... .......... .......... .......... 64%  138M 0s\n",
            " 36800K .......... .......... .......... .......... .......... 64%  171M 0s\n",
            " 36850K .......... .......... .......... .......... .......... 64%  178M 0s\n",
            " 36900K .......... .......... .......... .......... .......... 64%  166M 0s\n",
            " 36950K .......... .......... .......... .......... .......... 64%  142M 0s\n",
            " 37000K .......... .......... .......... .......... .......... 64% 99.9M 0s\n",
            " 37050K .......... .......... .......... .......... .......... 64% 88.3M 0s\n",
            " 37100K .......... .......... .......... .......... .......... 65% 94.3M 0s\n",
            " 37150K .......... .......... .......... .......... .......... 65% 97.0M 0s\n",
            " 37200K .......... .......... .......... .......... .......... 65%  197M 0s\n",
            " 37250K .......... .......... .......... .......... .......... 65%  125M 0s\n",
            " 37300K .......... .......... .......... .......... .......... 65%  155M 0s\n",
            " 37350K .......... .......... .......... .......... .......... 65%  153M 0s\n",
            " 37400K .......... .......... .......... .......... .......... 65%  179M 0s\n",
            " 37450K .......... .......... .......... .......... .......... 65%  147M 0s\n",
            " 37500K .......... .......... .......... .......... .......... 65%  181M 0s\n",
            " 37550K .......... .......... .......... .......... .......... 65%  143M 0s\n",
            " 37600K .......... .......... .......... .......... .......... 65%  176M 0s\n",
            " 37650K .......... .......... .......... .......... .......... 66%  120M 0s\n",
            " 37700K .......... .......... .......... .......... .......... 66%  180M 0s\n",
            " 37750K .......... .......... .......... .......... .......... 66%  138M 0s\n",
            " 37800K .......... .......... .......... .......... .......... 66%  161M 0s\n",
            " 37850K .......... .......... .......... .......... .......... 66%  157M 0s\n",
            " 37900K .......... .......... .......... .......... .......... 66%  145M 0s\n",
            " 37950K .......... .......... .......... .......... .......... 66%  135M 0s\n",
            " 38000K .......... .......... .......... .......... .......... 66%  171M 0s\n",
            " 38050K .......... .......... .......... .......... .......... 66%  172M 0s\n",
            " 38100K .......... .......... .......... .......... .......... 66%  198M 0s\n",
            " 38150K .......... .......... .......... .......... .......... 66%  164M 0s\n",
            " 38200K .......... .......... .......... .......... .......... 66%  157M 0s\n",
            " 38250K .......... .......... .......... .......... .......... 67%  171M 0s\n",
            " 38300K .......... .......... .......... .......... .......... 67%  113M 0s\n",
            " 38350K .......... .......... .......... .......... .......... 67% 76.9M 0s\n",
            " 38400K .......... .......... .......... .......... .......... 67%  164M 0s\n",
            " 38450K .......... .......... .......... .......... .......... 67%  167M 0s\n",
            " 38500K .......... .......... .......... .......... .......... 67%  120M 0s\n",
            " 38550K .......... .......... .......... .......... .......... 67%  127M 0s\n",
            " 38600K .......... .......... .......... .......... .......... 67%  147M 0s\n",
            " 38650K .......... .......... .......... .......... .......... 67% 81.7M 0s\n",
            " 38700K .......... .......... .......... .......... .......... 67%  161M 0s\n",
            " 38750K .......... .......... .......... .......... .......... 67%  151M 0s\n",
            " 38800K .......... .......... .......... .......... .......... 68%  183M 0s\n",
            " 38850K .......... .......... .......... .......... .......... 68%  194M 0s\n",
            " 38900K .......... .......... .......... .......... .......... 68%  159M 0s\n",
            " 38950K .......... .......... .......... .......... .......... 68%  161M 0s\n",
            " 39000K .......... .......... .......... .......... .......... 68%  137M 0s\n",
            " 39050K .......... .......... .......... .......... .......... 68%  101M 0s\n",
            " 39100K .......... .......... .......... .......... .......... 68%  129M 0s\n",
            " 39150K .......... .......... .......... .......... .......... 68% 77.3M 0s\n",
            " 39200K .......... .......... .......... .......... .......... 68% 79.8M 0s\n",
            " 39250K .......... .......... .......... .......... .......... 68% 91.1M 0s\n",
            " 39300K .......... .......... .......... .......... .......... 68% 95.0M 0s\n",
            " 39350K .......... .......... .......... .......... .......... 69% 62.2M 0s\n",
            " 39400K .......... .......... .......... .......... .......... 69%  146M 0s\n",
            " 39450K .......... .......... .......... .......... .......... 69%  153M 0s\n",
            " 39500K .......... .......... .......... .......... .......... 69%  172M 0s\n",
            " 39550K .......... .......... .......... .......... .......... 69%  142M 0s\n",
            " 39600K .......... .......... .......... .......... .......... 69%  170M 0s\n",
            " 39650K .......... .......... .......... .......... .......... 69%  183M 0s\n",
            " 39700K .......... .......... .......... .......... .......... 69%  182M 0s\n",
            " 39750K .......... .......... .......... .......... .......... 69%  151M 0s\n",
            " 39800K .......... .......... .......... .......... .......... 69%  176M 0s\n",
            " 39850K .......... .......... .......... .......... .......... 69%  176M 0s\n",
            " 39900K .......... .......... .......... .......... .......... 69% 94.7M 0s\n",
            " 39950K .......... .......... .......... .......... .......... 70%  117M 0s\n",
            " 40000K .......... .......... .......... .......... .......... 70%  191M 0s\n",
            " 40050K .......... .......... .......... .......... .......... 70% 99.1M 0s\n",
            " 40100K .......... .......... .......... .......... .......... 70%  162M 0s\n",
            " 40150K .......... .......... .......... .......... .......... 70%  158M 0s\n",
            " 40200K .......... .......... .......... .......... .......... 70%  174M 0s\n",
            " 40250K .......... .......... .......... .......... .......... 70%  105M 0s\n",
            " 40300K .......... .......... .......... .......... .......... 70% 92.3M 0s\n",
            " 40350K .......... .......... .......... .......... .......... 70%  137M 0s\n",
            " 40400K .......... .......... .......... .......... .......... 70%  157M 0s\n",
            " 40450K .......... .......... .......... .......... .......... 70%  166M 0s\n",
            " 40500K .......... .......... .......... .......... .......... 71%  156M 0s\n",
            " 40550K .......... .......... .......... .......... .......... 71%  153M 0s\n",
            " 40600K .......... .......... .......... .......... .......... 71%  123M 0s\n",
            " 40650K .......... .......... .......... .......... .......... 71%  139M 0s\n",
            " 40700K .......... .......... .......... .......... .......... 71%  165M 0s\n",
            " 40750K .......... .......... .......... .......... .......... 71%  126M 0s\n",
            " 40800K .......... .......... .......... .......... .......... 71%  182M 0s\n",
            " 40850K .......... .......... .......... .......... .......... 71%  169M 0s\n",
            " 40900K .......... .......... .......... .......... .......... 71%  156M 0s\n",
            " 40950K .......... .......... .......... .......... .......... 71%  161M 0s\n",
            " 41000K .......... .......... .......... .......... .......... 71%  152M 0s\n",
            " 41050K .......... .......... .......... .......... .......... 71%  108M 0s\n",
            " 41100K .......... .......... .......... .......... .......... 72%  108M 0s\n",
            " 41150K .......... .......... .......... .......... .......... 72%  127M 0s\n",
            " 41200K .......... .......... .......... .......... .......... 72%  145M 0s\n",
            " 41250K .......... .......... .......... .......... .......... 72%  169M 0s\n",
            " 41300K .......... .......... .......... .......... .......... 72% 94.6M 0s\n",
            " 41350K .......... .......... .......... .......... .......... 72%  165M 0s\n",
            " 41400K .......... .......... .......... .......... .......... 72%  183M 0s\n",
            " 41450K .......... .......... .......... .......... .......... 72%  153M 0s\n",
            " 41500K .......... .......... .......... .......... .......... 72%  179M 0s\n",
            " 41550K .......... .......... .......... .......... .......... 72%  156M 0s\n",
            " 41600K .......... .......... .......... .......... .......... 72%  194M 0s\n",
            " 41650K .......... .......... .......... .......... .......... 73%  175M 0s\n",
            " 41700K .......... .......... .......... .......... .......... 73%  165M 0s\n",
            " 41750K .......... .......... .......... .......... .......... 73% 73.7M 0s\n",
            " 41800K .......... .......... .......... .......... .......... 73%  158M 0s\n",
            " 41850K .......... .......... .......... .......... .......... 73%  150M 0s\n",
            " 41900K .......... .......... .......... .......... .......... 73% 94.1M 0s\n",
            " 41950K .......... .......... .......... .......... .......... 73%  102M 0s\n",
            " 42000K .......... .......... .......... .......... .......... 73%  184M 0s\n",
            " 42050K .......... .......... .......... .......... .......... 73%  168M 0s\n",
            " 42100K .......... .......... .......... .......... .......... 73%  131M 0s\n",
            " 42150K .......... .......... .......... .......... .......... 73%  126M 0s\n",
            " 42200K .......... .......... .......... .......... .......... 73%  124M 0s\n",
            " 42250K .......... .......... .......... .......... .......... 74%  109M 0s\n",
            " 42300K .......... .......... .......... .......... .......... 74%  140M 0s\n",
            " 42350K .......... .......... .......... .......... .......... 74% 95.7M 0s\n",
            " 42400K .......... .......... .......... .......... .......... 74%  159M 0s\n",
            " 42450K .......... .......... .......... .......... .......... 74%  180M 0s\n",
            " 42500K .......... .......... .......... .......... .......... 74%  174M 0s\n",
            " 42550K .......... .......... .......... .......... .......... 74%  162M 0s\n",
            " 42600K .......... .......... .......... .......... .......... 74%  156M 0s\n",
            " 42650K .......... .......... .......... .......... .......... 74%  183M 0s\n",
            " 42700K .......... .......... .......... .......... .......... 74%  150M 0s\n",
            " 42750K .......... .......... .......... .......... .......... 74%  100M 0s\n",
            " 42800K .......... .......... .......... .......... .......... 75%  101M 0s\n",
            " 42850K .......... .......... .......... .......... .......... 75%  128M 0s\n",
            " 42900K .......... .......... .......... .......... .......... 75%  146M 0s\n",
            " 42950K .......... .......... .......... .......... .......... 75%  154M 0s\n",
            " 43000K .......... .......... .......... .......... .......... 75%  116M 0s\n",
            " 43050K .......... .......... .......... .......... .......... 75%  164M 0s\n",
            " 43100K .......... .......... .......... .......... .......... 75%  192M 0s\n",
            " 43150K .......... .......... .......... .......... .......... 75%  128M 0s\n",
            " 43200K .......... .......... .......... .......... .......... 75%  165M 0s\n",
            " 43250K .......... .......... .......... .......... .......... 75%  178M 0s\n",
            " 43300K .......... .......... .......... .......... .......... 75%  158M 0s\n",
            " 43350K .......... .......... .......... .......... .......... 76%  146M 0s\n",
            " 43400K .......... .......... .......... .......... .......... 76%  167M 0s\n",
            " 43450K .......... .......... .......... .......... .......... 76% 86.9M 0s\n",
            " 43500K .......... .......... .......... .......... .......... 76%  172M 0s\n",
            " 43550K .......... .......... .......... .......... .......... 76%  147M 0s\n",
            " 43600K .......... .......... .......... .......... .......... 76% 97.2M 0s\n",
            " 43650K .......... .......... .......... .......... .......... 76%  155M 0s\n",
            " 43700K .......... .......... .......... .......... .......... 76%  163M 0s\n",
            " 43750K .......... .......... .......... .......... .......... 76%  145M 0s\n",
            " 43800K .......... .......... .......... .......... .......... 76%  167M 0s\n",
            " 43850K .......... .......... .......... .......... .......... 76%  141M 0s\n",
            " 43900K .......... .......... .......... .......... .......... 76%  138M 0s\n",
            " 43950K .......... .......... .......... .......... .......... 77%  133M 0s\n",
            " 44000K .......... .......... .......... .......... .......... 77%  166M 0s\n",
            " 44050K .......... .......... .......... .......... .......... 77%  153M 0s\n",
            " 44100K .......... .......... .......... .......... .......... 77%  175M 0s\n",
            " 44150K .......... .......... .......... .......... .......... 77%  144M 0s\n",
            " 44200K .......... .......... .......... .......... .......... 77%  127M 0s\n",
            " 44250K .......... .......... .......... .......... .......... 77%  109M 0s\n",
            " 44300K .......... .......... .......... .......... .......... 77%  184M 0s\n",
            " 44350K .......... .......... .......... .......... .......... 77%  143M 0s\n",
            " 44400K .......... .......... .......... .......... .......... 77%  179M 0s\n",
            " 44450K .......... .......... .......... .......... .......... 77%  127M 0s\n",
            " 44500K .......... .......... .......... .......... .......... 78% 83.7M 0s\n",
            " 44550K .......... .......... .......... .......... .......... 78% 74.2M 0s\n",
            " 44600K .......... .......... .......... .......... .......... 78% 92.8M 0s\n",
            " 44650K .......... .......... .......... .......... .......... 78%  110M 0s\n",
            " 44700K .......... .......... .......... .......... .......... 78%  108M 0s\n",
            " 44750K .......... .......... .......... .......... .......... 78%  150M 0s\n",
            " 44800K .......... .......... .......... .......... .......... 78%  167M 0s\n",
            " 44850K .......... .......... .......... .......... .......... 78%  187M 0s\n",
            " 44900K .......... .......... .......... .......... .......... 78%  175M 0s\n",
            " 44950K .......... .......... .......... .......... .......... 78%  165M 0s\n",
            " 45000K .......... .......... .......... .......... .......... 78%  202M 0s\n",
            " 45050K .......... .......... .......... .......... .......... 78%  124M 0s\n",
            " 45100K .......... .......... .......... .......... .......... 79%  139M 0s\n",
            " 45150K .......... .......... .......... .......... .......... 79% 76.4M 0s\n",
            " 45200K .......... .......... .......... .......... .......... 79%  181M 0s\n",
            " 45250K .......... .......... .......... .......... .......... 79%  112M 0s\n",
            " 45300K .......... .......... .......... .......... .......... 79%  154M 0s\n",
            " 45350K .......... .......... .......... .......... .......... 79%  151M 0s\n",
            " 45400K .......... .......... .......... .......... .......... 79%  142M 0s\n",
            " 45450K .......... .......... .......... .......... .......... 79%  176M 0s\n",
            " 45500K .......... .......... .......... .......... .......... 79%  171M 0s\n",
            " 45550K .......... .......... .......... .......... .......... 79%  145M 0s\n",
            " 45600K .......... .......... .......... .......... .......... 79%  160M 0s\n",
            " 45650K .......... .......... .......... .......... .......... 80%  165M 0s\n",
            " 45700K .......... .......... .......... .......... .......... 80%  166M 0s\n",
            " 45750K .......... .......... .......... .......... .......... 80%  163M 0s\n",
            " 45800K .......... .......... .......... .......... .......... 80%  184M 0s\n",
            " 45850K .......... .......... .......... .......... .......... 80%  167M 0s\n",
            " 45900K .......... .......... .......... .......... .......... 80%  166M 0s\n",
            " 45950K .......... .......... .......... .......... .......... 80% 98.0M 0s\n",
            " 46000K .......... .......... .......... .......... .......... 80%  135M 0s\n",
            " 46050K .......... .......... .......... .......... .......... 80%  152M 0s\n",
            " 46100K .......... .......... .......... .......... .......... 80%  163M 0s\n",
            " 46150K .......... .......... .......... .......... .......... 80%  140M 0s\n",
            " 46200K .......... .......... .......... .......... .......... 81%  164M 0s\n",
            " 46250K .......... .......... .......... .......... .......... 81%  171M 0s\n",
            " 46300K .......... .......... .......... .......... .......... 81%  135M 0s\n",
            " 46350K .......... .......... .......... .......... .......... 81% 88.6M 0s\n",
            " 46400K .......... .......... .......... .......... .......... 81%  166M 0s\n",
            " 46450K .......... .......... .......... .......... .......... 81%  125M 0s\n",
            " 46500K .......... .......... .......... .......... .......... 81% 93.0M 0s\n",
            " 46550K .......... .......... .......... .......... .......... 81% 60.6M 0s\n",
            " 46600K .......... .......... .......... .......... .......... 81% 87.1M 0s\n",
            " 46650K .......... .......... .......... .......... .......... 81%  163M 0s\n",
            " 46700K .......... .......... .......... .......... .......... 81%  172M 0s\n",
            " 46750K .......... .......... .......... .......... .......... 81%  136M 0s\n",
            " 46800K .......... .......... .......... .......... .......... 82%  118M 0s\n",
            " 46850K .......... .......... .......... .......... .......... 82%  121M 0s\n",
            " 46900K .......... .......... .......... .......... .......... 82%  164M 0s\n",
            " 46950K .......... .......... .......... .......... .......... 82%  168M 0s\n",
            " 47000K .......... .......... .......... .......... .......... 82%  109M 0s\n",
            " 47050K .......... .......... .......... .......... .......... 82%  170M 0s\n",
            " 47100K .......... .......... .......... .......... .......... 82%  121M 0s\n",
            " 47150K .......... .......... .......... .......... .......... 82%  134M 0s\n",
            " 47200K .......... .......... .......... .......... .......... 82%  177M 0s\n",
            " 47250K .......... .......... .......... .......... .......... 82%  175M 0s\n",
            " 47300K .......... .......... .......... .......... .......... 82%  179M 0s\n",
            " 47350K .......... .......... .......... .......... .......... 83%  140M 0s\n",
            " 47400K .......... .......... .......... .......... .......... 83%  167M 0s\n",
            " 47450K .......... .......... .......... .......... .......... 83%  168M 0s\n",
            " 47500K .......... .......... .......... .......... .......... 83%  155M 0s\n",
            " 47550K .......... .......... .......... .......... .......... 83%  122M 0s\n",
            " 47600K .......... .......... .......... .......... .......... 83%  115M 0s\n",
            " 47650K .......... .......... .......... .......... .......... 83%  117M 0s\n",
            " 47700K .......... .......... .......... .......... .......... 83%  129M 0s\n",
            " 47750K .......... .......... .......... .......... .......... 83%  139M 0s\n",
            " 47800K .......... .......... .......... .......... .......... 83%  150M 0s\n",
            " 47850K .......... .......... .......... .......... .......... 83%  161M 0s\n",
            " 47900K .......... .......... .......... .......... .......... 83%  174M 0s\n",
            " 47950K .......... .......... .......... .......... .......... 84%  154M 0s\n",
            " 48000K .......... .......... .......... .......... .......... 84% 97.4M 0s\n",
            " 48050K .......... .......... .......... .......... .......... 84%  149M 0s\n",
            " 48100K .......... .......... .......... .......... .......... 84%  175M 0s\n",
            " 48150K .......... .......... .......... .......... .......... 84%  157M 0s\n",
            " 48200K .......... .......... .......... .......... .......... 84%  106M 0s\n",
            " 48250K .......... .......... .......... .......... .......... 84%  149M 0s\n",
            " 48300K .......... .......... .......... .......... .......... 84%  115M 0s\n",
            " 48350K .......... .......... .......... .......... .......... 84%  113M 0s\n",
            " 48400K .......... .......... .......... .......... .......... 84%  171M 0s\n",
            " 48450K .......... .......... .......... .......... .......... 84%  190M 0s\n",
            " 48500K .......... .......... .......... .......... .......... 85%  163M 0s\n",
            " 48550K .......... .......... .......... .......... .......... 85%  160M 0s\n",
            " 48600K .......... .......... .......... .......... .......... 85%  184M 0s\n",
            " 48650K .......... .......... .......... .......... .......... 85%  152M 0s\n",
            " 48700K .......... .......... .......... .......... .......... 85%  165M 0s\n",
            " 48750K .......... .......... .......... .......... .......... 85%  147M 0s\n",
            " 48800K .......... .......... .......... .......... .......... 85%  165M 0s\n",
            " 48850K .......... .......... .......... .......... .......... 85%  183M 0s\n",
            " 48900K .......... .......... .......... .......... .......... 85%  188M 0s\n",
            " 48950K .......... .......... .......... .......... .......... 85%  148M 0s\n",
            " 49000K .......... .......... .......... .......... .......... 85% 92.1M 0s\n",
            " 49050K .......... .......... .......... .......... .......... 85%  161M 0s\n",
            " 49100K .......... .......... .......... .......... .......... 86%  148M 0s\n",
            " 49150K .......... .......... .......... .......... .......... 86% 93.3M 0s\n",
            " 49200K .......... .......... .......... .......... .......... 86% 93.2M 0s\n",
            " 49250K .......... .......... .......... .......... .......... 86%  163M 0s\n",
            " 49300K .......... .......... .......... .......... .......... 86%  165M 0s\n",
            " 49350K .......... .......... .......... .......... .......... 86% 93.5M 0s\n",
            " 49400K .......... .......... .......... .......... .......... 86% 95.3M 0s\n",
            " 49450K .......... .......... .......... .......... .......... 86%  129M 0s\n",
            " 49500K .......... .......... .......... .......... .......... 86%  162M 0s\n",
            " 49550K .......... .......... .......... .......... .......... 86%  147M 0s\n",
            " 49600K .......... .......... .......... .......... .......... 86%  193M 0s\n",
            " 49650K .......... .......... .......... .......... .......... 87%  196M 0s\n",
            " 49700K .......... .......... .......... .......... .......... 87%  205M 0s\n",
            " 49750K .......... .......... .......... .......... .......... 87%  116M 0s\n",
            " 49800K .......... .......... .......... .......... .......... 87%  113M 0s\n",
            " 49850K .......... .......... .......... .......... .......... 87%  215M 0s\n",
            " 49900K .......... .......... .......... .......... .......... 87%  204M 0s\n",
            " 49950K .......... .......... .......... .......... .......... 87%  100M 0s\n",
            " 50000K .......... .......... .......... .......... .......... 87%  143M 0s\n",
            " 50050K .......... .......... .......... .......... .......... 87%  198M 0s\n",
            " 50100K .......... .......... .......... .......... .......... 87%  125M 0s\n",
            " 50150K .......... .......... .......... .......... .......... 87% 94.1M 0s\n",
            " 50200K .......... .......... .......... .......... .......... 88%  134M 0s\n",
            " 50250K .......... .......... .......... .......... .......... 88%  204M 0s\n",
            " 50300K .......... .......... .......... .......... .......... 88%  218M 0s\n",
            " 50350K .......... .......... .......... .......... .......... 88%  141M 0s\n",
            " 50400K .......... .......... .......... .......... .......... 88%  183M 0s\n",
            " 50450K .......... .......... .......... .......... .......... 88%  198M 0s\n",
            " 50500K .......... .......... .......... .......... .......... 88%  184M 0s\n",
            " 50550K .......... .......... .......... .......... .......... 88%  180M 0s\n",
            " 50600K .......... .......... .......... .......... .......... 88%  229M 0s\n",
            " 50650K .......... .......... .......... .......... .......... 88%  228M 0s\n",
            " 50700K .......... .......... .......... .......... .......... 88%  234M 0s\n",
            " 50750K .......... .......... .......... .......... .......... 88%  157M 0s\n",
            " 50800K .......... .......... .......... .......... .......... 89%  199M 0s\n",
            " 50850K .......... .......... .......... .......... .......... 89%  118M 0s\n",
            " 50900K .......... .......... .......... .......... .......... 89%  178M 0s\n",
            " 50950K .......... .......... .......... .......... .......... 89%  153M 0s\n",
            " 51000K .......... .......... .......... .......... .......... 89%  192M 0s\n",
            " 51050K .......... .......... .......... .......... .......... 89%  192M 0s\n",
            " 51100K .......... .......... .......... .......... .......... 89%  147M 0s\n",
            " 51150K .......... .......... .......... .......... .......... 89%  149M 0s\n",
            " 51200K .......... .......... .......... .......... .......... 89%  207M 0s\n",
            " 51250K .......... .......... .......... .......... .......... 89%  108M 0s\n",
            " 51300K .......... .......... .......... .......... .......... 89%  124M 0s\n",
            " 51350K .......... .......... .......... .......... .......... 90%  164M 0s\n",
            " 51400K .......... .......... .......... .......... .......... 90%  102M 0s\n",
            " 51450K .......... .......... .......... .......... .......... 90%  196M 0s\n",
            " 51500K .......... .......... .......... .......... .......... 90%  195M 0s\n",
            " 51550K .......... .......... .......... .......... .......... 90%  162M 0s\n",
            " 51600K .......... .......... .......... .......... .......... 90%  235M 0s\n",
            " 51650K .......... .......... .......... .......... .......... 90%  191M 0s\n",
            " 51700K .......... .......... .......... .......... .......... 90% 89.5M 0s\n",
            " 51750K .......... .......... .......... .......... .......... 90%  145M 0s\n",
            " 51800K .......... .......... .......... .......... .......... 90%  120M 0s\n",
            " 51850K .......... .......... .......... .......... .......... 90%  194M 0s\n",
            " 51900K .......... .......... .......... .......... .......... 90%  209M 0s\n",
            " 51950K .......... .......... .......... .......... .......... 91%  155M 0s\n",
            " 52000K .......... .......... .......... .......... .......... 91%  188M 0s\n",
            " 52050K .......... .......... .......... .......... .......... 91%  159M 0s\n",
            " 52100K .......... .......... .......... .......... .......... 91%  188M 0s\n",
            " 52150K .......... .......... .......... .......... .......... 91%  142M 0s\n",
            " 52200K .......... .......... .......... .......... .......... 91%  154M 0s\n",
            " 52250K .......... .......... .......... .......... .......... 91%  180M 0s\n",
            " 52300K .......... .......... .......... .......... .......... 91%  193M 0s\n",
            " 52350K .......... .......... .......... .......... .......... 91%  153M 0s\n",
            " 52400K .......... .......... .......... .......... .......... 91% 89.4M 0s\n",
            " 52450K .......... .......... .......... .......... .......... 91% 71.9M 0s\n",
            " 52500K .......... .......... .......... .......... .......... 92%  113M 0s\n",
            " 52550K .......... .......... .......... .......... .......... 92%  120M 0s\n",
            " 52600K .......... .......... .......... .......... .......... 92%  150M 0s\n",
            " 52650K .......... .......... .......... .......... .......... 92%  124M 0s\n",
            " 52700K .......... .......... .......... .......... .......... 92% 83.0M 0s\n",
            " 52750K .......... .......... .......... .......... .......... 92%  115M 0s\n",
            " 52800K .......... .......... .......... .......... .......... 92%  170M 0s\n",
            " 52850K .......... .......... .......... .......... .......... 92%  169M 0s\n",
            " 52900K .......... .......... .......... .......... .......... 92%  190M 0s\n",
            " 52950K .......... .......... .......... .......... .......... 92%  147M 0s\n",
            " 53000K .......... .......... .......... .......... .......... 92%  122M 0s\n",
            " 53050K .......... .......... .......... .......... .......... 92%  169M 0s\n",
            " 53100K .......... .......... .......... .......... .......... 93%  174M 0s\n",
            " 53150K .......... .......... .......... .......... .......... 93%  141M 0s\n",
            " 53200K .......... .......... .......... .......... .......... 93% 97.2M 0s\n",
            " 53250K .......... .......... .......... .......... .......... 93%  126M 0s\n",
            " 53300K .......... .......... .......... .......... .......... 93%  174M 0s\n",
            " 53350K .......... .......... .......... .......... .......... 93%  150M 0s\n",
            " 53400K .......... .......... .......... .......... .......... 93%  100M 0s\n",
            " 53450K .......... .......... .......... .......... .......... 93%  120M 0s\n",
            " 53500K .......... .......... .......... .......... .......... 93%  172M 0s\n",
            " 53550K .......... .......... .......... .......... .......... 93%  140M 0s\n",
            " 53600K .......... .......... .......... .......... .......... 93%  162M 0s\n",
            " 53650K .......... .......... .......... .......... .......... 94%  182M 0s\n",
            " 53700K .......... .......... .......... .......... .......... 94%  169M 0s\n",
            " 53750K .......... .......... .......... .......... .......... 94%  155M 0s\n",
            " 53800K .......... .......... .......... .......... .......... 94%  159M 0s\n",
            " 53850K .......... .......... .......... .......... .......... 94%  177M 0s\n",
            " 53900K .......... .......... .......... .......... .......... 94%  175M 0s\n",
            " 53950K .......... .......... .......... .......... .......... 94%  137M 0s\n",
            " 54000K .......... .......... .......... .......... .......... 94%  153M 0s\n",
            " 54050K .......... .......... .......... .......... .......... 94%  166M 0s\n",
            " 54100K .......... .......... .......... .......... .......... 94% 72.2M 0s\n",
            " 54150K .......... .......... .......... .......... .......... 94%  140M 0s\n",
            " 54200K .......... .......... .......... .......... .......... 95%  190M 0s\n",
            " 54250K .......... .......... .......... .......... .......... 95%  189M 0s\n",
            " 54300K .......... .......... .......... .......... .......... 95%  160M 0s\n",
            " 54350K .......... .......... .......... .......... .......... 95%  140M 0s\n",
            " 54400K .......... .......... .......... .......... .......... 95%  165M 0s\n",
            " 54450K .......... .......... .......... .......... .......... 95%  172M 0s\n",
            " 54500K .......... .......... .......... .......... .......... 95%  197M 0s\n",
            " 54550K .......... .......... .......... .......... .......... 95%  169M 0s\n",
            " 54600K .......... .......... .......... .......... .......... 95%  201M 0s\n",
            " 54650K .......... .......... .......... .......... .......... 95%  186M 0s\n",
            " 54700K .......... .......... .......... .......... .......... 95%  193M 0s\n",
            " 54750K .......... .......... .......... .......... .......... 95% 93.4M 0s\n",
            " 54800K .......... .......... .......... .......... .......... 96% 89.2M 0s\n",
            " 54850K .......... .......... .......... .......... .......... 96%  170M 0s\n",
            " 54900K .......... .......... .......... .......... .......... 96%  189M 0s\n",
            " 54950K .......... .......... .......... .......... .......... 96%  153M 0s\n",
            " 55000K .......... .......... .......... .......... .......... 96%  193M 0s\n",
            " 55050K .......... .......... .......... .......... .......... 96%  181M 0s\n",
            " 55100K .......... .......... .......... .......... .......... 96%  164M 0s\n",
            " 55150K .......... .......... .......... .......... .......... 96%  160M 0s\n",
            " 55200K .......... .......... .......... .......... .......... 96%  183M 0s\n",
            " 55250K .......... .......... .......... .......... .......... 96%  174M 0s\n",
            " 55300K .......... .......... .......... .......... .......... 96%  177M 0s\n",
            " 55350K .......... .......... .......... .......... .......... 97%  157M 0s\n",
            " 55400K .......... .......... .......... .......... .......... 97%  201M 0s\n",
            " 55450K .......... .......... .......... .......... .......... 97%  183M 0s\n",
            " 55500K .......... .......... .......... .......... .......... 97%  185M 0s\n",
            " 55550K .......... .......... .......... .......... .......... 97%  144M 0s\n",
            " 55600K .......... .......... .......... .......... .......... 97%  189M 0s\n",
            " 55650K .......... .......... .......... .......... .......... 97%  136M 0s\n",
            " 55700K .......... .......... .......... .......... .......... 97% 84.5M 0s\n",
            " 55750K .......... .......... .......... .......... .......... 97% 76.5M 0s\n",
            " 55800K .......... .......... .......... .......... .......... 97%  140M 0s\n",
            " 55850K .......... .......... .......... .......... .......... 97%  152M 0s\n",
            " 55900K .......... .......... .......... .......... .......... 97%  145M 0s\n",
            " 55950K .......... .......... .......... .......... .......... 98%  127M 0s\n",
            " 56000K .......... .......... .......... .......... .......... 98%  165M 0s\n",
            " 56050K .......... .......... .......... .......... .......... 98%  162M 0s\n",
            " 56100K .......... .......... .......... .......... .......... 98%  172M 0s\n",
            " 56150K .......... .......... .......... .......... .......... 98%  137M 0s\n",
            " 56200K .......... .......... .......... .......... .......... 98%  169M 0s\n",
            " 56250K .......... .......... .......... .......... .......... 98%  158M 0s\n",
            " 56300K .......... .......... .......... .......... .......... 98%  149M 0s\n",
            " 56350K .......... .......... .......... .......... .......... 98%  141M 0s\n",
            " 56400K .......... .......... .......... .......... .......... 98%  167M 0s\n",
            " 56450K .......... .......... .......... .......... .......... 98%  173M 0s\n",
            " 56500K .......... .......... .......... .......... .......... 99%  178M 0s\n",
            " 56550K .......... .......... .......... .......... .......... 99%  160M 0s\n",
            " 56600K .......... .......... .......... .......... .......... 99%  155M 0s\n",
            " 56650K .......... .......... .......... .......... .......... 99%  124M 0s\n",
            " 56700K .......... .......... .......... .......... .......... 99%  168M 0s\n",
            " 56750K .......... .......... .......... .......... .......... 99%  138M 0s\n",
            " 56800K .......... .......... .......... .......... .......... 99%  177M 0s\n",
            " 56850K .......... .......... .......... .......... .......... 99%  169M 0s\n",
            " 56900K .......... .......... .......... .......... .......... 99%  174M 0s\n",
            " 56950K .......... .......... .......... .......... .......... 99%  147M 0s\n",
            " 57000K .......... .......... .......... .......... .......... 99%  175M 0s\n",
            " 57050K .......... .......... .......... .......... ........  100%  154M=0.5s\n",
            "\n",
            "2021-03-11 11:27:02 (119 MB/s) - ‘Miniconda3-4.5.4-Linux-x86_64.sh’ saved [58468498/58468498]\n",
            "\n",
            "Python 3.6.5 :: Anaconda, Inc.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imA_NQjLUQBl",
        "outputId": "16da4c35-0563-4d81-ebc8-63ccc792a63a"
      },
      "source": [
        "!which conda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/bin/conda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEqGrUdzUWKg",
        "outputId": "cfa0cb49-7238-48be-a220-ebe6b779f031"
      },
      "source": [
        "!conda --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conda 4.5.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bISk7wp2UYkP",
        "outputId": "b38c3885-4abe-4aeb-e84b-56a4c0dbfe83"
      },
      "source": [
        "%%bash\n",
        "\n",
        "conda install --channel defaults conda python=3.6 --yes\n",
        "conda update --channel defaults --all --yes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - conda\n",
            "    - python=3.6\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    tqdm-4.56.0                |     pyhd3eb1b0_0          76 KB\n",
            "    libffi-3.3                 |       he6710b0_2          54 KB\n",
            "    sqlite-3.33.0              |       h62c20be_0         2.0 MB\n",
            "    ncurses-6.2                |       he6710b0_1         1.1 MB\n",
            "    idna-2.10                  |     pyhd3eb1b0_0          52 KB\n",
            "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
            "    chardet-4.0.0              |py36h06a4308_1003         213 KB\n",
            "    six-1.15.0                 |     pyhd3eb1b0_0          13 KB\n",
            "    setuptools-52.0.0          |   py36h06a4308_0         933 KB\n",
            "    pycosat-0.6.3              |   py36h27cfd23_0         107 KB\n",
            "    readline-8.1               |       h27cfd23_0         464 KB\n",
            "    requests-2.25.1            |     pyhd3eb1b0_0          51 KB\n",
            "    certifi-2020.12.5          |   py36h06a4308_0         144 KB\n",
            "    libstdcxx-ng-9.1.0         |       hdf63c60_0         4.0 MB\n",
            "    brotlipy-0.7.0             |py36h27cfd23_1003         349 KB\n",
            "    zlib-1.2.11                |       h7b6447c_3         120 KB\n",
            "    conda-package-handling-1.7.2|   py36h03888b9_0         967 KB\n",
            "    cryptography-3.3.1         |   py36h3c74f83_1         633 KB\n",
            "    pyopenssl-20.0.1           |     pyhd3eb1b0_1          48 KB\n",
            "    wheel-0.36.2               |     pyhd3eb1b0_0          31 KB\n",
            "    tk-8.6.10                  |       hbc83047_0         3.2 MB\n",
            "    openssl-1.1.1j             |       h27cfd23_0         3.8 MB\n",
            "    python-3.6.13              |       hdb3f193_0        33.9 MB\n",
            "    ruamel_yaml-0.15.87        |   py36h7b6447c_1         256 KB\n",
            "    conda-4.9.2                |   py36h06a4308_0         3.1 MB\n",
            "    cffi-1.14.5                |   py36h261ae71_0         224 KB\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    urllib3-1.26.3             |     pyhd3eb1b0_0          99 KB\n",
            "    libedit-3.1.20191231       |       h14c3975_1         121 KB\n",
            "    pycparser-2.20             |             py_2          94 KB\n",
            "    pysocks-1.7.1              |   py36h06a4308_0          30 KB\n",
            "    yaml-0.2.5                 |       h7b6447c_0          87 KB\n",
            "    ca-certificates-2021.1.19  |       h06a4308_1         125 KB\n",
            "    pip-21.0.1                 |   py36h06a4308_0         2.0 MB\n",
            "    ld_impl_linux-64-2.33.1    |       h53a641e_7         645 KB\n",
            "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        67.5 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:          0.1-main               \n",
            "    brotlipy:               0.7.0-py36h27cfd23_1003\n",
            "    conda-package-handling: 1.7.2-py36h03888b9_0   \n",
            "    ld_impl_linux-64:       2.33.1-h53a641e_7      \n",
            "    tqdm:                   4.56.0-pyhd3eb1b0_0    \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates:        2018.03.07-0            --> 2021.1.19-h06a4308_1    \n",
            "    certifi:                2018.4.16-py36_0        --> 2020.12.5-py36h06a4308_0\n",
            "    cffi:                   1.11.5-py36h9745a5d_0   --> 1.14.5-py36h261ae71_0   \n",
            "    chardet:                3.0.4-py36h0f667ec_1    --> 4.0.0-py36h06a4308_1003 \n",
            "    conda:                  4.5.4-py36_0            --> 4.9.2-py36h06a4308_0    \n",
            "    cryptography:           2.2.2-py36h14c3975_0    --> 3.3.1-py36h3c74f83_1    \n",
            "    idna:                   2.6-py36h82fb2a8_1      --> 2.10-pyhd3eb1b0_0       \n",
            "    libedit:                3.1.20170329-h6b74fdf_2 --> 3.1.20191231-h14c3975_1 \n",
            "    libffi:                 3.2.1-hd88cf55_4        --> 3.3-he6710b0_2          \n",
            "    libgcc-ng:              7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0        \n",
            "    libstdcxx-ng:           7.2.0-hdf63c60_3        --> 9.1.0-hdf63c60_0        \n",
            "    ncurses:                6.1-hf484d3e_0          --> 6.2-he6710b0_1          \n",
            "    openssl:                1.0.2o-h20670df_0       --> 1.1.1j-h27cfd23_0       \n",
            "    pip:                    10.0.1-py36_0           --> 21.0.1-py36h06a4308_0   \n",
            "    pycosat:                0.6.3-py36h0a5515d_0    --> 0.6.3-py36h27cfd23_0    \n",
            "    pycparser:              2.18-py36hf9f622e_1     --> 2.20-py_2               \n",
            "    pyopenssl:              18.0.0-py36_0           --> 20.0.1-pyhd3eb1b0_1     \n",
            "    pysocks:                1.6.8-py36_0            --> 1.7.1-py36h06a4308_0    \n",
            "    python:                 3.6.5-hc3d631a_2        --> 3.6.13-hdb3f193_0       \n",
            "    readline:               7.0-ha6073c6_4          --> 8.1-h27cfd23_0          \n",
            "    requests:               2.18.4-py36he2e5f8d_1   --> 2.25.1-pyhd3eb1b0_0     \n",
            "    ruamel_yaml:            0.15.37-py36h14c3975_2  --> 0.15.87-py36h7b6447c_1  \n",
            "    setuptools:             39.2.0-py36_0           --> 52.0.0-py36h06a4308_0   \n",
            "    six:                    1.11.0-py36h372c433_1   --> 1.15.0-pyhd3eb1b0_0     \n",
            "    sqlite:                 3.23.1-he433501_0       --> 3.33.0-h62c20be_0       \n",
            "    tk:                     8.6.7-hc745277_3        --> 8.6.10-hbc83047_0       \n",
            "    urllib3:                1.22-py36hbe7ace6_0     --> 1.26.3-pyhd3eb1b0_0     \n",
            "    wheel:                  0.31.1-py36_0           --> 0.36.2-pyhd3eb1b0_0     \n",
            "    xz:                     5.2.4-h14c3975_4        --> 5.2.5-h7b6447c_0        \n",
            "    yaml:                   0.1.7-had09818_2        --> 0.2.5-h7b6447c_0        \n",
            "    zlib:                   1.2.11-ha838bed_2       --> 1.2.11-h7b6447c_3       \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    six-1.15.0                 |   py36h06a4308_0          27 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:          27 KB\n",
            "\n",
            "The following packages will be REMOVED:\n",
            "\n",
            "  asn1crypto-0.24.0-py36_0\n",
            "  conda-env-2.6.0-h36134e3_1\n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  six                pkgs/main/noarch::six-1.15.0-pyhd3eb1~ --> pkgs/main/linux-64::six-1.15.0-py36h06a4308_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\rsix-1.15.0           | 27 KB     |            |   0% \rsix-1.15.0           | 27 KB     | ########## | 100% \rsix-1.15.0           | 27 KB     | ########## | 100% \n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtqdm-4.56.0          |   76 KB |            |   0% \rtqdm-4.56.0          |   76 KB | ########## | 100% \n",
            "\rlibffi-3.3           |   54 KB |            |   0% \rlibffi-3.3           |   54 KB | ########## | 100% \n",
            "\rsqlite-3.33.0        |  2.0 MB |            |   0% \rsqlite-3.33.0        |  2.0 MB | ########   |  80% \rsqlite-3.33.0        |  2.0 MB | ########## | 100% \n",
            "\rncurses-6.2          |  1.1 MB |            |   0% \rncurses-6.2          |  1.1 MB | #######8   |  78% \rncurses-6.2          |  1.1 MB | #########7 |  98% \rncurses-6.2          |  1.1 MB | ########## | 100% \n",
            "\ridna-2.10            |   52 KB |            |   0% \ridna-2.10            |   52 KB | ########## | 100% \n",
            "\rxz-5.2.5             |  438 KB |            |   0% \rxz-5.2.5             |  438 KB | #########7 |  98% \rxz-5.2.5             |  438 KB | ########## | 100% \n",
            "\rchardet-4.0.0        |  213 KB |            |   0% \rchardet-4.0.0        |  213 KB | ########## | 100% \n",
            "\rsix-1.15.0           |   13 KB |            |   0% \rsix-1.15.0           |   13 KB | ########## | 100% \n",
            "\rsetuptools-52.0.0    |  933 KB |            |   0% \rsetuptools-52.0.0    |  933 KB | ########1  |  81% \rsetuptools-52.0.0    |  933 KB | ########## | 100% \n",
            "\rpycosat-0.6.3        |  107 KB |            |   0% \rpycosat-0.6.3        |  107 KB | ########## | 100% \n",
            "\rreadline-8.1         |  464 KB |            |   0% \rreadline-8.1         |  464 KB | ########## | 100% \n",
            "\rrequests-2.25.1      |   51 KB |            |   0% \rrequests-2.25.1      |   51 KB | ########## | 100% \n",
            "\rcertifi-2020.12.5    |  144 KB |            |   0% \rcertifi-2020.12.5    |  144 KB | ########## | 100% \n",
            "\rlibstdcxx-ng-9.1.0   |  4.0 MB |            |   0% \rlibstdcxx-ng-9.1.0   |  4.0 MB | #######6   |  77% \rlibstdcxx-ng-9.1.0   |  4.0 MB | #########8 |  99% \rlibstdcxx-ng-9.1.0   |  4.0 MB | ########## | 100% \n",
            "\rbrotlipy-0.7.0       |  349 KB |            |   0% \rbrotlipy-0.7.0       |  349 KB | ########## | 100% \n",
            "\rzlib-1.2.11          |  120 KB |            |   0% \rzlib-1.2.11          |  120 KB | ########## | 100% \n",
            "\rconda-package-handli |  967 KB |            |   0% \rconda-package-handli |  967 KB | #########  |  91% \rconda-package-handli |  967 KB | ########## | 100% \n",
            "\rcryptography-3.3.1   |  633 KB |            |   0% \rcryptography-3.3.1   |  633 KB | #########  |  90% \rcryptography-3.3.1   |  633 KB | ########## | 100% \n",
            "\rpyopenssl-20.0.1     |   48 KB |            |   0% \rpyopenssl-20.0.1     |   48 KB | ########## | 100% \n",
            "\rwheel-0.36.2         |   31 KB |            |   0% \rwheel-0.36.2         |   31 KB | ########## | 100% \n",
            "\rtk-8.6.10            |  3.2 MB |            |   0% \rtk-8.6.10            |  3.2 MB | #######8   |  78% \rtk-8.6.10            |  3.2 MB | ########## | 100% \n",
            "\ropenssl-1.1.1j       |  3.8 MB |            |   0% \ropenssl-1.1.1j       |  3.8 MB | #######6   |  76% \ropenssl-1.1.1j       |  3.8 MB | #########4 |  94% \ropenssl-1.1.1j       |  3.8 MB | ########## | 100% \n",
            "\rpython-3.6.13        | 33.9 MB |            |   0% \rpython-3.6.13        | 33.9 MB | ##         |  21% \rpython-3.6.13        | 33.9 MB | #####      |  50% \rpython-3.6.13        | 33.9 MB | #######5   |  75% \rpython-3.6.13        | 33.9 MB | #########1 |  92% \rpython-3.6.13        | 33.9 MB | ########## | 100% \n",
            "\rruamel_yaml-0.15.87  |  256 KB |            |   0% \rruamel_yaml-0.15.87  |  256 KB | ########## | 100% \n",
            "\rconda-4.9.2          |  3.1 MB |            |   0% \rconda-4.9.2          |  3.1 MB | #######9   |  79% \rconda-4.9.2          |  3.1 MB | #########3 |  93% \rconda-4.9.2          |  3.1 MB | ########## | 100% \n",
            "\rcffi-1.14.5          |  224 KB |            |   0% \rcffi-1.14.5          |  224 KB | ########## | 100% \n",
            "\r_libgcc_mutex-0.1    |    3 KB |            |   0% \r_libgcc_mutex-0.1    |    3 KB | ########## | 100% \n",
            "\rurllib3-1.26.3       |   99 KB |            |   0% \rurllib3-1.26.3       |   99 KB | ########## | 100% \n",
            "\rlibedit-3.1.20191231 |  121 KB |            |   0% \rlibedit-3.1.20191231 |  121 KB | ########## | 100% \n",
            "\rpycparser-2.20       |   94 KB |            |   0% \rpycparser-2.20       |   94 KB | ########## | 100% \n",
            "\rpysocks-1.7.1        |   30 KB |            |   0% \rpysocks-1.7.1        |   30 KB | ########## | 100% \n",
            "\ryaml-0.2.5           |   87 KB |            |   0% \ryaml-0.2.5           |   87 KB | ########## | 100% \n",
            "\rca-certificates-2021 |  125 KB |            |   0% \rca-certificates-2021 |  125 KB | ########## | 100% \n",
            "\rpip-21.0.1           |  2.0 MB |            |   0% \rpip-21.0.1           |  2.0 MB | #######8   |  79% \rpip-21.0.1           |  2.0 MB | #########4 |  94% \rpip-21.0.1           |  2.0 MB | ########## | 100% \n",
            "\rld_impl_linux-64-2.3 |  645 KB |            |   0% \rld_impl_linux-64-2.3 |  645 KB | #########6 |  96% \rld_impl_linux-64-2.3 |  645 KB | ########## | 100% \n",
            "\rlibgcc-ng-9.1.0      |  8.1 MB |            |   0% \rlibgcc-ng-9.1.0      |  8.1 MB | #######5   |  75% \rlibgcc-ng-9.1.0      |  8.1 MB | #########8 |  98% \rlibgcc-ng-9.1.0      |  8.1 MB | ########## | 100% \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yzco20wxVIYP"
      },
      "source": [
        "# !conda --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBHVr-j_VaYt"
      },
      "source": [
        "# !python --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrUuWNc0VeLq",
        "outputId": "f5faf1e8-3353-4234-9726-d8afca7c0dde"
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AcLTrOrVjAp"
      },
      "source": [
        "\n",
        "import sys\n",
        "\n",
        "_ = (sys.path\n",
        "        .append(\"/usr/local/lib/python3.6/site-packages\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBPH7UboVpfN",
        "outputId": "481119df-02f5-4ff9-a109-54e2877e06c5"
      },
      "source": [
        "sys.path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython',\n",
              " '/usr/local/lib/python3.6/site-packages']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOEcpwZqVsQb"
      },
      "source": [
        "# !conda install --channel conda-forge featuretools --yes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-y4ba7fcEsg"
      },
      "source": [
        "# !conda init bash\n",
        "# !exec bash"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YrxDR14cT_I"
      },
      "source": [
        "# %%shell\n",
        "\n",
        "# eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
        "# conda activate prakhar\n",
        "# conda info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9P8WbA_AfhO2",
        "outputId": "9f9fb137-c6c7-430b-c7c6-9bfea8508905"
      },
      "source": [
        "!git clone https://github.com/HHousen/transformersum.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformersum'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 1282 (delta 24), reused 36 (delta 16), pack-reused 1232\u001b[K\n",
            "Receiving objects: 100% (1282/1282), 11.43 MiB | 30.88 MiB/s, done.\n",
            "Resolving deltas: 100% (808/808), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "97FuxfbqgTiX",
        "outputId": "5e4ae30c-6dd3-4b4b-8277-0af5f4883529"
      },
      "source": [
        "!cd transformersum && conda env create --file environment.yml"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "krb5-1.17.2          | 1.4 MB    | : 100% 1.0/1 [00:00<00:00,  2.62it/s]\n",
            "orc-1.6.7            | 750 KB    | : 100% 1.0/1 [00:00<00:00,  6.65it/s]\n",
            "sphinxcontrib-htmlhe | 27 KB     | : 100% 1.0/1 [00:00<00:00, 16.01it/s]\n",
            "cymem-2.0.5          | 41 KB     | : 100% 1.0/1 [00:00<00:00, 21.84it/s]\n",
            "smart_open-2.2.1     | 78 KB     | : 100% 1.0/1 [00:00<00:00, 14.00it/s]\n",
            "bzip2-1.0.8          | 484 KB    | : 100% 1.0/1 [00:00<00:00,  8.48it/s]\n",
            "aws-checksums-0.1.11 | 50 KB     | : 100% 1.0/1 [00:00<00:00, 23.12it/s]\n",
            "spacy-legacy-3.0.1   | 10 KB     | : 100% 1.0/1 [00:00<00:00, 23.74it/s]\n",
            "threadpoolctl-2.1.0  | 15 KB     | : 100% 1.0/1 [00:00<00:00, 25.77it/s]\n",
            "sphinxcontrib-appleh | 28 KB     | : 100% 1.0/1 [00:00<00:00, 17.99it/s]\n",
            "murmurhash-1.0.5     | 26 KB     | : 100% 1.0/1 [00:00<00:00, 22.46it/s]\n",
            "arrow-cpp-3.0.0      | 20.9 MB   | : 100% 1.0/1 [00:03<00:00,  3.27s/it]               \n",
            "libssh2-1.9.0        | 226 KB    | : 100% 1.0/1 [00:00<00:00,  9.49it/s]\n",
            "aws-sdk-cpp-1.8.151  | 4.5 MB    | : 100% 1.0/1 [00:01<00:00,  1.41s/it]               \n",
            "libblas-3.9.0        | 12 KB     | : 100% 1.0/1 [00:00<00:00, 24.67it/s]\n",
            "libcblas-3.9.0       | 11 KB     | : 100% 1.0/1 [00:00<00:00, 24.86it/s]\n",
            "typing-extensions-3. | 8 KB      | : 100% 1.0/1 [00:00<00:00, 32.30it/s]\n",
            "xz-5.2.5             | 343 KB    | : 100% 1.0/1 [00:00<00:00, 10.32it/s]\n",
            "boto-2.49.0          | 838 KB    | : 100% 1.0/1 [00:00<00:00,  4.39it/s]\n",
            "pip-21.0.1           | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.72it/s]\n",
            "abseil-cpp-20200923. | 955 KB    | : 100% 1.0/1 [00:00<00:00,  4.39it/s]\n",
            "ca-certificates-2020 | 137 KB    | : 100% 1.0/1 [00:00<00:00, 20.73it/s]\n",
            "aws-c-event-stream-0 | 47 KB     | : 100% 1.0/1 [00:00<00:00, 24.78it/s]\n",
            "alabaster-0.7.12     | 15 KB     | : 100% 1.0/1 [00:00<00:00, 27.90it/s]\n",
            "tk-8.6.10            | 3.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.93it/s]\n",
            "cachetools-4.2.1     | 13 KB     | : 100% 1.0/1 [00:00<00:00, 30.88it/s]\n",
            "tensorboard-plugin-w | 670 KB    | : 100% 1.0/1 [00:00<00:00,  6.88it/s]\n",
            "glog-0.4.0           | 104 KB    | : 100% 1.0/1 [00:00<00:00, 21.64it/s]\n",
            "markdown-3.3.4       | 67 KB     | : 100% 1.0/1 [00:00<00:00, 22.01it/s]\n",
            "blinker-1.4          | 13 KB     | : 100% 1.0/1 [00:00<00:00, 26.12it/s]\n",
            "certifi-2020.12.5    | 143 KB    | : 100% 1.0/1 [00:00<00:00, 20.33it/s]\n",
            "python-dateutil-2.8. | 220 KB    | : 100% 1.0/1 [00:00<00:00, 16.60it/s]\n",
            "protobuf-3.15.5      | 348 KB    | : 100% 1.0/1 [00:00<00:00,  8.18it/s]\n",
            "libevent-2.1.10      | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.75it/s]\n",
            "scikit-learn-0.24.1  | 7.6 MB    | : 100% 1.0/1 [00:01<00:00,  1.42s/it]\n",
            "yarl-1.5.1           | 141 KB    | : 100% 1.0/1 [00:00<00:00, 15.05it/s]\n",
            "libstdcxx-ng-9.3.0   | 4.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.73it/s]\n",
            "google-resumable-med | 40 KB     | : 100% 1.0/1 [00:00<00:00, 22.09it/s]\n",
            "c-ares-1.17.1        | 109 KB    | : 100% 1.0/1 [00:00<00:00, 16.98it/s]\n",
            "pycparser-2.20       | 94 KB     | : 100% 1.0/1 [00:00<00:00, 14.44it/s]\n",
            "liblapack-3.9.0      | 11 KB     | : 100% 1.0/1 [00:00<00:00, 24.13it/s]\n",
            "sqlite-3.34.0        | 1.4 MB    | : 100% 1.0/1 [00:00<00:00,  4.83it/s]\n",
            "pyasn1-0.4.8         | 53 KB     | : 100% 1.0/1 [00:00<00:00, 19.03it/s]\n",
            "gflags-2.2.2         | 114 KB    | : 100% 1.0/1 [00:00<00:00, 18.04it/s]\n",
            "multidict-5.1.0      | 68 KB     | : 100% 1.0/1 [00:00<00:00, 20.70it/s]\n",
            "pyparsing-2.4.7      | 60 KB     | : 100% 1.0/1 [00:00<00:00, 23.92it/s]\n",
            "zipp-3.4.1           | 11 KB     | : 100% 1.0/1 [00:00<00:00, 29.43it/s]\n",
            "brotli-1.0.9         | 389 KB    | : 100% 1.0/1 [00:00<00:00, 10.77it/s]\n",
            "async-timeout-3.0.1  | 11 KB     | : 100% 1.0/1 [00:00<00:00, 31.01it/s]\n",
            "idna-2.10            | 52 KB     | : 100% 1.0/1 [00:00<00:00, 25.02it/s]\n",
            "aiohttp-3.7.4        | 646 KB    | : 100% 1.0/1 [00:00<00:00,  6.10it/s]\n",
            "libgcc-ng-9.3.0      | 7.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.05s/it]\n",
            "wasabi-0.8.2         | 23 KB     | : 100% 1.0/1 [00:00<00:00, 28.77it/s]\n",
            "imagesize-1.2.0      | 8 KB      | : 100% 1.0/1 [00:00<00:00, 28.51it/s]\n",
            "readline-8.0         | 281 KB    | : 100% 1.0/1 [00:00<00:00, 11.94it/s]\n",
            "sphinxcontrib-devhel | 22 KB     | : 100% 1.0/1 [00:00<00:00, 17.46it/s]\n",
            "tzdata-2021a         | 121 KB    | : 100% 1.0/1 [00:00<00:00,  9.70it/s]\n",
            "sphinxcontrib-serial | 24 KB     | : 100% 1.0/1 [00:00<00:00, 18.62it/s]\n",
            "catalogue-2.0.1      | 17 KB     | : 100% 1.0/1 [00:00<00:00, 20.39it/s]\n",
            "brotlipy-0.7.0       | 341 KB    | : 100% 1.0/1 [00:00<00:00, 13.46it/s]\n",
            "re2-2020.11.01       | 226 KB    | : 100% 1.0/1 [00:00<00:00, 16.64it/s]\n",
            "jinja2-2.11.3        | 93 KB     | : 100% 1.0/1 [00:00<00:00, 18.97it/s]\n",
            "click-7.1.2          | 64 KB     | : 100% 1.0/1 [00:00<00:00, 18.66it/s]\n",
            "babel-2.9.0          | 6.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.15s/it]\n",
            "absl-py-0.12.0       | 96 KB     | : 100% 1.0/1 [00:00<00:00, 16.07it/s]\n",
            "requests-oauthlib-1. | 21 KB     | : 100% 1.0/1 [00:00<00:00, 27.54it/s]\n",
            "libthrift-0.14.1     | 4.5 MB    | : 100% 1.0/1 [00:00<00:00,  1.39it/s]\n",
            "libprotobuf-3.15.5   | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  2.06it/s]\n",
            "tensorboard-2.4.1    | 8.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.27s/it]               \n",
            "aws-c-common-0.5.2   | 165 KB    | : 100% 1.0/1 [00:00<00:00, 14.13it/s]\n",
            "google-cloud-storage | 66 KB     | : 100% 1.0/1 [00:00<00:00, 19.41it/s]\n",
            "parquet-cpp-1.5.1    | 3 KB      | : 100% 1.0/1 [00:00<00:00, 30.17it/s]\n",
            "pysocks-1.7.1        | 28 KB     | : 100% 1.0/1 [00:00<00:00, 27.73it/s]\n",
            "cython-blis-0.7.4    | 3.9 MB    | : 100% 1.0/1 [00:00<00:00,  1.55it/s]\n",
            "six-1.15.0           | 14 KB     | : 100% 1.0/1 [00:00<00:00, 23.23it/s]\n",
            "sleef-3.5.1          | 1.5 MB    | : 100% 1.0/1 [00:00<00:00,  3.34it/s]\n",
            "aws-c-cal-0.4.5      | 35 KB     | : 100% 1.0/1 [00:00<00:00, 20.23it/s]\n",
            "snowballstemmer-2.1. | 57 KB     | : 100% 1.0/1 [00:00<00:00, 16.97it/s]\n",
            "libgfortran5-9.3.0   | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.60it/s]\n",
            "google-api-core-1.25 | 58 KB     | : 100% 1.0/1 [00:00<00:00, 21.47it/s]\n",
            "attrs-20.3.0         | 41 KB     | : 100% 1.0/1 [00:00<00:00, 22.83it/s]\n",
            "pydantic-1.7.3       | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.89it/s]\n",
            "libgfortran-ng-9.3.0 | 22 KB     | : 100% 1.0/1 [00:00<00:00, 27.28it/s]\n",
            "pytorch-1.8.0        | 41.8 MB   | : 100% 1.0/1 [00:07<00:00,  7.80s/it]\n",
            "urllib3-1.26.3       | 99 KB     | : 100% 1.0/1 [00:00<00:00, 18.80it/s]\n",
            "bz2file-0.98         | 9 KB      | : 100% 1.0/1 [00:00<00:00, 30.55it/s]\n",
            "zlib-1.2.11          | 106 KB    | : 100% 1.0/1 [00:00<00:00, 21.35it/s]\n",
            "_libgcc_mutex-0.1    | 3 KB      | : 100% 1.0/1 [00:00<00:00, 35.47it/s]\n",
            "joblib-1.0.1         | 206 KB    | : 100% 1.0/1 [00:00<00:00, 12.46it/s]\n",
            "python-3.9.2         | 27.3 MB   | : 100% 1.0/1 [00:04<00:00,  4.06s/it]\n",
            "pyopenssl-20.0.1     | 48 KB     | : 100% 1.0/1 [00:00<00:00, 17.42it/s]\n",
            "googleapis-common-pr | 70 KB     | : 100% 1.0/1 [00:00<00:00, 14.21it/s]\n",
            "markupsafe-1.1.1     | 27 KB     | : 100% 1.0/1 [00:00<00:00, 27.81it/s]\n",
            "pygments-2.8.1       | 736 KB    | : 100% 1.0/1 [00:00<00:00,  5.38it/s]\n",
            "future-0.18.2        | 718 KB    | : 100% 1.0/1 [00:00<00:00,  5.03it/s]\n",
            "sphinxcontrib-jsmath | 7 KB      | : 100% 1.0/1 [00:00<00:00, 28.76it/s]\n",
            "importlib-metadata-3 | 24 KB     | : 100% 1.0/1 [00:00<00:00, 27.74it/s]\n",
            "boto3-1.17.25        | 70 KB     | : 100% 1.0/1 [00:00<00:00, 13.57it/s]\n",
            "srsly-2.4.0          | 536 KB    | : 100% 1.0/1 [00:00<00:00,  5.19it/s]\n",
            "rsa-4.7.2            | 28 KB     | : 100% 1.0/1 [00:00<00:00, 22.94it/s]\n",
            "grpcio-1.36.1        | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.61it/s]\n",
            "google-crc32c-1.1.2  | 23 KB     | : 100% 1.0/1 [00:00<00:00, 20.53it/s]\n",
            "tqdm-4.59.0          | 77 KB     | : 100% 1.0/1 [00:00<00:00, 21.74it/s]\n",
            "cryptography-3.4.6   | 1.1 MB    | : 100% 1.0/1 [00:00<00:00,  4.21it/s]\n",
            "cffi-1.14.5          | 227 KB    | : 100% 1.0/1 [00:00<00:00, 14.06it/s]\n",
            "s3transfer-0.3.4     | 51 KB     | : 100% 1.0/1 [00:00<00:00, 23.05it/s]\n",
            "typer-0.3.2          | 23 KB     | : 100% 1.0/1 [00:00<00:00, 30.23it/s]\n",
            "dataclasses-0.8      | 7 KB      | : 100% 1.0/1 [00:00<00:00, 31.52it/s]\n",
            "jmespath-0.10.0      | 21 KB     | : 100% 1.0/1 [00:00<00:00, 18.20it/s]\n",
            "scipy-1.6.0          | 20.7 MB   | : 100% 1.0/1 [00:03<00:00,  3.22s/it]               \n",
            "libcurl-7.75.0       | 328 KB    | : 100% 1.0/1 [00:00<00:00, 11.96it/s]\n",
            "chardet-4.0.0        | 205 KB    | : 100% 1.0/1 [00:00<00:00, 12.05it/s]\n",
            "packaging-20.9       | 35 KB     | : 100% 1.0/1 [00:00<00:00, 24.33it/s]\n",
            "ld_impl_linux-64-2.3 | 618 KB    | : 100% 1.0/1 [00:00<00:00,  6.78it/s]\n",
            "preshed-3.0.5        | 120 KB    | : 100% 1.0/1 [00:00<00:00, 16.60it/s]\n",
            "pyjwt-2.0.1          | 17 KB     | : 100% 1.0/1 [00:00<00:00, 26.49it/s]\n",
            "grpc-cpp-1.36.2      | 3.4 MB    | : 100% 1.0/1 [00:00<00:00,  1.65it/s]\n",
            "wheel-0.36.2         | 31 KB     | : 100% 1.0/1 [00:00<00:00, 23.03it/s]\n",
            "pyasn1-modules-0.2.7 | 60 KB     | : 100% 1.0/1 [00:00<00:00, 16.14it/s]\n",
            "pathy-0.4.0          | 33 KB     | : 100% 1.0/1 [00:00<00:00, 24.87it/s]\n",
            "s2n-1.0.0            | 333 KB    | : 100% 1.0/1 [00:00<00:00, 12.46it/s]\n",
            "ncurses-6.2          | 985 KB    | : 100% 1.0/1 [00:00<00:00,  2.49it/s]\n",
            "snappy-1.1.8         | 32 KB     | : 100% 1.0/1 [00:00<00:00, 25.40it/s]\n",
            "pytz-2021.1          | 239 KB    | : 100% 1.0/1 [00:00<00:00,  7.77it/s]\n",
            "llvm-openmp-11.0.1   | 4.7 MB    | : 100% 1.0/1 [00:00<00:00,  1.45it/s]\n",
            "google-auth-1.24.0   | 62 KB     | : 100% 1.0/1 [00:00<00:00, 21.44it/s]\n",
            "libutf8proc-2.6.1    | 95 KB     | : 100% 1.0/1 [00:00<00:00, 21.41it/s]\n",
            "_openmp_mutex-4.5    | 5 KB      | : 100% 1.0/1 [00:00<00:00, 23.67it/s]\n",
            "mkl-2020.4           | 215.6 MB  | : 100% 1.0/1 [00:35<00:00, 35.49s/it]               \n",
            "requests-2.25.1      | 51 KB     | : 100% 1.0/1 [00:00<00:00, 16.47it/s]\n",
            "pyarrow-3.0.0        | 2.4 MB    | : 100% 1.0/1 [00:00<00:00,  2.44it/s]\n",
            "libnghttp2-1.43.0    | 808 KB    | : 100% 1.0/1 [00:00<00:00,  5.85it/s]\n",
            "werkzeug-1.0.1       | 239 KB    | : 100% 1.0/1 [00:00<00:00, 12.15it/s]\n",
            "libedit-3.1.20191231 | 121 KB    | : 100% 1.0/1 [00:00<00:00, 14.05it/s]\n",
            "google-auth-oauthlib | 18 KB     | : 100% 1.0/1 [00:00<00:00, 19.34it/s]\n",
            "ninja-1.10.2         | 2.4 MB    | : 100% 1.0/1 [00:00<00:00,  2.50it/s]\n",
            "sphinxcontrib-qthelp | 25 KB     | : 100% 1.0/1 [00:00<00:00, 18.66it/s]\n",
            "setuptools-49.6.0    | 943 KB    | : 100% 1.0/1 [00:00<00:00,  4.46it/s]\n",
            "lz4-c-1.9.3          | 179 KB    | : 100% 1.0/1 [00:00<00:00, 16.26it/s]\n",
            "typing_extensions-3. | 25 KB     | : 100% 1.0/1 [00:00<00:00, 24.61it/s]\n",
            "libffi-3.3           | 51 KB     | : 100% 1.0/1 [00:00<00:00, 26.03it/s]\n",
            "libev-4.33           | 104 KB    | : 100% 1.0/1 [00:00<00:00, 20.00it/s]\n",
            "openssl-1.1.1j       | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  3.09it/s]\n",
            "aws-c-io-0.9.1       | 119 KB    | : 100% 1.0/1 [00:00<00:00, 10.91it/s]\n",
            "python_abi-3.9       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 24.76it/s]\n",
            "numpy-1.20.1         | 5.8 MB    | : 100% 1.0/1 [00:01<00:00,  1.07s/it]\n",
            "botocore-1.20.25     | 4.5 MB    | : 100% 1.0/1 [00:01<00:00,  1.68s/it]\n",
            "docutils-0.16        | 737 KB    | : 100% 1.0/1 [00:00<00:00,  4.76it/s]\n",
            "google-cloud-core-1. | 26 KB     | : 100% 1.0/1 [00:00<00:00, 26.65it/s]\n",
            "spacy-3.0.4          | 10.4 MB   | : 100% 1.0/1 [00:02<00:00,  2.43s/it]\n",
            "sphinx-3.5.2         | 1.4 MB    | : 100% 1.0/1 [00:00<00:00,  2.34it/s]\n",
            "shellingham-1.4.0    | 11 KB     | : 100% 1.0/1 [00:00<00:00, 30.22it/s]\n",
            "thinc-8.0.2          | 1.0 MB    | : 100% 1.0/1 [00:00<00:00,  3.65it/s]\n",
            "oauthlib-3.0.1       | 82 KB     | : 100% 1.0/1 [00:00<00:00, 18.11it/s]\n",
            "libcrc32c-1.1.1      | 20 KB     | : 100% 1.0/1 [00:00<00:00, 29.08it/s]\n",
            "colorama-0.4.4       | 18 KB     | : 100% 1.0/1 [00:00<00:00, 26.73it/s]\n",
            "zstd-1.4.9           | 431 KB    | : 100% 1.0/1 [00:00<00:00, 10.96it/s]\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Installing pip dependencies: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ Ran pip subprocess with arguments:\n",
            "['/usr/local/envs/transformersum/bin/python', '-m', 'pip', 'install', '-U', '-r', '/content/transformersum/condaenv.1s_m3s5p.requirements.txt']\n",
            "Pip subprocess output:\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.2.3-py3-none-any.whl (821 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "Collecting torch_optimizer\n",
            "  Downloading torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.10.22-py2.py3-none-any.whl (2.0 MB)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/envs/transformersum/lib/python3.9/site-packages (from -r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 6)) (20.9)\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.4.1-py3-none-any.whl (186 kB)\n",
            "Collecting gradio\n",
            "  Downloading gradio-1.6.3-py3-none-any.whl (1.1 MB)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.2.3-cp39-cp39-manylinux1_x86_64.whl (9.7 MB)\n",
            "Collecting dill\n",
            "  Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
            "Collecting tqdm<4.50.0,>=4.27\n",
            "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (1.20.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (2.25.1)\n",
            "Collecting huggingface-hub==0.0.2\n",
            "  Downloading huggingface_hub-0.0.2-py3-none-any.whl (24 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.0.tar.gz (64 kB)\n",
            "Collecting fsspec\n",
            "  Downloading fsspec-0.8.7-py3-none-any.whl (103 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.11.1-py39-none-any.whl (126 kB)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (3.0.0)\n",
            "Collecting filelock\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests>=2.19.0->datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (2020.12.5)\n",
            "Collecting IPython\n",
            "  Downloading ipython-7.21.0-py3-none-any.whl (784 kB)\n",
            "Collecting scikit-image\n",
            "  Downloading scikit_image-0.18.1-cp39-cp39-manylinux1_x86_64.whl (28.9 MB)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Collecting Flask>=1.1.1\n",
            "  Downloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
            "Collecting paramiko\n",
            "  Downloading paramiko-2.7.2-py2.py3-none-any.whl (206 kB)\n",
            "Collecting Flask-BasicAuth\n",
            "  Downloading Flask-BasicAuth-0.2.0.tar.gz (16 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/envs/transformersum/lib/python3.9/site-packages (from gradio->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 8)) (1.6.0)\n",
            "Collecting flask-cachebuster\n",
            "  Downloading Flask-CacheBuster-1.0.0.tar.gz (3.1 kB)\n",
            "Collecting markdown2\n",
            "  Downloading markdown2-2.4.0-py2.py3-none-any.whl (34 kB)\n",
            "Collecting Flask-Cors>=3.0.8\n",
            "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
            "Collecting analytics-python\n",
            "  Downloading analytics_python-1.2.9-py2.py3-none-any.whl (13 kB)\n",
            "Collecting itsdangerous>=0.24\n",
            "  Downloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from Flask>=1.1.1->gradio->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 8)) (2.11.3)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from Flask>=1.1.1->gradio->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from Flask>=1.1.1->gradio->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: Six in /usr/local/envs/transformersum/lib/python3.9/site-packages (from Flask-Cors>=3.0.8->gradio->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 8)) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from Jinja2>=2.10.1->Flask>=1.1.1->gradio->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 8)) (1.1.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from packaging->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (0.18.2)\n",
            "Collecting PyYAML!=5.4.*,>=5.1\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/envs/transformersum/lib/python3.9/site-packages (from fsspec->datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (3.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (49.6.0.post20210108)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (3.15.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (1.36.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (1.24.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (0.2.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/envs/transformersum/lib/python3.9/site-packages (from torch>=1.4->pytorch_lightning->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 1)) (3.7.4.3)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.1-cp39-cp39-manylinux2010_x86_64.whl (3.2 MB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2020.11.13-cp39-cp39-manylinux2014_x86_64.whl (732 kB)\n",
            "Collecting promise<3,>=2.0\n",
            "  Downloading promise-2.3.tar.gz (19 kB)\n",
            "Collecting psutil>=5.0.0\n",
            "  Downloading psutil-5.8.0-cp39-cp39-manylinux2010_x86_64.whl (293 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from wandb->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 4)) (2.8.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.14-py3-none-any.whl (159 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "  Downloading sentry_sdk-1.0.0-py2.py3-none-any.whl (131 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading smmap-3.0.5-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from aiohttp->fsspec->datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (3.0.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from aiohttp->fsspec->datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (1.5.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from aiohttp->fsspec->datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (5.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from aiohttp->fsspec->datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (20.3.0)\n",
            "Collecting backcall\n",
            "  Downloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.16-py3-none-any.whl (366 kB)\n",
            "Collecting pickleshare\n",
            "  Downloading pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting decorator\n",
            "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting pexpect>4.3\n",
            "  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "Requirement already satisfied: pygments in /usr/local/envs/transformersum/lib/python3.9/site-packages (from IPython->gradio->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 8)) (2.8.1)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.0-py2.py3-none-any.whl (1.4 MB)\n",
            "Collecting traitlets>=4.2\n",
            "  Downloading traitlets-5.0.5-py3-none-any.whl (100 kB)\n",
            "Collecting parso<0.9.0,>=0.8.0\n",
            "  Downloading parso-0.8.1-py2.py3-none-any.whl (93 kB)\n",
            "Collecting ptyprocess>=0.5\n",
            "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting wcwidth\n",
            "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting ipython-genutils\n",
            "  Downloading ipython_genutils-0.2.0-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/envs/transformersum/lib/python3.9/site-packages (from nltk->rouge-score->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from pandas->datasets->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 7)) (2021.1)\n",
            "Collecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.4.0-cp35-abi3-manylinux1_x86_64.whl (961 kB)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from paramiko->gradio->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 8)) (3.4.6)\n",
            "Collecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-3.2.0-cp36-abi3-manylinux2010_x86_64.whl (63 kB)\n",
            "Requirement already satisfied: cffi>=1.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from bcrypt>=3.1.3->paramiko->gradio->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 8)) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/envs/transformersum/lib/python3.9/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio->-r /content/transformersum/condaenv.1s_m3s5p.requirements.txt (line 8)) (2.20)\n",
            "Collecting networkx>=2.0\n",
            "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
            "Collecting matplotlib!=3.0.0,>=2.0.0\n",
            "  Downloading matplotlib-3.3.4-cp39-cp39-manylinux1_x86_64.whl (11.5 MB)\n",
            "Collecting tifffile>=2019.7.26\n",
            "  Downloading tifffile-2021.3.5-py3-none-any.whl (162 kB)\n",
            "Collecting PyWavelets>=1.1.1\n",
            "  Downloading PyWavelets-1.1.1-cp39-cp39-manylinux1_x86_64.whl (4.3 MB)\n",
            "Collecting imageio>=2.3.0\n",
            "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
            "Collecting pillow!=7.1.0,!=7.1.1,>=4.3.0\n",
            "  Downloading Pillow-8.1.2-cp39-cp39-manylinux1_x86_64.whl (2.2 MB)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.3.1-cp39-cp39-manylinux1_x86_64.whl (1.2 MB)\n",
            "Collecting cycler>=0.10\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Building wheels for collected packages: PyYAML, promise, subprocess32, ffmpy, Flask-BasicAuth, flask-cachebuster, nltk, pathtools, sacremoses, xxhash\n",
            "  Building wheel for PyYAML (setup.py): started\n",
            "  Building wheel for PyYAML (setup.py): finished with status 'done'\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp39-cp39-linux_x86_64.whl size=163444 sha256=04bc088423e26e1dd5143534de28e4da945186ff642b1ab14e8e815e710c8a2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/60/81/5cd74b8ee068fbe9e04ca0d53148f28f5c6e2c5b177d5dd622\n",
            "  Building wheel for promise (setup.py): started\n",
            "  Building wheel for promise (setup.py): finished with status 'done'\n",
            "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=4df01acfc28499faed37fe28023d4a417a0ecc97244bd99583c835046ea77b06\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
            "  Building wheel for subprocess32 (setup.py): started\n",
            "  Building wheel for subprocess32 (setup.py): finished with status 'done'\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6488 sha256=1871e7d6f858455e828da3815238c2837b99d519443d32ecdc869b5595024da8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/f1/ce/3658488c09ec9d6b3037da989642c87575411f113798c90e14\n",
            "  Building wheel for ffmpy (setup.py): started\n",
            "  Building wheel for ffmpy (setup.py): finished with status 'done'\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4709 sha256=a0a3faa5f79a50ca0fb8428d0902ee74929678305a804d4eba415cbd63a59837\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/e2/96/f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
            "  Building wheel for Flask-BasicAuth (setup.py): started\n",
            "  Building wheel for Flask-BasicAuth (setup.py): finished with status 'done'\n",
            "  Created wheel for Flask-BasicAuth: filename=Flask_BasicAuth-0.2.0-py3-none-any.whl size=4226 sha256=3c7e5edb448c7a69cf35154764aea4cc26475e188b2168587f8025d34c2a8565\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/5a/db/e442580c22be34f69e537448832d7e1ee5a9c5adb63ace30bf\n",
            "  Building wheel for flask-cachebuster (setup.py): started\n",
            "  Building wheel for flask-cachebuster (setup.py): finished with status 'done'\n",
            "  Created wheel for flask-cachebuster: filename=Flask_CacheBuster-1.0.0-py3-none-any.whl size=3372 sha256=4bf4f976809a210d64a077cf6c24ffffa3c84b781c04f0387aa1e86f78cc618d\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/0b/05/607940155598cf002272cafb7c7616cdecfc458dbf4f9a5676\n",
            "  Building wheel for nltk (setup.py): started\n",
            "  Building wheel for nltk (setup.py): finished with status 'done'\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434677 sha256=547705f3771d6007ef7d45ada54a10b3364f7b9333c4e36b34f05b1f61c7846c\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/ae/bb/5e2a232ebaa1d2f38dd5f587e9fc4cf6ccb12758d14dac14d8\n",
            "  Building wheel for pathtools (setup.py): started\n",
            "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8784 sha256=46ef13b04372f6bc976535d02be975da38b427b5834d982592522b7b3025dc8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "  Building wheel for sacremoses (setup.py): started\n",
            "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=bf624d965b9dcc44ef0f0c2cd697668cef727531214ee1009b4a66cb630c2bca\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/73/d4/926dd30cdb250a7dddfbccebe12b7fbc9831c39c77b0163689\n",
            "  Building wheel for xxhash (setup.py): started\n",
            "  Building wheel for xxhash (setup.py): finished with status 'done'\n",
            "  Created wheel for xxhash: filename=xxhash-2.0.0-cp39-cp39-linux_x86_64.whl size=30280 sha256=d8aea9fa0bf81f672beb0a262df298657ce772e58379b399450b04faeb5978d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/6a/a1/5f7536c0da8296389fcf6ef99b8b7eee9c29fb09591d0a1c69\n",
            "Successfully built PyYAML promise subprocess32 ffmpy Flask-BasicAuth flask-cachebuster nltk pathtools sacremoses xxhash\n",
            "Installing collected packages: wcwidth, smmap, ptyprocess, pillow, parso, kiwisolver, itsdangerous, ipython-genutils, decorator, cycler, traitlets, tqdm, tifffile, regex, PyWavelets, pynacl, prompt-toolkit, pickleshare, pexpect, networkx, matplotlib, jedi, imageio, gitdb, fsspec, Flask, filelock, dill, bcrypt, backcall, xxhash, tokenizers, subprocess32, shortuuid, sentry-sdk, scikit-image, sacremoses, PyYAML, pytorch-ranger, psutil, promise, pathtools, paramiko, pandas, nltk, multiprocess, markdown2, IPython, huggingface-hub, GitPython, Flask-Cors, flask-cachebuster, Flask-BasicAuth, ffmpy, docker-pycreds, configparser, analytics-python, wandb, transformers, torch-optimizer, rouge-score, pytorch-lightning, gradio, datasets\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.59.0\n",
            "    Uninstalling tqdm-4.59.0:\n",
            "      Successfully uninstalled tqdm-4.59.0\n",
            "Successfully installed Flask-1.1.2 Flask-BasicAuth-0.2.0 Flask-Cors-3.0.10 GitPython-3.1.14 IPython-7.21.0 PyWavelets-1.1.1 PyYAML-5.3.1 analytics-python-1.2.9 backcall-0.2.0 bcrypt-3.2.0 configparser-5.0.2 cycler-0.10.0 datasets-1.4.1 decorator-4.4.2 dill-0.3.3 docker-pycreds-0.4.0 ffmpy-0.3.0 filelock-3.0.12 flask-cachebuster-1.0.0 fsspec-0.8.7 gitdb-4.0.5 gradio-1.6.3 huggingface-hub-0.0.2 imageio-2.9.0 ipython-genutils-0.2.0 itsdangerous-1.1.0 jedi-0.18.0 kiwisolver-1.3.1 markdown2-2.4.0 matplotlib-3.3.4 multiprocess-0.70.11.1 networkx-2.5 nltk-3.5 pandas-1.2.3 paramiko-2.7.2 parso-0.8.1 pathtools-0.1.2 pexpect-4.8.0 pickleshare-0.7.5 pillow-8.1.2 promise-2.3 prompt-toolkit-3.0.16 psutil-5.8.0 ptyprocess-0.7.0 pynacl-1.4.0 pytorch-lightning-1.2.3 pytorch-ranger-0.1.1 regex-2020.11.13 rouge-score-0.0.4 sacremoses-0.0.43 scikit-image-0.18.1 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 tifffile-2021.3.5 tokenizers-0.10.1 torch-optimizer-0.1.0 tqdm-4.49.0 traitlets-5.0.5 transformers-4.3.3 wandb-0.10.22 wcwidth-0.2.5 xxhash-2.0.0\n",
            "\n",
            "\b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate transformersum\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U8LS3MeLgxn1",
        "outputId": "7fe18a6c-7e93-4af4-e017-3f3ef41327f5"
      },
      "source": [
        "%%shell\n",
        "\n",
        "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
        "conda activate transformersum\n",
        "\n",
        "\n",
        "pip install gdown\n",
        "gdown https://drive.google.com/uc?id=1-1VC6ECFAfqNqVOvPtWHT_1YSJORvMMs\n",
        "\n",
        "pip install spacy \n",
        "python -m spacy download en_core_web_sm\n",
        "\n",
        "conda deactivate"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gdown\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/envs/transformersum/lib/python3.9/site-packages (from gdown) (3.0.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/envs/transformersum/lib/python3.9/site-packages (from gdown) (4.49.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/envs/transformersum/lib/python3.9/site-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: six in /usr/local/envs/transformersum/lib/python3.9/site-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests[socks]->gdown) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9681 sha256=6409babd4849010aff435f0b26162aabbe6d2f5c72ffd9399a7e7d3622395ff7\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/65/a3/57172a39ac442300d84329fe2730327f7674bdfb2ab7ba3e74\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "Successfully installed gdown-3.12.2\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-1VC6ECFAfqNqVOvPtWHT_1YSJORvMMs\n",
            "To: /content/epoch=3.ckpt\n",
            "796MB [00:09, 82.8MB/s]\n",
            "Requirement already satisfied: spacy in /usr/local/envs/transformersum/lib/python3.9/site-packages (3.0.4)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (3.0.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (2.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (20.9)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (8.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (4.49.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (2.25.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (1.7.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (49.6.0.post20210108)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (0.7.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (1.20.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (2.2.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy) (1.17.25)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy) (0.3.4)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.25 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy) (1.20.25)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from botocore<1.21.0,>=1.20.25->boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.25->boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from jinja2->spacy) (1.1.1)\n",
            "Warning: please export TSAN_OPTIONS='ignore_noninstrumented_modules=1' to avoid false positive reports from the OpenMP runtime!\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7 MB 65 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (49.6.0.post20210108)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.49.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.2.1)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.17.25)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.25 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.25)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from botocore<1.21.0,>=1.20.25->boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.25->boto3->smart-open<4.0.0,>=2.2.0->pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Q4iyH2IokXE"
      },
      "source": [
        "!mv epoch=3.ckpt transformersum/src\n",
        "#!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nuLMLAAHY0no"
      },
      "source": [
        "# %%writefile code.py\n",
        "\n",
        "# from extractive import ExtractiveSummarizer\n",
        "# model = ExtractiveSummarizer.load_from_checkpoint(\"epoch=3.ckpt\")\n",
        "# text_to_summarize = \"The Indonesian government formally enforces rules of study, worship, and work from home from March 16, 2020. Minimizing and limiting meetings involving physical contact is an effort to reduce the spread of the COVID-19 virus. These conditions have implications for the effectiveness of the learning process in higher education. The purpose of this study was to identify the impact of student psychology on online learning during the COVID-19 pandemic. The research method uses a qualitative research type of phenomenology. The research subjects were 30 students of Mulawarman University who were interviewed via telephone. The research findings show that (1) students have started to get bored with online learning after the first two weeks of learning from home, (2) considerable anxiety on research subjects whose parents have low income, because they have to buy quotas to be able to participate in online learning, ( 3) mood or mood changes occur due to too many assignments and are considered ineffective by students. Suggestions and recommendations from this research are the need for severe efforts in assisting the psychological well-being of students through the involvement of counselors and psychologists.\"\n",
        "# summary = model.predict(text_to_summarize)\n",
        "# print (summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZQ27pXbwY1dP"
      },
      "source": [
        "# !mv code.py transformersum/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NCunQwQGXrW7"
      },
      "source": [
        "# correct sentecezier in extractive.py\n",
        "# correct maxlen in data.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GiGrwfM0YrpJ",
        "outputId": "c8c6eb52-5ac9-464b-b2a2-ab9cca2b2468"
      },
      "source": [
        "%%writefile extractive.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import logging\n",
        "import types\n",
        "from typing import List, Union\n",
        "import statistics\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "from argparse import ArgumentParser, Namespace\n",
        "import pytorch_lightning as pl\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from spacy.lang.en import English\n",
        "from pooling import Pooling\n",
        "from data import SentencesProcessor, FSIterableDataset, pad_batch_collate, FSDataset\n",
        "from classifier import (\n",
        "    LinearClassifier,\n",
        "    SimpleLinearClassifier,\n",
        "    TransformerEncoderClassifier,\n",
        ")\n",
        "from helpers import (\n",
        "    load_json,\n",
        "    block_trigrams,\n",
        "    test_rouge,\n",
        "    generic_configure_optimizers,\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "from transformers.data.metrics import acc_and_f1\n",
        "\n",
        "# CUSTOM_MODEL_CLASSES = (\"longformer\",)\n",
        "\n",
        "try:\n",
        "    from transformers.models.auto.modeling_auto import MODEL_MAPPING\n",
        "\n",
        "    MODEL_CLASSES = tuple(\n",
        "        [m.model_type for m in MODEL_MAPPING]\n",
        "    )  # + CUSTOM_MODEL_CLASSES\n",
        "except ImportError:\n",
        "    logger.warning(\n",
        "        \"Could not import `MODEL_MAPPING` from transformers because it is an old version.\"\n",
        "    )\n",
        "\n",
        "    MODEL_CLASSES = (\n",
        "        tuple(\n",
        "            \"Note: Only showing custom models because old version of `transformers` detected.\"\n",
        "        )\n",
        "        # + CUSTOM_MODEL_CLASSES\n",
        "    )\n",
        "\n",
        "\n",
        "def longformer_modifier(final_dictionary):\n",
        "    \"\"\"\n",
        "    Creates the ``global_attention_mask`` for the longformer. Tokens with global attention\n",
        "    attend to all other tokens, and all other tokens attend to them. This is important for\n",
        "    task-specific finetuning because it makes the model more flexible at representing the\n",
        "    task. For example, for classification, the `<s>` token should be given global attention.\n",
        "    For QA, all question tokens should also have global attention. For summarization,\n",
        "    global attention is given to all of the `<s>` (RoBERTa 'CLS' equivalent) tokens. Please\n",
        "    refer to the `Longformer paper <https://arxiv.org/abs/2004.05150>`_ for more details. Mask\n",
        "    values selected in ``[0, 1]``: ``0`` for local attention, ``1`` for global attention.\n",
        "    \"\"\"\n",
        "    # `batch_size` is the number of attention masks (one mask per input sequence)\n",
        "    batch_size = len(final_dictionary[\"attention_mask\"])\n",
        "    # `sequence_length` is the number of tokens for the first sequence in the batch\n",
        "    sequence_length = len(final_dictionary[\"attention_mask\"][0])\n",
        "    # create `global_attention_mask` using the above details\n",
        "    global_attention_mask = torch.tensor([[0] * sequence_length] * batch_size)\n",
        "    # set the `sent_rep_token_ids` to 1, which is global attention\n",
        "    for idx, items in enumerate(final_dictionary[\"sent_rep_token_ids\"]):\n",
        "        global_attention_mask[idx, items] = 1\n",
        "\n",
        "    final_dictionary[\"global_attention_mask\"] = global_attention_mask\n",
        "    # The `global_attention_mask` is passed through the model's `forward`\n",
        "    # function as `**kwargs`.\n",
        "    return final_dictionary\n",
        "\n",
        "\n",
        "class ExtractiveSummarizer(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    A machine learning model that extractively summarizes an input text by scoring the sentences.\n",
        "    Main class that handles the data loading, initial processing, training/testing/validating setup,\n",
        "    and contains the actual model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hparams, embedding_model_config=None, classifier_obj=None):\n",
        "        super(ExtractiveSummarizer, self).__init__()\n",
        "\n",
        "        if type(hparams) is not Namespace:\n",
        "            hparams = Namespace(**hparams)\n",
        "\n",
        "        # Set new parameters to defaults if they do not exist in the `hparams` Namespace\n",
        "        hparams.gradient_checkpointing = getattr(\n",
        "            hparams, \"gradient_checkpointing\", False\n",
        "        )\n",
        "        hparams.tokenizer_no_use_fast = getattr(hparams, \"tokenizer_no_use_fast\", False)\n",
        "        hparams.data_type = getattr(hparams, \"data_type\", \"none\")\n",
        "\n",
        "        self.hparams = hparams\n",
        "        self.forward_modify_inputs_callback = None\n",
        "\n",
        "        if not embedding_model_config:\n",
        "            embedding_model_config = AutoConfig.from_pretrained(\n",
        "                hparams.model_name_or_path,\n",
        "                gradient_checkpointing=hparams.gradient_checkpointing,\n",
        "            )\n",
        "\n",
        "        self.word_embedding_model = AutoModel.from_config(embedding_model_config)\n",
        "\n",
        "        if (\n",
        "            any(x in hparams.model_name_or_path for x in [\"roberta\", \"distil\", \"longformer\"])\n",
        "        ) and not hparams.no_use_token_type_ids:\n",
        "            logger.warning(\n",
        "                (\n",
        "                    \"You are using a %s model but did not set \"\n",
        "                    + \"--no_use_token_type_ids. This model does not support `token_type_ids` so \"\n",
        "                    + \"this option has been automatically enabled.\"\n",
        "                ),\n",
        "                hparams.model_type,\n",
        "            )\n",
        "            self.hparams.no_use_token_type_ids = True\n",
        "\n",
        "        self.emd_model_frozen = False\n",
        "        if hparams.num_frozen_steps > 0:\n",
        "            self.emd_model_frozen = True\n",
        "            self.freeze_web_model()\n",
        "\n",
        "        if hparams.pooling_mode == \"sent_rep_tokens\":\n",
        "            self.pooling_model = Pooling(\n",
        "                sent_rep_tokens=True, mean_tokens=False, max_tokens=False\n",
        "            )\n",
        "        elif hparams.pooling_mode == \"max_tokens\":\n",
        "            self.pooling_model = Pooling(\n",
        "                sent_rep_tokens=False, mean_tokens=False, max_tokens=True\n",
        "            )\n",
        "        else:\n",
        "            self.pooling_model = Pooling(\n",
        "                sent_rep_tokens=False, mean_tokens=True, max_tokens=False\n",
        "            )\n",
        "\n",
        "        # if a classifier object was passed when creating this model then store that as the `encoder`\n",
        "        if classifier_obj:\n",
        "            self.encoder = classifier_obj\n",
        "        # otherwise create the classifier using the `hparams.classifier` parameter if available\n",
        "        # if the `hparams.classifier` parameter is missing then create a `LinearClassifier`\n",
        "        else:\n",
        "            # returns `classifier` value if it exists, otherwise returns False\n",
        "            classifier_exists = getattr(hparams, \"classifier\", False)\n",
        "            if (not classifier_exists) or (hparams.classifier == \"linear\"):\n",
        "                self.encoder = LinearClassifier(\n",
        "                    self.word_embedding_model.config.hidden_size,\n",
        "                    dropout=hparams.classifier_dropout,\n",
        "                )\n",
        "            elif hparams.classifier == \"simple_linear\":\n",
        "                self.encoder = SimpleLinearClassifier(\n",
        "                    self.word_embedding_model.config.hidden_size\n",
        "                )\n",
        "            elif hparams.classifier == \"transformer\":\n",
        "                self.encoder = TransformerEncoderClassifier(\n",
        "                    self.word_embedding_model.config.hidden_size,\n",
        "                    dropout=hparams.classifier_dropout,\n",
        "                    num_layers=hparams.classifier_transformer_num_layers,\n",
        "                )\n",
        "            elif hparams.classifier == \"transformer_linear\":\n",
        "                linear = LinearClassifier(\n",
        "                    self.word_embedding_model.config.hidden_size,\n",
        "                    dropout=hparams.classifier_dropout,\n",
        "                )\n",
        "                self.encoder = TransformerEncoderClassifier(\n",
        "                    self.word_embedding_model.config.hidden_size,\n",
        "                    dropout=hparams.classifier_dropout,\n",
        "                    num_layers=hparams.classifier_transformer_num_layers,\n",
        "                    custom_reduction=linear,\n",
        "                )\n",
        "            else:\n",
        "                logger.error(\n",
        "                    \"%s is not a valid value for `--classifier`. Exiting...\",\n",
        "                    hparams.classifier,\n",
        "                )\n",
        "                sys.exit(1)\n",
        "\n",
        "        # Set `hparams.no_test_block_trigrams` to False if it does not exist,\n",
        "        # otherwise set its value to itself, resulting in no change\n",
        "        self.hparams.no_test_block_trigrams = getattr(\n",
        "            hparams, \"no_test_block_trigrams\", False\n",
        "        )\n",
        "\n",
        "        # BCELoss: https://pytorch.org/docs/stable/nn.html#bceloss\n",
        "        # `reduction` is \"none\" so the mean can be computed with padding ignored.\n",
        "        # `nn.BCEWithLogitsLoss` (which combines a sigmoid layer and the BCELoss\n",
        "        # in one single class) is used because it takes advantage of the log-sum-exp\n",
        "        # trick for numerical stability. Padding values are 0 and if 0 is the input\n",
        "        # to the sigmoid function the output will be 0.5. This will cause issues when\n",
        "        # inputs with more padding will have higher loss values. To solve this, all\n",
        "        # padding values are set to -9e3 as the last step of each encoder. The sigmoid\n",
        "        # function transforms -9e3 to nearly 0, thus preserving the proper loss\n",
        "        # calculation. See `compute_loss()` for more info.\n",
        "        self.loss_func = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
        "\n",
        "        # Data\n",
        "        self.processor = SentencesProcessor(name=\"main_processor\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            hparams.tokenizer_name\n",
        "            if hparams.tokenizer_name\n",
        "            else hparams.model_name_or_path,\n",
        "            use_fast=(not self.hparams.tokenizer_no_use_fast),\n",
        "        )\n",
        "\n",
        "        self.train_dataloader_object = None  # not created yet\n",
        "        self.datasets = None\n",
        "        self.pad_batch_collate = None\n",
        "        self.global_step_tracker = None\n",
        "        self.rouge_metrics = None\n",
        "        self.rouge_scorer = None\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        sent_rep_mask=None,\n",
        "        token_type_ids=None,\n",
        "        sent_rep_token_ids=None,\n",
        "        sent_lengths=None,\n",
        "        sent_lengths_mask=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        r\"\"\"Model forward function. See the `60 minute bliz tutorial <https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html>`_\n",
        "        if you are unsure what a forward function is.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): Indices of input sequence tokens in the vocabulary.\n",
        "                `What are input IDs? <https://huggingface.co/transformers/glossary.html#input-ids>`_\n",
        "            attention_mask (torch.Tensor): Mask to avoid performing attention on padding token\n",
        "                indices. Mask values selected in ``[0, 1]``: ``1`` for tokens that are NOT\n",
        "                MASKED, ``0`` for MASKED tokens. `What are attention masks? <https://huggingface.co/transformers/glossary.html#attention-mask>`_\n",
        "            sent_rep_mask (torch.Tensor, optional): Indicates which numbers in ``sent_rep_token_ids``\n",
        "                are actually the locations of sentence representation ids and which are padding.\n",
        "                Defaults to None.\n",
        "            token_type_ids (torch.Tensor, optional): Usually, segment token indices to indicate\n",
        "                first and second portions of the inputs. However, for summarization they are used\n",
        "                to indicate different sentences. Depending on the size of the token type id vocabulary,\n",
        "                these values may alternate between ``0`` and ``1`` or they may increase sequentially\n",
        "                for each sentence in the input.. Defaults to None.\n",
        "            sent_rep_token_ids (torch.Tensor, optional): The locations of the sentence representation\n",
        "                tokens. Defaults to None.\n",
        "            sent_lengths (torch.Tensor, optional):  A list of the lengths of each sentence in\n",
        "                ``input_ids``. See :meth:`data.pad_batch_collate` for more info about the\n",
        "                generation of thisfeature. Defaults to None.\n",
        "            sent_lengths_mask (torch.Tensor, optional): Created on-the-fly by :meth:`data.pad_batch_collate`.\n",
        "                Similar to ``sent_rep_mask``: ``1`` for value and ``0`` for padding. See\n",
        "                :meth:`data.pad_batch_collate` for more info about the generation of this\n",
        "                feature. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            tuple: Contains the sentence scores and mask as ``torch.Tensor``\\ s. The mask is either\n",
        "            the ``sent_rep_mask`` or ``sent_lengths_mask`` depending on the pooling mode used\n",
        "            during model initialization.\n",
        "        \"\"\"\n",
        "        inputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "        }\n",
        "        if not self.hparams.no_use_token_type_ids:\n",
        "            inputs[\"token_type_ids\"] = token_type_ids\n",
        "\n",
        "        if self.forward_modify_inputs_callback:\n",
        "            inputs = self.forward_modify_inputs_callback(inputs)  # skipcq: PYL-E1102\n",
        "\n",
        "        outputs = self.word_embedding_model(**inputs, **kwargs)\n",
        "        word_vectors = outputs[0]\n",
        "\n",
        "        sents_vec, mask = self.pooling_model(\n",
        "            word_vectors=word_vectors,\n",
        "            sent_rep_token_ids=sent_rep_token_ids,\n",
        "            sent_rep_mask=sent_rep_mask,\n",
        "            sent_lengths=sent_lengths,\n",
        "            sent_lengths_mask=sent_lengths_mask,\n",
        "        )\n",
        "\n",
        "        sent_scores = self.encoder(sents_vec, mask)\n",
        "        return sent_scores, mask\n",
        "\n",
        "    def unfreeze_web_model(self):\n",
        "        \"\"\"Un-freezes the ``word_embedding_model``\"\"\"\n",
        "        for param in self.word_embedding_model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def freeze_web_model(self):\n",
        "        \"\"\"Freezes the encoder ``word_embedding_model``\"\"\"\n",
        "        for param in self.word_embedding_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def compute_loss(self, outputs, labels, mask):\n",
        "        \"\"\"Compute the loss between model outputs and ground-truth labels.\n",
        "\n",
        "        Args:\n",
        "            outputs (torch.Tensor): Output sentence scores obtained from :meth:`~extractive.ExtractiveSummarizer.forward`\n",
        "            labels (torch.Tensor): Ground-truth labels (``1`` for sentences that should be in\n",
        "                the summary, ``0`` otherwise) from the batch generated during the data\n",
        "                preprocessing stage.\n",
        "            mask (torch.Tensor): Mask returned by :meth:`~extractive.ExtractiveSummarizer.forward`,\n",
        "                either ``sent_rep_mask`` or ``sent_lengths_mask`` depending on the pooling mode used\n",
        "                during model initialization.\n",
        "\n",
        "        Returns:\n",
        "            [tuple]: Losses: (total_loss, total_norm_batch_loss, sum_avg_seq_loss, mean_avg_seq_loss,\n",
        "                average_loss)\n",
        "        \"\"\"\n",
        "        try:\n",
        "            loss = self.loss_func(outputs, labels.float())\n",
        "        except ValueError as e:\n",
        "            logger.error(e)\n",
        "            logger.error(\n",
        "                \"Details about above error:\\n1. outputs=%s\\n2. labels.float()=%s\",\n",
        "                outputs,\n",
        "                labels.float(),\n",
        "            )\n",
        "            sys.exit(1)\n",
        "\n",
        "        # set all padding values to zero\n",
        "        loss = loss * mask.float()\n",
        "        # add up all the loss values for each sequence (including padding because\n",
        "        # padding values are zero and thus will have no effect)\n",
        "        sum_loss_per_sequence = loss.sum(dim=1)\n",
        "        # count the number of losses that are not padding per sequence\n",
        "        num_not_padded_per_sequence = mask.sum(dim=1).float()\n",
        "        # find the average loss per sequence\n",
        "        average_per_sequence = sum_loss_per_sequence / num_not_padded_per_sequence\n",
        "        # get the sum of the average loss per sequence\n",
        "        sum_avg_seq_loss = average_per_sequence.sum()  # sum_average_per_sequence\n",
        "        # get the mean of `average_per_sequence`\n",
        "        batch_size = average_per_sequence.size(0)\n",
        "        mean_avg_seq_loss = sum_avg_seq_loss / batch_size\n",
        "\n",
        "        # calculate the sum of all the loss values for each sequence\n",
        "        total_loss = sum_loss_per_sequence.sum()\n",
        "        # count the total number of losses that are not padding\n",
        "        total_num_not_padded = num_not_padded_per_sequence.sum().float()\n",
        "        # average loss\n",
        "        average_loss = total_loss / total_num_not_padded\n",
        "        # total loss normalized by batch size\n",
        "        total_norm_batch_loss = total_loss / batch_size\n",
        "        return (\n",
        "            total_loss,\n",
        "            total_norm_batch_loss,\n",
        "            sum_avg_seq_loss,\n",
        "            mean_avg_seq_loss,\n",
        "            average_loss,\n",
        "        )\n",
        "\n",
        "    def setup(self, stage):\n",
        "        \"\"\"Download the `word_embedding_model` if the model will be trained.\"\"\"\n",
        "        # The model is having training resumed if the `hparams` contains `resume_from_checkpoint`\n",
        "        # and `resume_from_checkpoint` is True.\n",
        "        resuming = (\n",
        "            hasattr(self.hparams, \"resume_from_checkpoint\")\n",
        "            and self.hparams.resume_from_checkpoint\n",
        "        )\n",
        "        # `stage` can be \"fit\" or \"test\". Only load the pre-trained weights when\n",
        "        # beginning to fit for the first time (when we are not resuming)\n",
        "        if stage == \"fit\" and not resuming:\n",
        "            logger.info(\"Loading `word_embedding_model` pre-trained weights.\")\n",
        "            self.word_embedding_model = AutoModel.from_pretrained(\n",
        "                self.hparams.model_name_or_path, config=self.word_embedding_model.config\n",
        "            )\n",
        "\n",
        "    def json_to_dataset(\n",
        "        self, tokenizer, hparams, inputs=None, num_files=0, processor=None,\n",
        "    ):\n",
        "        \"\"\"Convert json output from ``convert_to_extractive.py`` to a \".pt\" file containing\n",
        "        lists or tensors using a :class:`data.SentencesProcessor`. This function is run by\n",
        "        :meth:`~extractive.ExtractiveSummarizer.prepare_data` in parallel.\n",
        "\n",
        "        Args:\n",
        "            tokenizer (transformers.PreTrainedTokenizer): Tokenizer used to convert examples\n",
        "                to input_ids. Usually is ``self.tokenizer``.\n",
        "            hparams (argparse.Namespace): Hyper-parameters used to create the model. Usually\n",
        "                is ``self.hparams``.\n",
        "            inputs (tuple, optional): (idx, json_file) Current loop index and path to json\n",
        "                file. Defaults to None.\n",
        "            num_files (int, optional): The total number of files to process. Used to display\n",
        "                a nice progress indicator. Defaults to 0.\n",
        "            processor (data.SentencesProcessor, optional): The :class:`data.SentencesProcessor`\n",
        "                object to convert the json file to usable features. Defaults to None.\n",
        "        \"\"\"\n",
        "        idx, json_file = inputs\n",
        "        logger.info(\"Processing %s (%i/%i)\", json_file, idx + 1, num_files)\n",
        "\n",
        "        # open current json file (which is a set of documents)\n",
        "        documents, file_path = load_json(json_file)\n",
        "\n",
        "        all_sources = []\n",
        "        all_ids = []\n",
        "        all_targets = []\n",
        "        for doc in documents:  # for each document in the json file\n",
        "            source = doc[\"src\"]\n",
        "            if \"tgt\" in doc:\n",
        "                target = doc[\"tgt\"]\n",
        "                all_targets.append(target)\n",
        "\n",
        "            ids = doc[\"labels\"]\n",
        "\n",
        "            all_sources.append(source)\n",
        "            all_ids.append(ids)\n",
        "\n",
        "        processor.add_examples(\n",
        "            all_sources,\n",
        "            labels=all_ids,\n",
        "            targets=all_targets if all_targets else None,\n",
        "            overwrite_examples=True,\n",
        "            overwrite_labels=True,\n",
        "        )\n",
        "\n",
        "        processor.get_features(\n",
        "            tokenizer,\n",
        "            bert_compatible_cls=hparams.processor_no_bert_compatible_cls,\n",
        "            create_segment_ids=hparams.create_token_type_ids,\n",
        "            sent_rep_token_id=\"cls\",\n",
        "            create_source=all_targets,  # create the source if targets were present\n",
        "            n_process=hparams.processing_num_threads,\n",
        "            max_length=(\n",
        "                hparams.max_seq_length\n",
        "                if hparams.max_seq_length\n",
        "                else self.tokenizer.model_max_length\n",
        "            ),\n",
        "            pad_on_left=self.tokenizer.padding_side == \"left\",\n",
        "            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
        "            return_type=\"lists\",\n",
        "            save_to_path=hparams.data_path,\n",
        "            save_to_name=os.path.basename(file_path),\n",
        "            save_as_type=hparams.data_type,\n",
        "        )\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"\n",
        "        Runs :meth:`~extractive.ExtractiveSummarizer.json_to_dataset` in parallel.\n",
        "        :meth:`~extractive.ExtractiveSummarizer.json_to_dataset` is the function that actually\n",
        "        loads and processes the examples as described below.\n",
        "        Algorithm: For each json file outputted by the ``convert_to_extractive.py`` script:\n",
        "\n",
        "        1. Load json file.\n",
        "        2. Add each document in json file to ``SentencesProcessor`` defined in ``self.processor``, overwriting any previous data in the processor.\n",
        "        3. Run :meth:`data.SentencesProcessor.get_features` to save the extracted features to disk as a ``.pt`` file containing a pickled python list of dictionaries, which each dictionary contains the extracted features.\n",
        "\n",
        "        Memory Usage Note: If sharding was turned off during the ``convert_to_extractive`` process\n",
        "        then this function will run once, loading the entire dataset into memory to process\n",
        "        just like the ``convert_to_extractive.py`` script.\n",
        "        \"\"\"\n",
        "\n",
        "        def get_inferred_data_type(dataset_files):\n",
        "            dataset_files_extensions = [os.path.splitext(x)[1] for x in dataset_files]\n",
        "            dataset_files_extensions_equal = len(set(dataset_files_extensions)) <= 1\n",
        "\n",
        "            if (\n",
        "                not dataset_files_extensions_equal\n",
        "            ) and self.hparams.data_type == \"none\":\n",
        "                logger.error(\n",
        "                    \"Cannot infer data file type because files with different extensions detected. Please set `--data_type`.\"\n",
        "                )\n",
        "                sys.exit(1)\n",
        "\n",
        "            most_common = None\n",
        "            if len(dataset_files_extensions) > 0:\n",
        "                # If the most common file extension found is not the specified data type\n",
        "                # then warn the user they may have chosen the wrong data type.\n",
        "                most_common = statistics.mode(dataset_files_extensions)[1:]\n",
        "                if (\n",
        "                    most_common != self.hparams.data_type\n",
        "                    and self.hparams.data_type != \"none\"\n",
        "                ):\n",
        "                    logger.warning(\n",
        "                        \"`--data_type` is '%s', but the most common file type detected in the `--data_path` is '%s'. Using '%s' as the type. Data will be processed if this type does not exist. Did you choose the correct data type?\",\n",
        "                        self.hparams.data_type,\n",
        "                        most_common,\n",
        "                        self.hparams.data_type,\n",
        "                    )\n",
        "\n",
        "            if len(dataset_files) == 0 and self.hparams.data_type == \"none\":\n",
        "                logger.error(\n",
        "                    \"Data is going to be processed, but you have not specified an output format. Set `--data_type` to the desired format.\"\n",
        "                )\n",
        "                sys.exit(1)\n",
        "\n",
        "            if self.hparams.data_type == \"none\":\n",
        "                inferred_data_type = most_common\n",
        "            else:\n",
        "                inferred_data_type = self.hparams.data_type\n",
        "\n",
        "            return inferred_data_type\n",
        "\n",
        "        datasets = {}\n",
        "\n",
        "        # loop through all data_splits\n",
        "        data_splits = [\n",
        "            self.hparams.train_name,\n",
        "            self.hparams.val_name,\n",
        "            self.hparams.test_name,\n",
        "        ]\n",
        "        for corpus_type in data_splits:\n",
        "            # get the current list of dataset files. if preprocessing has already happened\n",
        "            # then this will be the list of files that should be passed to an FSDataset.\n",
        "            # if preprocessing has not happened then `dataset_files` should be an empty list\n",
        "            # and the data will be processed\n",
        "            dataset_files = glob.glob(\n",
        "                os.path.join(self.hparams.data_path, \"*\" + corpus_type + \".*.*\")\n",
        "            )\n",
        "            # remove json files from glob results since they are unprocessed files\n",
        "            dataset_files = [x for x in dataset_files if \"json\" not in x]\n",
        "\n",
        "            inferred_data_type = get_inferred_data_type(dataset_files)\n",
        "\n",
        "            # rescan for dataset files after data type is determined\n",
        "            dataset_files = glob.glob(\n",
        "                os.path.join(\n",
        "                    self.hparams.data_path,\n",
        "                    \"*\" + corpus_type + \".*.\" + inferred_data_type,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # if no dataset files detected or model is set to `only_preprocess`\n",
        "            if (not dataset_files) or (self.hparams.only_preprocess):\n",
        "                json_files = glob.glob(\n",
        "                    os.path.join(self.hparams.data_path, \"*\" + corpus_type + \".*.json*\")\n",
        "                )\n",
        "                if len(json_files) == 0:\n",
        "                    logger.error(\n",
        "                        \"No JSON dataset files detected for %s split. Make sure the `--data_path` is correct.\",\n",
        "                        corpus_type,\n",
        "                    )\n",
        "                    sys.exit(1)\n",
        "\n",
        "                if self.hparams.preprocess_resume:\n",
        "                    completed_files = [\n",
        "                        os.path.splitext(os.path.basename(i))[0] for i in dataset_files\n",
        "                    ]\n",
        "                    logger.info(\"Not Processing Shards: %s\", completed_files)\n",
        "\n",
        "                    def remove_complete(doc):\n",
        "                        # if compression was enabled (files end in \".gz\") then remove the \".gz\"\n",
        "                        if doc.endswith(\".gz\"):\n",
        "                            doc = os.path.splitext(doc)[0]\n",
        "                        # remove the \".json\" extension\n",
        "                        doc = os.path.splitext(os.path.basename(doc))[0]\n",
        "\n",
        "                        # remove the document if it was already processed\n",
        "                        if doc in completed_files:\n",
        "                            return False  # remove\n",
        "                        return True  # keep\n",
        "\n",
        "                    json_files = list(filter(remove_complete, json_files))\n",
        "\n",
        "                num_files = len(json_files)\n",
        "\n",
        "                # pool = Pool(self.hparams.num_threads)\n",
        "                json_to_dataset_processor = partial(\n",
        "                    self.json_to_dataset,\n",
        "                    self.tokenizer,\n",
        "                    self.hparams,\n",
        "                    num_files=num_files,\n",
        "                    processor=self.processor,\n",
        "                )\n",
        "\n",
        "                for _ in map(\n",
        "                    json_to_dataset_processor, zip(range(len(json_files)), json_files),\n",
        "                ):\n",
        "                    pass\n",
        "                # pool.close()\n",
        "                # pool.join()\n",
        "\n",
        "                # since the dataset has been prepared, the processed dataset files should\n",
        "                # exist on disk. scan for final dataset files again.\n",
        "                dataset_files = glob.glob(\n",
        "                    os.path.join(\n",
        "                        self.hparams.data_path,\n",
        "                        \"*\" + corpus_type + \".*.\" + inferred_data_type,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            # if set to only preprocess the data then continue to next loop (aka next split of dataset)\n",
        "            if self.hparams.only_preprocess:\n",
        "                continue\n",
        "\n",
        "            # always create actual dataset, either after writing the shard  files to disk\n",
        "            # or by skipping that step (because preprocessed files detected) and going right to loading.\n",
        "            if self.hparams.dataloader_type == \"map\":\n",
        "                if inferred_data_type != \"txt\":\n",
        "                    logger.error(\n",
        "                        \"\"\"The `--dataloader_type` is 'map' but the `--data_type` was not \n",
        "                        inferred to be 'txt'. The map-style dataloader requires 'txt' data. \n",
        "                        Either set `--dataloader_type` to 'iterable' to use the old data \n",
        "                        format or process the JSON to TXT by setting `--data_type` to\n",
        "                        'txt'. Alternatively, you can convert directly from PT to TXT \n",
        "                        using `scripts/convert_extractive_pt_to_txt.py`.\"\"\"\n",
        "                    )\n",
        "                    sys.exit(1)\n",
        "                datasets[corpus_type] = FSDataset(dataset_files, verbose=True)\n",
        "            elif self.hparams.dataloader_type == \"iterable\":\n",
        "                # Since `FSIterableDataset` is an `IterableDataset` the `DataLoader` will ask\n",
        "                # the `Dataset` for the length instead of calculating it because the length\n",
        "                # of `IterableDatasets` might not be known, but it is in this case.\n",
        "                datasets[corpus_type] = FSIterableDataset(dataset_files, verbose=True)\n",
        "                # Force use one worker if using an iterable dataset to prevent duplicate data\n",
        "                self.hparams.dataloader_num_workers = 1\n",
        "\n",
        "        # if set to only preprocess the data then exit after all loops have been completed\n",
        "        if self.hparams.only_preprocess:\n",
        "            logger.warning(\n",
        "                \"Exiting since data has been preprocessed and written to disk and `hparams.only_preprocess` is True.\"\n",
        "            )\n",
        "            sys.exit(0)\n",
        "\n",
        "        self.datasets = datasets\n",
        "\n",
        "        # Create `pad_batch_collate` function\n",
        "        # If the model is a longformer then create the `global_attention_mask`\n",
        "        if self.hparams.model_type == \"longformer\":\n",
        "\n",
        "            self.pad_batch_collate = partial(\n",
        "                pad_batch_collate, modifier=longformer_modifier\n",
        "            )\n",
        "        else:\n",
        "            # default is to just use the normal `pad_batch_collate` function\n",
        "            self.pad_batch_collate = pad_batch_collate\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"Create dataloader for training if it has not already been created.\"\"\"\n",
        "        if self.train_dataloader_object:\n",
        "            return self.train_dataloader_object\n",
        "        if not hasattr(self, \"datasets\"):\n",
        "            self.prepare_data()\n",
        "        self.global_step_tracker = 0\n",
        "\n",
        "        train_dataset = self.datasets[self.hparams.train_name]\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            num_workers=self.hparams.dataloader_num_workers,\n",
        "            # sampler=train_sampler,\n",
        "            batch_size=self.hparams.batch_size,\n",
        "            collate_fn=self.pad_batch_collate,\n",
        "        )\n",
        "\n",
        "        self.train_dataloader_object = train_dataloader\n",
        "        return train_dataloader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"Create dataloader for validation.\"\"\"\n",
        "        valid_dataset = self.datasets[self.hparams.val_name]\n",
        "        valid_dataloader = DataLoader(\n",
        "            valid_dataset,\n",
        "            num_workers=self.hparams.dataloader_num_workers,\n",
        "            # sampler=valid_sampler,\n",
        "            batch_size=self.hparams.batch_size,\n",
        "            collate_fn=self.pad_batch_collate,\n",
        "        )\n",
        "        return valid_dataloader\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        \"\"\"Create dataloader for testing.\"\"\"\n",
        "        self.rouge_metrics = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
        "        self.rouge_scorer = rouge_scorer.RougeScorer(\n",
        "            self.rouge_metrics, use_stemmer=True\n",
        "        )\n",
        "        test_dataset = self.datasets[self.hparams.test_name]\n",
        "        test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            num_workers=self.hparams.dataloader_num_workers,\n",
        "            # sampler=test_sampler,\n",
        "            batch_size=self.hparams.batch_size,\n",
        "            collate_fn=self.pad_batch_collate,\n",
        "        )\n",
        "        return test_dataloader\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Configure the optimizers. Returns the optimizer and scheduler specified by\n",
        "        the values in ``self.hparams``.\n",
        "        \"\"\"\n",
        "        # create the train dataloader so the number of examples can be determined\n",
        "        self.train_dataloader_object = self.train_dataloader()\n",
        "\n",
        "        return generic_configure_optimizers(\n",
        "            self.hparams, self.train_dataloader_object, self.named_parameters()\n",
        "        )\n",
        "\n",
        "    def training_step(self, batch, batch_idx):  # skipcq: PYL-W0613\n",
        "        \"\"\"Training step: `PyTorch Lightning Documentation <https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.html#pytorch_lightning.core.LightningModule.training_step>`__\"\"\"\n",
        "        # Get batch information\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        # delete labels so now batch contains everything to be inputted into the model\n",
        "        del batch[\"labels\"]\n",
        "\n",
        "        # If global_step has increased by 1:\n",
        "        # Begin training the `word_embedding_model` after `num_frozen_steps` steps\n",
        "        if (self.global_step_tracker + 1) == self.trainer.global_step:\n",
        "            self.global_step_tracker = self.trainer.global_step\n",
        "\n",
        "            if self.emd_model_frozen and (\n",
        "                self.trainer.global_step > self.hparams.num_frozen_steps\n",
        "            ):\n",
        "                self.emd_model_frozen = False\n",
        "                self.unfreeze_web_model()\n",
        "\n",
        "        # Compute model forward\n",
        "        outputs, mask = self.forward(**batch)\n",
        "\n",
        "        # Compute loss\n",
        "        (\n",
        "            loss_total,\n",
        "            loss_total_norm_batch,\n",
        "            loss_avg_seq_sum,\n",
        "            loss_avg_seq_mean,\n",
        "            loss_avg,\n",
        "        ) = self.compute_loss(outputs, labels, mask)\n",
        "\n",
        "        # Generate logs\n",
        "        loss_dict = {\n",
        "            \"train_loss_total\": loss_total,\n",
        "            \"train_loss_total_norm_batch\": loss_total_norm_batch,\n",
        "            \"train_loss_avg_seq_sum\": loss_avg_seq_sum,\n",
        "            \"train_loss_avg_seq_mean\": loss_avg_seq_mean,\n",
        "            \"train_loss_avg\": loss_avg,\n",
        "        }\n",
        "\n",
        "        for name, value in loss_dict.items():\n",
        "            self.log(name, value, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        return loss_dict[\"train_\" + self.hparams.loss_key]\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):  # skipcq: PYL-W0613\n",
        "        \"\"\"\n",
        "        Validation step: `PyTorch Lightning Documentation <https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.html#pytorch_lightning.core.LightningModule.validation_step>`__\n",
        "        Similar to :meth:`~extractive.ExtractiveSummarizer.training_step` in that in runs the inputs\n",
        "        through the model. However, this method also calculates accuracy and f1 score by marking every\n",
        "        sentence score >0.5 as 1 (meaning should be in the summary) and each score <0.5 as 0 (meaning\n",
        "        should not be in the summary).\n",
        "        \"\"\"\n",
        "        # Get batch information\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        # delete labels so now batch contains everything to be inputted into the model\n",
        "        del batch[\"labels\"]\n",
        "\n",
        "        # Compute model forward\n",
        "        outputs, mask = self.forward(**batch)\n",
        "\n",
        "        # Compute loss\n",
        "        (\n",
        "            loss_total,\n",
        "            loss_total_norm_batch,\n",
        "            loss_avg_seq_sum,\n",
        "            loss_avg_seq_mean,\n",
        "            loss_avg,\n",
        "        ) = self.compute_loss(outputs, labels, mask)\n",
        "\n",
        "        # Compute accuracy metrics\n",
        "        y_hat = torch.sigmoid(outputs)\n",
        "        y_hat[y_hat > 0.5] = 1\n",
        "        y_hat[y_hat <= 0.5] = 0\n",
        "        y_hat = torch.flatten(y_hat)\n",
        "        y_true = torch.flatten(labels)\n",
        "        result = acc_and_f1(\n",
        "            y_hat.detach().cpu().numpy(), y_true.float().detach().cpu().numpy()\n",
        "        )\n",
        "        acc = torch.tensor(result[\"acc\"])\n",
        "        f1 = torch.tensor(result[\"f1\"])\n",
        "        acc_f1 = torch.tensor(result[\"acc_and_f1\"])\n",
        "\n",
        "        output = OrderedDict(\n",
        "            {\n",
        "                \"val_loss_total\": loss_total,\n",
        "                \"val_loss_total_norm_batch\": loss_total_norm_batch,\n",
        "                \"val_loss_avg_seq_sum\": loss_avg_seq_sum,\n",
        "                \"val_loss_avg_seq_mean\": loss_avg_seq_mean,\n",
        "                \"val_loss_avg\": loss_avg,\n",
        "                \"val_acc\": acc,\n",
        "                \"val_f1\": f1,\n",
        "                \"val_acc_and_f1\": acc_f1,\n",
        "            }\n",
        "        )\n",
        "        return output\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        \"\"\"\n",
        "        Called at the end of a validation epoch: `PyTorch Lightning Documentation <https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.html#pytorch_lightning.core.LightningModule.validation_epoch_end>`__\n",
        "        Finds the mean of all the metrics logged by :meth:`~extractive.ExtractiveSummarizer.validation_step`.\n",
        "        \"\"\"\n",
        "        # Get the average loss and accuracy metrics over all evaluation runs\n",
        "        avg_loss_total = torch.stack([x[\"val_loss_total\"] for x in outputs]).mean()\n",
        "        avg_loss_total_norm_batch = torch.stack(\n",
        "            [x[\"val_loss_total_norm_batch\"] for x in outputs]\n",
        "        ).mean()\n",
        "        avg_loss_avg_seq_sum = torch.stack(\n",
        "            [x[\"val_loss_avg_seq_sum\"] for x in outputs]\n",
        "        ).mean()\n",
        "        avg_loss_avg_seq_mean = torch.stack(\n",
        "            [x[\"val_loss_avg_seq_mean\"] for x in outputs]\n",
        "        ).mean()\n",
        "        avg_loss_avg = torch.stack([x[\"val_loss_avg\"] for x in outputs]).mean()\n",
        "        avg_val_acc = torch.stack([x[\"val_acc\"] for x in outputs]).mean()\n",
        "        avg_val_f1 = torch.stack([x[\"val_f1\"] for x in outputs]).mean()\n",
        "        avg_val_acc_and_f1 = torch.stack([x[\"val_acc_and_f1\"] for x in outputs]).mean()\n",
        "\n",
        "        # Generate logs\n",
        "        loss_dict = {\n",
        "            \"val_loss_total\": avg_loss_total,\n",
        "            \"val_loss_total_norm_batch\": avg_loss_total_norm_batch,\n",
        "            \"val_loss_avg_seq_sum\": avg_loss_avg_seq_sum,\n",
        "            \"val_loss_avg_seq_mean\": avg_loss_avg_seq_mean,\n",
        "            \"val_loss_avg\": avg_loss_avg,\n",
        "            \"val_acc\": avg_val_acc,\n",
        "            \"val_f1\": avg_val_f1,\n",
        "            \"val_acc_and_f1\": avg_val_acc_and_f1,\n",
        "        }\n",
        "\n",
        "        for name, value in loss_dict.items():\n",
        "            self.log(name, value, prog_bar=True, sync_dist=True)\n",
        "\n",
        "        self.log(\"val_loss\", loss_dict[\"val_\" + self.hparams.loss_key], sync_dist=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Test step: `PyTorch Lightning Documentation <https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.html#pytorch_lightning.core.LightningModule.test_step>`__\n",
        "        Similar to :meth:`~extractive.ExtractiveSummarizer.validation_step` in that in runs the inputs\n",
        "        through the model. However, this method also calculates the ROUGE scores for each example-summary\n",
        "        pair.\n",
        "        \"\"\"\n",
        "        # Get batch information\n",
        "        labels = batch[\"labels\"]\n",
        "        sources = batch[\"source\"]\n",
        "        targets = batch[\"target\"]\n",
        "\n",
        "        # delete labels, sources, and targets so now batch contains everything to be inputted into the model\n",
        "        del batch[\"labels\"]\n",
        "        del batch[\"source\"]\n",
        "        del batch[\"target\"]\n",
        "\n",
        "        # Compute model forward\n",
        "        outputs, _ = self.forward(**batch)\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "\n",
        "        # Compute accuracy metrics\n",
        "        y_hat = outputs.clone().detach()\n",
        "        y_hat[y_hat > 0.5] = 1\n",
        "        y_hat[y_hat <= 0.5] = 0\n",
        "        y_hat = torch.flatten(y_hat)\n",
        "        y_true = torch.flatten(labels)\n",
        "        result = acc_and_f1(\n",
        "            y_hat.detach().cpu().numpy(), y_true.float().detach().cpu().numpy()\n",
        "        )\n",
        "        acc = torch.tensor(result[\"acc\"])\n",
        "        f1 = torch.tensor(result[\"f1\"])\n",
        "        acc_f1 = torch.tensor(result[\"acc_and_f1\"])\n",
        "\n",
        "        sorted_ids = (\n",
        "            torch.argsort(outputs, dim=1, descending=True).detach().cpu().numpy()\n",
        "        )\n",
        "        if self.hparams.test_id_method == \"top_k\":\n",
        "            selected_ids = sorted_ids  # [:, : self.hparams.test_k]\n",
        "        elif self.hparams.test_id_method == \"greater_k\":\n",
        "            # `indexes` is sorted by original sentence order (sentences that appear first in the\n",
        "            # original document are first in the summary)\n",
        "            # if none of the rankings for a sample are greater than `test_k` then the top 3\n",
        "            # sorted by ranking are used\n",
        "            indexes = np.argwhere(outputs.detach().cpu().numpy() > self.hparams.test_k)\n",
        "            selected_ids = [[] for _ in range(outputs.size(0))]\n",
        "            previous_index = -1\n",
        "            # if the final document did not have any values greater than `hparams.test_k`\n",
        "            # then set it to the -1 (the skip token checked below)\n",
        "            final_index = outputs.size(0) - 1\n",
        "            if indexes.size == 0 or indexes[-1, 0] != final_index:\n",
        "                indexes = np.append(indexes, [[final_index, -1]], axis=0)\n",
        "\n",
        "            for index, value in indexes:\n",
        "                # if the index has changed and is not one greater then the previous then\n",
        "                # index was skipped because no elements greater than k\n",
        "                if (index not in (previous_index, previous_index + 1)) or value == -1:\n",
        "                    # For the first time the above loop runs, `previous_index` is -1 because no\n",
        "                    # no index has been checked yet. The -1 is necessary to check if the 0th\n",
        "                    # index is skipped. But, if the 0th index is skipped then the values need to be\n",
        "                    # added to the 0th index, not the -1st, so 1 is added to `previous_index` to\n",
        "                    # make it 0.\n",
        "                    if previous_index == -1:\n",
        "                        previous_index += 1\n",
        "                    # multiple entires might have been skipped\n",
        "                    num_skipped = index - previous_index\n",
        "                    for idx in range(num_skipped):\n",
        "                        # the index was skipped so add the top three for that index\n",
        "                        selected_ids[previous_index + idx] = sorted_ids[\n",
        "                            previous_index + idx, :3\n",
        "                        ].tolist()\n",
        "                # current entry was marked as skip\n",
        "                if value == -1:\n",
        "                    selected_ids[index] = sorted_ids[index, :3].tolist()\n",
        "                else:\n",
        "                    selected_ids[index].append(value)\n",
        "                previous_index = index\n",
        "        else:\n",
        "            logger.error(\n",
        "                \"%s is not a valid option for `--test_id_method`.\",\n",
        "                self.hparams.test_id_method,\n",
        "            )\n",
        "\n",
        "        rouge_outputs = []\n",
        "        predictions = []\n",
        "        # get ROUGE scores for each (source, target) pair\n",
        "        for idx, (source, source_ids, target) in enumerate(\n",
        "            zip(sources, selected_ids, targets)\n",
        "        ):\n",
        "            current_prediction = []\n",
        "            for sent_idx, i in enumerate(source_ids):\n",
        "                if i >= len(source):\n",
        "                    logger.debug(\n",
        "                        \"Only %i examples selected from document %i in batch %i. This is likely because some sentences received ranks so small they rounded to zero and a padding 'sentence' was randomly chosen.\",\n",
        "                        sent_idx + 1,\n",
        "                        idx,\n",
        "                        batch_idx,\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "                candidate = source[i].strip()\n",
        "                # If trigram blocking is enabled and searching for matching trigrams finds no matches\n",
        "                # then add the candidate to the current prediction list.\n",
        "                # During the predicting process, Trigram Blocking is used to reduce redundancy. Given\n",
        "                # selected summary S and a candidate sentence c, we will skip c is there exists a\n",
        "                # trigram overlapping between c and S.\n",
        "                if (not self.hparams.no_test_block_trigrams) and (\n",
        "                    not block_trigrams(candidate, current_prediction)\n",
        "                ):\n",
        "                    current_prediction.append(candidate)\n",
        "\n",
        "                # If the testing method is \"top_k\" and correct number of sentences have been\n",
        "                # added then break the loop and stop adding sentences. If the testing method\n",
        "                # is \"greater_k\" then we will continue to add all the sentences from `selected_ids`\n",
        "                if (self.hparams.test_id_method == \"top_k\") and (\n",
        "                    len(current_prediction) >= self.hparams.test_k\n",
        "                ):\n",
        "                    break\n",
        "\n",
        "            # See this issue https://github.com/google-research/google-research/issues/168\n",
        "            # for info about the differences between `pyrouge` and `rouge-score`.\n",
        "            # Archive Link: https://web.archive.org/web/20200622205503/https://github.com/google-research/google-research/issues/168\n",
        "            if self.hparams.test_use_pyrouge:\n",
        "                # Convert `current_prediction` from list to string with a \"<q>\" between each\n",
        "                # item/sentence. In ROUGE 1.5.5 (`pyrouge`), a \"\\n\" indicates sentence\n",
        "                # boundaries but the below \"save_gold.txt\" and \"save_pred.txt\" could not be\n",
        "                # created if each sentence had to be separated by a newline. Thus, each\n",
        "                # sentence is separated by a \"<q>\" token and is then converted to a newline\n",
        "                # in `helpers.test_rouge`.\n",
        "                current_prediction = \"<q>\".join(current_prediction)\n",
        "                predictions.append(current_prediction)\n",
        "            else:\n",
        "                # Convert `current_prediction` from list to string with a newline between each\n",
        "                # item/sentence. `rouge-score` splits sentences by newline.\n",
        "                current_prediction = \"\\n\".join(current_prediction)\n",
        "                target = target.replace(\"<q>\", \"\\n\")\n",
        "                rouge_outputs.append(\n",
        "                    self.rouge_scorer.score(target, current_prediction)\n",
        "                )\n",
        "\n",
        "        if self.hparams.test_use_pyrouge:\n",
        "            with open(\"save_gold.txt\", \"a+\") as save_gold, open(\n",
        "                \"save_pred.txt\", \"a+\"\n",
        "            ) as save_pred:\n",
        "                for i in enumerate(targets):\n",
        "                    save_gold.write(targets[i].strip() + \"\\n\")\n",
        "                for i in enumerate(predictions):\n",
        "                    save_pred.write(predictions[i].strip() + \"\\n\")\n",
        "\n",
        "        output = OrderedDict(\n",
        "            {\n",
        "                \"test_acc\": acc,\n",
        "                \"test_f1\": f1,\n",
        "                \"test_acc_and_f1\": acc_f1,\n",
        "                \"rouge_scores\": rouge_outputs,\n",
        "            }\n",
        "        )\n",
        "        return output\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        \"\"\"\n",
        "        Called at the end of a testing epoch: `PyTorch Lightning Documentation <https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.core.html#pytorch_lightning.core.LightningModule.test_epoch_end>`__\n",
        "        Finds the mean of all the metrics logged by :meth:`~extractive.ExtractiveSummarizer.test_step`.\n",
        "        \"\"\"\n",
        "        # Get the accuracy metrics over all testing runs\n",
        "        avg_test_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
        "        avg_test_f1 = torch.stack([x[\"test_f1\"] for x in outputs]).mean()\n",
        "        avg_test_acc_and_f1 = torch.stack(\n",
        "            [x[\"test_acc_and_f1\"] for x in outputs]\n",
        "        ).mean()\n",
        "\n",
        "        rouge_scores_log = {}\n",
        "\n",
        "        if self.hparams.test_use_pyrouge:\n",
        "            test_rouge(\"tmp\", \"save_pred.txt\", \"save_gold.txt\")\n",
        "        else:\n",
        "            aggregator = scoring.BootstrapAggregator()\n",
        "\n",
        "            # In `outputs` there is an entry for each batch that was passwed through the\n",
        "            # `test_step()` function. For each batch a list containing the rouge scores\n",
        "            # for each example exists under the key \"rouge_scores\" in `batch_list`. Thus,\n",
        "            # the below list comprehension loops through the list of outputs and grabs the\n",
        "            # items stored under the \"rouge_scores\" key. Then it flattens the list of lists\n",
        "            # to a list of rouge score objects that can be added to the `aggregator`.\n",
        "            rouge_scores_list = [\n",
        "                rouge_score_set\n",
        "                for batch_list in outputs\n",
        "                for rouge_score_set in batch_list[\"rouge_scores\"]\n",
        "            ]\n",
        "            for score in rouge_scores_list:\n",
        "                aggregator.add_scores(score)\n",
        "            # The aggregator returns a dictionary with keys coresponding to the rouge metric\n",
        "            # and values that are `AggregateScore` objects. Each `AggregateScore` object is a\n",
        "            # named tuple with a low, mid, and high value. Each value is a `Score` object, which\n",
        "            # is also a named tuple, that contains the precision, recall, and fmeasure values.\n",
        "            # For more info see the source code: https://github.com/google-research/google-research/blob/master/rouge/scoring.py\n",
        "            rouge_result = aggregator.aggregate()\n",
        "\n",
        "            for metric, value in rouge_result.items():\n",
        "                rouge_scores_log[metric + \"-precision\"] = value.mid.precision\n",
        "                rouge_scores_log[metric + \"-recall\"] = value.mid.recall\n",
        "                rouge_scores_log[metric + \"-fmeasure\"] = value.mid.fmeasure\n",
        "\n",
        "        # Generate logs\n",
        "        loss_dict = {\n",
        "            \"test_acc\": avg_test_acc,\n",
        "            \"test_f1\": avg_test_f1,\n",
        "            \"avg_test_acc_and_f1\": avg_test_acc_and_f1,\n",
        "        }\n",
        "\n",
        "        for name, value in loss_dict.items():\n",
        "            self.log(name, value, prog_bar=True, sync_dist=True)\n",
        "        for name, value in rouge_scores_log.items():\n",
        "            self.log(name, value, prog_bar=False, sync_dist=True)\n",
        "\n",
        "    def predict_sentences(\n",
        "        self,\n",
        "        input_sentences: Union[List[str], types.GeneratorType],\n",
        "        raw_scores=False,\n",
        "        num_summary_sentences=3,\n",
        "        tokenized=False,\n",
        "    ):\n",
        "        \"\"\"Summarizes ``input_sentences`` using the model.\n",
        "\n",
        "        Args:\n",
        "            input_sentences (list or generator): The sentences to be summarized as a\n",
        "                list or a generator of spacy Spans (``spacy.tokens.span.Span``), which\n",
        "                can be obtained by running ``nlp(\"input document\").sents`` where\n",
        "                ``nlp`` is a spacy model with a sentencizer.\n",
        "            raw_scores (bool, optional): Return a list containing each sentence\n",
        "                and its corespoding score instead of the summary. Defaults to False.\n",
        "            num_summary_sentences (int, optional): The number of sentences in the\n",
        "                output summary. This value specifies the number of top sentences to\n",
        "                select as the summary. Defaults to 3.\n",
        "            tokenized (bool, optional): If the input sentences are already tokenized\n",
        "                using spacy. If true, ``input_sentences`` should be a list of lists\n",
        "                where the outer list contains sentences and the inner lists contain\n",
        "                tokens. Defaults to False.\n",
        "\n",
        "        Returns:\n",
        "            str: The summary text. If ``raw_scores`` is set then returns a list\n",
        "            of input sentences and their corespoding scores.\n",
        "        \"\"\"\n",
        "        # Create source text.\n",
        "        # Don't add periods when joining because that creates a space before the period.\n",
        "        if tokenized:\n",
        "            src_txt = [\n",
        "                \" \".join([token.text for token in sentence if str(token) != \".\"]) + \".\"\n",
        "                for sentence in input_sentences\n",
        "            ]\n",
        "        else:\n",
        "            nlp = English()\n",
        "            #sentencizer = nlp.create_pipe(\"sentencizer\")\n",
        "            nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "            src_txt = [\n",
        "                \" \".join([token.text for token in nlp(sentence) if str(token) != \".\"])\n",
        "                + \".\"\n",
        "                for sentence in input_sentences\n",
        "            ]\n",
        "\n",
        "        input_ids = SentencesProcessor.get_input_ids(\n",
        "            self.tokenizer,\n",
        "            src_txt,\n",
        "            sep_token=self.tokenizer.sep_token,\n",
        "            cls_token=self.tokenizer.cls_token,\n",
        "            bert_compatible_cls=True,\n",
        "        )\n",
        "\n",
        "        input_ids = torch.tensor(input_ids)\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "        sent_rep_token_ids = [\n",
        "            i for i, t in enumerate(input_ids) if t == self.tokenizer.cls_token_id\n",
        "        ]\n",
        "        sent_rep_mask = torch.tensor([1] * len(sent_rep_token_ids))\n",
        "\n",
        "        input_ids.unsqueeze_(0)\n",
        "        attention_mask.unsqueeze_(0)\n",
        "        sent_rep_mask.unsqueeze_(0)\n",
        "\n",
        "        self.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs, _ = self.forward(\n",
        "                input_ids,\n",
        "                attention_mask,\n",
        "                sent_rep_mask=sent_rep_mask,\n",
        "                sent_rep_token_ids=sent_rep_token_ids,\n",
        "            )\n",
        "            outputs = torch.sigmoid(outputs)\n",
        "\n",
        "        if raw_scores:\n",
        "            # key=sentence\n",
        "            # value=score\n",
        "            sent_scores = list(zip(src_txt, outputs.tolist()[0]))\n",
        "            return sent_scores\n",
        "\n",
        "        sorted_ids = (\n",
        "            torch.argsort(outputs, dim=1, descending=True).detach().cpu().numpy()\n",
        "        )\n",
        "        logger.debug(\"Sorted sentence ids: %s\", sorted_ids)\n",
        "        selected_ids = sorted_ids[0, :num_summary_sentences]\n",
        "        logger.debug(\"Selected sentence ids: %s\", selected_ids)\n",
        "\n",
        "        selected_sents = []\n",
        "        for i in selected_ids:\n",
        "            selected_sents.append(src_txt[i])\n",
        "\n",
        "        return \" \".join(selected_sents).strip()\n",
        "\n",
        "    def predict(self, input_text: str, raw_scores=False, num_summary_sentences=3):\n",
        "        \"\"\"Summarizes ``input_text`` using the model.\n",
        "\n",
        "        Args:\n",
        "            input_text (str): The text to be summarized.\n",
        "            raw_scores (bool, optional): Return a list containing each sentence\n",
        "                and its corespoding score instead of the summary. Defaults to False.\n",
        "            num_summary_sentences (int, optional): The number of sentences in the\n",
        "                output summary. This value specifies the number of top sentences to\n",
        "                select as the summary. Defaults to 3.\n",
        "\n",
        "        Returns:\n",
        "            str: The summary text. If ``raw_scores`` is set then returns a list\n",
        "            of input sentences and their corespoding scores.\n",
        "        \"\"\"\n",
        "        nlp = English()\n",
        "        #sentencizer = nlp.create_pipe(\"sentencizer\")\n",
        "        nlp.add_pipe(\"sentencizer\")\n",
        "        doc = nlp(input_text)\n",
        "\n",
        "        return self.predict_sentences(\n",
        "            input_sentences=doc.sents,\n",
        "            raw_scores=raw_scores,\n",
        "            num_summary_sentences=num_summary_sentences,\n",
        "            tokenized=True,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        \"\"\"Arguments specific to this model\"\"\"\n",
        "        parser = ArgumentParser(parents=[parent_parser])\n",
        "        parser.add_argument(\n",
        "            \"--model_name_or_path\",\n",
        "            type=str,\n",
        "            default=\"bert-base-uncased\",\n",
        "            help=\"Path to pre-trained model or shortcut name. A list of shortcut names can be found at https://huggingface.co/transformers/pretrained_models.html. Community-uploaded models are located at https://huggingface.co/models.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--model_type\",\n",
        "            type=str,\n",
        "            default=\"bert\",\n",
        "            help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES),\n",
        "        )\n",
        "        parser.add_argument(\"--tokenizer_name\", type=str, default=\"\")\n",
        "        parser.add_argument(\n",
        "            \"--tokenizer_no_use_fast\",\n",
        "            action=\"store_true\",\n",
        "            help=\"Don't use the fast version of the tokenizer for the specified model. More info: https://huggingface.co/transformers/main_classes/tokenizer.html.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--max_seq_length\",\n",
        "            type=int,\n",
        "            default=0,\n",
        "            help=\"The maximum sequence length of processed documents.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--data_path\", type=str, help=\"Directory containing the dataset.\"\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--data_type\",\n",
        "            default=\"none\",\n",
        "            type=str,\n",
        "            choices=[\"txt\", \"pt\", \"none\"],\n",
        "            help=\"\"\"The file extension of the prepared data. The 'map' `--dataloader_type` \n",
        "            requires `txt` and the 'iterable' `--dataloader_type` works with both. If the data \n",
        "            is not prepared yet (in JSON format) this value specifies the output format \n",
        "            after processing. If the data is prepared, this value specifies the format to load. \n",
        "            If it is `none` then the type of data to be loaded will be inferred from the \n",
        "            `data_path`. If data needs to be prepared, this cannot be set to `none`.\"\"\",\n",
        "        )\n",
        "        parser.add_argument(\"--num_threads\", type=int, default=4)\n",
        "        parser.add_argument(\"--processing_num_threads\", type=int, default=2)\n",
        "        parser.add_argument(\n",
        "            \"--pooling_mode\",\n",
        "            type=str,\n",
        "            default=\"sent_rep_tokens\",\n",
        "            choices=[\"sent_rep_tokens\", \"mean_tokens\", \"max_tokens\"],\n",
        "            help=\"How word vectors should be converted to sentence embeddings.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--num_frozen_steps\",\n",
        "            type=int,\n",
        "            default=0,\n",
        "            help=\"Freeze (don't train) the word embedding model for this many steps.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--batch_size\",\n",
        "            default=8,\n",
        "            type=int,\n",
        "            help=\"Batch size per GPU/CPU for training/evaluation/testing.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--dataloader_type\",\n",
        "            default=\"map\",\n",
        "            type=str,\n",
        "            choices=[\"map\", \"iterable\"],\n",
        "            help=\"The style of dataloader to use. `map` is faster and uses less memory.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--dataloader_num_workers\",\n",
        "            default=4,\n",
        "            type=int,\n",
        "            help=\"\"\"The number of workers to use when loading data. A general place to \n",
        "            start is to set num_workers equal to the number of CPU cores on your machine. \n",
        "            If `--dataloader_type` is 'iterable' then this setting has no effect and \n",
        "            num_workers will be 1. More details here: https://pytorch-lightning.readthedocs.io/en/latest/performance.html#num-workers\"\"\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--processor_no_bert_compatible_cls\",\n",
        "            action=\"store_false\",\n",
        "            help=\"If model uses bert compatible [CLS] tokens for sentence representations.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--only_preprocess\",\n",
        "            action=\"store_true\",\n",
        "            help=\"\"\"Only preprocess and write the data to disk. Don't train model.\n",
        "            This will force data to be preprocessed, even if it was already computed and\n",
        "            is detected on disk, and any previous processed files will be overwritten.\"\"\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--preprocess_resume\",\n",
        "            action=\"store_true\",\n",
        "            help='Resume preprocessing. `--only_preprocess` must be set in order to resume. Determines which files to process by finding the shards that do not have a coresponding \".pt\" file in the data directory.',\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--create_token_type_ids\",\n",
        "            type=str,\n",
        "            choices=[\"binary\", \"sequential\"],\n",
        "            default=\"binary\",\n",
        "            help=\"Create token type ids during preprocessing.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--no_use_token_type_ids\",\n",
        "            action=\"store_true\",\n",
        "            help=\"Set to not train with `token_type_ids` (don't pass them into the model).\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--classifier\",\n",
        "            type=str,\n",
        "            choices=[\"linear\", \"simple_linear\", \"transformer\", \"transformer_linear\"],\n",
        "            default=\"simple_linear\",\n",
        "            help=\"\"\"Which classifier/encoder to use to reduce the hidden dimension of the sentence vectors.\n",
        "                    `linear` - a `LinearClassifier` with two linear layers, dropout, and an activation function.\n",
        "                    `simple_linear` - a `LinearClassifier` with one linear layer and a sigmoid.\n",
        "                    `transformer` - a `TransformerEncoderClassifier` which runs the sentence vectors through some\n",
        "                                    `nn.TransformerEncoderLayer`s and then a simple `nn.Linear` layer.\n",
        "                    `transformer_linear` - a `TransformerEncoderClassifier` with a `LinearClassifier` as the\n",
        "                                           `reduction` parameter, which results in the same thing as the `transformer` option but with a\n",
        "                                           `LinearClassifier` instead of a `nn.Linear` layer.\"\"\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--classifier_dropout\",\n",
        "            type=float,\n",
        "            default=0.1,\n",
        "            help=\"The value for the dropout layers in the classifier.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--classifier_transformer_num_layers\",\n",
        "            type=int,\n",
        "            default=2,\n",
        "            help='The number of layers for the `transformer` classifier. Only has an effect if `--classifier` contains \"transformer\".',\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--train_name\",\n",
        "            type=str,\n",
        "            default=\"train\",\n",
        "            help=\"name for set of training files on disk (for loading and saving)\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--val_name\",\n",
        "            type=str,\n",
        "            default=\"val\",\n",
        "            help=\"name for set of validation files on disk (for loading and saving)\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--test_name\",\n",
        "            type=str,\n",
        "            default=\"test\",\n",
        "            help=\"name for set of testing files on disk (for loading and saving)\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--test_id_method\",\n",
        "            type=str,\n",
        "            default=\"top_k\",\n",
        "            choices=[\"greater_k\", \"top_k\"],\n",
        "            help=\"How to chose the top predictions from the model for ROUGE scores.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--test_k\",\n",
        "            type=float,\n",
        "            default=3,\n",
        "            help=\"The `k` parameter for the `--test_id_method`. Must be set if using the `greater_k` option. (default: 3)\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--no_test_block_trigrams\",\n",
        "            action=\"store_true\",\n",
        "            help=\"Disable trigram blocking when calculating ROUGE scores during testing. This will increase repetition and thus decrease accuracy.\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--test_use_pyrouge\",\n",
        "            action=\"store_true\",\n",
        "            help=\"\"\"Use `pyrouge`, which is an interface to the official ROUGE software, instead of\n",
        "            the pure-python implementation provided by `rouge-score`. You must have the real ROUGE\n",
        "            package installed. More details about ROUGE 1.5.5 here: https://github.com/andersjo/pyrouge/tree/master/tools/ROUGE-1.5.5.\n",
        "            It is recommended to use this option for official scores. The `ROUGE-L` measurements\n",
        "            from `pyrouge` are equivalent to the `rougeLsum` measurements from the default\n",
        "            `rouge-score` package.\"\"\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--loss_key\",\n",
        "            type=str,\n",
        "            choices=[\n",
        "                \"loss_total\",\n",
        "                \"loss_total_norm_batch\",\n",
        "                \"loss_avg_seq_sum\",\n",
        "                \"loss_avg_seq_mean\",\n",
        "                \"loss_avg\",\n",
        "            ],\n",
        "            default=\"loss_avg_seq_mean\",\n",
        "            help=\"Which reduction method to use with BCELoss. See the `experiments/loss_functions/` folder for info on how the default (`loss_avg_seq_mean`) was chosen.\",\n",
        "        )\n",
        "        return parser\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing extractive.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h5vLMb20YuY7"
      },
      "source": [
        "!mv extractive.py transformersum/src/extractive.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "flQdjFSGYue3",
        "outputId": "d4bf574b-ef6f-4b40-bb65-3ddf242b87b3"
      },
      "source": [
        "%%writefile data.py\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import copy\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import linecache\n",
        "import torch\n",
        "import numpy as np\n",
        "from multiprocessing import Pool\n",
        "from functools import partial\n",
        "from helpers import pad\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def pad_batch_collate(batch, modifier=None):\n",
        "    r\"\"\"\n",
        "    Collate function to be passed to ``DataLoaders``. PyTorch Docs:\n",
        "    `https://pytorch.org/docs/stable/data.html#dataloader-collate-fn <https://pytorch.org/docs/stable/data.html#dataloader-collate-fn>`__\n",
        "\n",
        "    Calculates padding (per batch for efficiency) of ``labels`` and\n",
        "    ``token_type_ids`` if they exist within the batch from the ``Dataset``.\n",
        "    Also, pads ``sent_rep_token_ids`` and creates the ``sent_rep_mask`` to\n",
        "    indicate which numbers in the ``sent_rep_token_ids`` list are actually\n",
        "    the locations of sentence representation ids and which are padding.\n",
        "    Finally, calculates the ``attention_mask`` for each set of ``input_ids``\n",
        "    and pads both the ``attention_mask`` and the ``input_ids``. Converts all\n",
        "    inputs to tensors.\n",
        "\n",
        "    If ``sent_lengths`` are found then they will also automatically be\n",
        "    padded. However, the padding for sentence lengths is complicated. Each\n",
        "    list of sentence lengths needs to be the length of the longest list of\n",
        "    sentence lengths and the sum of all the lengths in each list needs to\n",
        "    add to the length of the input_ids width (the length of each input_id).\n",
        "    The second requirement exists because ``torch.split()`` (which is used\n",
        "    in the ``mean_tokens`` pooling algorithm to convert word vectors to\n",
        "    sentence embeddings in ``pooling.py``) will split a tensor into the\n",
        "    lengths requested but will error instead of returning any extra.\n",
        "    However, ``torch.split()`` will split a tensor into zero length\n",
        "    segments. Thus, to solve this, zeros are added to each sentence length\n",
        "    list for each example until one more padding value is needed to get the\n",
        "    maximum number of sentences. Once only one more value is needed, the\n",
        "    total value needded to reach the width of the ``input_ids`` is added.\n",
        "\n",
        "    ``source`` and ``target``, if present, are simply passed on without any\n",
        "    processing. Therefore, the standard ``collate_fn`` function for\n",
        "    ``DataLoader``\\ s will not work if these are present since they cannot\n",
        "    be converted to tensors without padding. This ``collate_fn`` must be\n",
        "    used if ``source`` or ``target`` is present in the loaded dataset.\n",
        "\n",
        "    The ``modifier`` argument accepts a function that takes the\n",
        "    ``final_dictionary`` and returns a modified ``final_dictionary``. The\n",
        "    ``modifier`` function will be called directly before\n",
        "    ``final_dictionary`` is returned in :meth:`~data.pad_batch_collate`. This allows\n",
        "    for easy extendability.\n",
        "    \"\"\"\n",
        "    elem = batch[0]\n",
        "    final_dictionary = {}\n",
        "\n",
        "    for key in elem:\n",
        "        # don't process `sent_lengths`\n",
        "        if key == \"sent_lengths\":\n",
        "            continue\n",
        "\n",
        "        feature_list = [d[key] for d in batch]\n",
        "        if key == \"sent_rep_token_ids\":\n",
        "            feature_list = pad(feature_list, -1)\n",
        "            sent_rep_token_ids = torch.tensor(feature_list)\n",
        "\n",
        "            sent_rep_mask = ~(sent_rep_token_ids == -1)\n",
        "            sent_rep_token_ids[sent_rep_token_ids == -1] = 0\n",
        "\n",
        "            final_dictionary[\"sent_rep_token_ids\"] = sent_rep_token_ids\n",
        "            final_dictionary[\"sent_rep_mask\"] = sent_rep_mask\n",
        "            continue  # go to next key\n",
        "        if key == \"input_ids\":\n",
        "            input_ids = feature_list\n",
        "\n",
        "            # Attention\n",
        "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "            # tokens are attended to.\n",
        "            attention_mask = [[1] * len(ids) for ids in input_ids]\n",
        "\n",
        "            input_ids_width = max([len(ids) for ids in input_ids])\n",
        "            input_ids = pad(input_ids, 0, width=input_ids_width)\n",
        "            input_ids = torch.tensor(input_ids)\n",
        "            attention_mask = pad(attention_mask, 0)\n",
        "            attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "            if \"sent_lengths\" in elem:\n",
        "                sent_lengths = []\n",
        "                sent_lengths_mask = []\n",
        "                sent_lengths_width = max([len(d[\"sent_lengths\"]) + 1 for d in batch])\n",
        "                for d in batch:\n",
        "                    current_sent_lens = d[\"sent_lengths\"]\n",
        "                    current_sent_lengths_mask = [True] * len(current_sent_lens)\n",
        "                    num_to_add = sent_lengths_width - len(current_sent_lens)\n",
        "                    total_value_to_add = input_ids_width - sum(current_sent_lens)\n",
        "                    while num_to_add > 1:\n",
        "                        num_to_add -= 1\n",
        "                        # total_value_to_add -= 1\n",
        "                        current_sent_lens.append(0)\n",
        "                        current_sent_lengths_mask.append(False)\n",
        "                    # if a value needs to be added to make `sum(current_sent_lens)` the total input\n",
        "                    # sequence length OR there is one more number to add (this can happen if the input\n",
        "                    # sequence exactly ends with a sentence, making the total of the lengths the length\n",
        "                    # of the sequence, or if there is one sentence that takes up the entire sequence)\n",
        "                    if total_value_to_add > 0 or num_to_add == 1:\n",
        "                        current_sent_lens.append(total_value_to_add)\n",
        "                        current_sent_lengths_mask.append(False)\n",
        "\n",
        "                    sent_lengths.append(current_sent_lens)\n",
        "                    sent_lengths_mask.append(current_sent_lengths_mask)\n",
        "                final_dictionary[\"sent_lengths\"] = sent_lengths\n",
        "                final_dictionary[\"sent_lengths_mask\"] = torch.tensor(sent_lengths_mask)\n",
        "\n",
        "            final_dictionary[\"input_ids\"] = input_ids\n",
        "            final_dictionary[\"attention_mask\"] = attention_mask\n",
        "\n",
        "            continue\n",
        "\n",
        "        if key in (\"source\", \"target\"):\n",
        "            final_dictionary[key] = feature_list\n",
        "            continue\n",
        "\n",
        "        if key in (\"labels\", \"token_type_ids\"):\n",
        "            feature_list = pad(feature_list, 0)\n",
        "\n",
        "        feature_list = torch.tensor(feature_list)\n",
        "        final_dictionary[key] = feature_list\n",
        "\n",
        "    if modifier:\n",
        "        final_dictionary = modifier(final_dictionary)\n",
        "\n",
        "    return final_dictionary\n",
        "\n",
        "\n",
        "class FSDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files_list, shuffle=True, verbose=False):\n",
        "        super(FSDataset).__init__()\n",
        "        if shuffle:\n",
        "            random.shuffle(files_list)  # happens in-place\n",
        "        self.files_list = files_list\n",
        "        self.shuffle = shuffle\n",
        "        self.verbose = verbose\n",
        "        self.lengths = self.get_files_lengths(files_list)\n",
        "\n",
        "    def get_files_lengths(self, files_list):\n",
        "        lengths = []\n",
        "        for data_file in files_list:\n",
        "            data_file_len = sum(1 for i in open(data_file, \"rb\"))\n",
        "            lengths.append(data_file_len)\n",
        "\n",
        "        lengths = np.cumsum(lengths)\n",
        "        self.lengths = lengths\n",
        "\n",
        "        return lengths\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_index = np.searchsorted(self.lengths, [index,], side=\"right\")[0]\n",
        "\n",
        "        if file_index > 0:\n",
        "            total_lines_in_other_files = self.lengths[file_index - 1]\n",
        "        else:\n",
        "            total_lines_in_other_files = 0\n",
        "        file_path = self.files_list[file_index]\n",
        "        linecache_index = index + 1  # linecache starts at 1\n",
        "        linecache_index -= (\n",
        "            total_lines_in_other_files  # remove lines counted from other files\n",
        "        )\n",
        "\n",
        "        line_str = linecache.getline(file_path, linecache_index).rstrip(\"\\n\")\n",
        "        try:\n",
        "            line_json = json.loads(line_str)\n",
        "        except:\n",
        "            print(\"** JSON Loading Error **\")\n",
        "            print(file_path)\n",
        "            print(index)\n",
        "            print(linecache_index)\n",
        "            print(line_str)\n",
        "        return line_json\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.lengths[-1]\n",
        "\n",
        "\n",
        "class FSIterableDataset(torch.utils.data.IterableDataset):\n",
        "    \"\"\"\n",
        "    A dataset to yield examples from a list of files that are saved python\n",
        "    objects that can be iterated over. These files could be other PyTorch\n",
        "    datasets (tested with ``TensorDataset``) or other python objects such as\n",
        "    lists, for example. Each file will be loaded one at a time until all the\n",
        "    examples have been yielded, at which point the next file will be loaded\n",
        "    and used to yield examples, and so on. This means a large dataset can be\n",
        "    broken into smaller chunks and this class can be used to load samples as\n",
        "    if those files were one dataset while only utilizing the ram required\n",
        "    for one chunk.\n",
        "\n",
        "    Explanation about ``batch_size`` and ``__len__()``: If the ``len()``\n",
        "    function is needed to be accurate then the ``batch_size`` must be\n",
        "    specified when constructing objects of this class. PyTorch\n",
        "    ``DataLoader`` objects will report accurate lengths by dividing the\n",
        "    number of examples in the dataset by the batch size only if the dataset\n",
        "    if not an ``IterableDataset``. If the dataset is an ``IterableDataset``\n",
        "    then a ``DataLoader`` will simply ask the dataset for its length,\n",
        "    without diving by the batch size, because in some cases the length of an\n",
        "    ``IterableDataset`` might be difficult or impossible to determine.\n",
        "    However, in this case the number of examples (length of dataset) is\n",
        "    known. The division by batch size must happen in the dataset (for\n",
        "    datasets of type ``IterableDataset``) since the ``DataLoader`` will not\n",
        "    calculate this.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Add shuffling\n",
        "    def __init__(self, files_list, shuffle=True, verbose=False):\n",
        "        super(FSIterableDataset).__init__()\n",
        "        if shuffle:\n",
        "            random.shuffle(files_list)  # happens in-place\n",
        "        self.files_list = files_list\n",
        "        self.shuffle = shuffle\n",
        "        self.verbose = verbose\n",
        "        self.total_length = None\n",
        "        self.file_type = os.path.splitext(files_list[0])[1]\n",
        "\n",
        "    def __iter__(self):\n",
        "        for data_file in self.files_list:\n",
        "            if self.verbose:\n",
        "                logger.info(\"Loading examples from %s\", data_file)\n",
        "\n",
        "            if self.file_type == \".pt\":\n",
        "                dataset_section = torch.load(data_file)\n",
        "            elif self.file_type == \".txt\":\n",
        "                with open(data_file, \"r\") as file:\n",
        "                    dataset_section = [json.loads(x) for x in file.readlines()]\n",
        "\n",
        "            for example in dataset_section:\n",
        "                yield example\n",
        "                # input(example)\n",
        "            # Clear memory usage before loading next file\n",
        "            dataset_section = None\n",
        "            gc.collect()\n",
        "            del dataset_section\n",
        "            gc.collect()\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.total_length:\n",
        "            return self.total_length\n",
        "\n",
        "        logger.debug(\n",
        "            \"Calculating length of `IterableDataset` by loading each file, getting the length, and unloading, which is slow.\"\n",
        "        )\n",
        "        total_length = 0\n",
        "        for data_file in self.files_list:\n",
        "            if self.file_type == \".pt\":\n",
        "                dataset_section = torch.load(data_file)\n",
        "            elif self.file_type == \".txt\":\n",
        "                with open(data_file, \"r\") as file:\n",
        "                    dataset_section = file.readlines()\n",
        "            total_length += len(dataset_section)\n",
        "\n",
        "        self.total_length = total_length\n",
        "        return total_length\n",
        "\n",
        "\n",
        "class InputExample:\n",
        "    def __init__(self, text, labels, guid=None, target=None):\n",
        "        \"\"\"A single training/test example for simple sequence classification.\n",
        "\n",
        "        Arguments:\n",
        "            text {list} -- The untokenized (for the appropriate model) text for the example.\n",
        "                             Should be broken into sentences and tokens.\n",
        "            labels {list} -- The labels of the example.\n",
        "\n",
        "        Keyword Arguments:\n",
        "            guid {int} -- A unique identification code for this example, not used. Default is None.\n",
        "            target {str} -- The ground truth abstractive summary. Default is None.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text = text\n",
        "        self.labels = labels\n",
        "        self.target = target\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class InputFeatures:\n",
        "    \"\"\"\n",
        "    A single set of features of data.\n",
        "\n",
        "    Args:\n",
        "        input_ids: Indices of input sequence tokens in the vocabulary.\n",
        "        attention_mask: Mask to avoid performing attention on padding token indices.\n",
        "            Mask values selected in `[0, 1]`:\n",
        "            Usually  `1` for tokens that are NOT MASKED, `0` for MASKED (padded) tokens.\n",
        "        token_type_ids: Usually, segment token indices to indicate first and second portions\n",
        "            of the inputs. However, for summarization they are used to indicate different\n",
        "            sentences. Depending on the size of the token type id vocabulary, these values\n",
        "            may alternate between ``0`` and ``1`` or they may increase sequentially for each\n",
        "            sentence in the input.\n",
        "        labels: Labels corresponding to the input.\n",
        "        sent_rep_token_ids: The locations of the sentence representation tokens.\n",
        "        sent_lengths: A list of the lengths of each sentence in the `source` and `input_ids`.\n",
        "        source: The actual source document as a list of sentences.\n",
        "        target: The ground truth abstractive summary.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_ids,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        labels=None,\n",
        "        sent_rep_token_ids=None,\n",
        "        sent_lengths=None,\n",
        "        source=None,\n",
        "        target=None,\n",
        "    ):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.labels = labels\n",
        "        self.sent_rep_token_ids = sent_rep_token_ids\n",
        "        self.sent_lengths = sent_lengths\n",
        "        self.source = source\n",
        "        self.target = target\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        # output = copy.deepcopy(self.__dict__)\n",
        "        _dict = self.__dict__\n",
        "        # removes empty and NoneType properties from `self.__dict__`\n",
        "        output = {}\n",
        "        for key, value in _dict.items():\n",
        "            if value:\n",
        "                output[key] = value\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class SentencesProcessor:\n",
        "    r\"\"\"Create a `SentencesProcessor`\n",
        "\n",
        "    Arguments:\n",
        "        name (str, optional): A label for the ``SentencesProcessor`` object, used internally for saving if\n",
        "            a save name is not specified in :meth:`data.SentencesProcessor.get_features`, Default is None.\n",
        "        labels (list, optional): The label that goes with each sample, can be a list of lists where\n",
        "            the inside lists are the labels for each sentence in the coresponding\n",
        "            example. Default is None.\n",
        "        examples (list, optional): List of ``InputExample``\\ s. Default is None.\n",
        "        verbose (bool, optional): Log extra information (such as examples of processed data points). Default is False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name=None, labels=None, examples=None, verbose=False):\n",
        "        self.name = name\n",
        "        self.labels = [] if labels is None else labels\n",
        "        self.examples = [] if examples is None else examples\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    @classmethod\n",
        "    def create_from_examples(cls, texts, labels=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Create a SentencesProcessor with ``**kwargs`` and add ``texts`` and `labels`` through\n",
        "        :meth:`~data.SentencesProcessor.add_examples`.\n",
        "        \"\"\"\n",
        "        processor = cls(**kwargs)\n",
        "        processor.add_examples(texts, labels=labels)\n",
        "        return processor\n",
        "\n",
        "    @classmethod\n",
        "    def get_input_ids(\n",
        "        cls,\n",
        "        tokenizer,\n",
        "        src_txt,\n",
        "        bert_compatible_cls=True,\n",
        "        sep_token=None,\n",
        "        cls_token=None,\n",
        "        max_length=512,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Get ``input_ids`` from ``src_txt`` using ``tokenizer``. See\n",
        "        :meth:`~data.SentencesProcessor.get_features` for more info.\n",
        "        \"\"\"\n",
        "        sep_token = str(sep_token)\n",
        "        cls_token = str(cls_token)\n",
        "        if max_length is None:\n",
        "            max_length = tokenizer.max_len\n",
        "\n",
        "        if max_length > 1_000_000:\n",
        "            logger.warning(\n",
        "                \"Tokenizer maximum length is greater than 1,000,000. This is likely a mistake. Resetting to 512 tokens.\"\n",
        "            )\n",
        "            max_length = 512\n",
        "\n",
        "        # adds a '[CLS]' token between each sentence and outputs `input_ids`\n",
        "        if bert_compatible_cls:\n",
        "            # If the CLS or SEP tokens exist in the document as part of the dataset, then\n",
        "            # set them to UNK\n",
        "            unk_token = str(tokenizer.unk_token)\n",
        "            src_txt = [\n",
        "                sent.replace(sep_token, unk_token).replace(cls_token, unk_token)\n",
        "                for sent in src_txt\n",
        "            ]\n",
        "\n",
        "            if not len(src_txt) < 2:  # if there is NOT 1 sentence\n",
        "                # separate each sentence with ' [SEP] [CLS] ' (or model equivalent tokens) and convert to string\n",
        "                separation_string = \" \" + sep_token + \" \" + cls_token + \" \"\n",
        "                text = separation_string.join(src_txt)\n",
        "            else:\n",
        "                try:\n",
        "                    text = src_txt[0]\n",
        "                except IndexError:\n",
        "                    text = src_txt\n",
        "\n",
        "            # tokenize\n",
        "            src_subtokens = tokenizer.tokenize(text)\n",
        "            # select first `(max_length-2)` tokens (so the following line of tokens can be added)\n",
        "            src_subtokens = src_subtokens[: (max_length - 2)]\n",
        "            # Insert '[CLS]' at beginning and append '[SEP]' to end (or model equivalent tokens)\n",
        "            src_subtokens.insert(0, cls_token)\n",
        "            src_subtokens.append(sep_token)\n",
        "            # create `input_ids`\n",
        "            input_ids = tokenizer.convert_tokens_to_ids(src_subtokens)\n",
        "        else:\n",
        "            input_ids = tokenizer.encode(\n",
        "                src_txt,\n",
        "                add_special_tokens=True,\n",
        "                max_length=min(max_length, tokenizer.max_len),\n",
        "            )\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "    def add_examples(\n",
        "        self,\n",
        "        texts,\n",
        "        labels=None,\n",
        "        ids=None,\n",
        "        oracle_ids=None,\n",
        "        targets=None,\n",
        "        overwrite_labels=False,\n",
        "        overwrite_examples=False,\n",
        "    ):\n",
        "        r\"\"\"Primary method of adding example sets of texts, labels, ids, and targets\n",
        "        to the ``SentencesProcessor``\n",
        "\n",
        "        Arguments:\n",
        "            texts (list): A list of documents where each document is a list of sentences where each\n",
        "                            sentence is a list of tokens. This is the output of `convert_to_extractive.py`\n",
        "                            and is in the 'src' field for each doc. See :meth:`extractive.ExtractiveSummarizer.prepare_data`.\n",
        "            labels (list, optional): A list of the labels for each document where each label is a list of labels\n",
        "                where the index of the label coresponds with the index of the sentence in the\n",
        "                respective entry in `texts.` Similarly to `texts`, this is handled automatically\n",
        "                by `ExtractiveSummarizer.prepare_data`. Default is None.\n",
        "            ids (list, optional): A list of ids for each document. Not used by `ExtractiveSummarizer`. Default is None.\n",
        "            oracle_ids (list, optional): Similar to labels but is a list of indexes of the chosen sentences\n",
        "                instead of a one-hot encoded vector. These will be converted to labels. Default is None.\n",
        "            targets (list, optional): A list of the abstractive target for each document. Default is None.\n",
        "            overwrite_labels (bool, optional): Replace any labels currently stored by the ``SentencesProcessor``. Default is False.\n",
        "            overwrite_examples (bool, optional): Replace any examples currently stored by the ``SentencesProcessor``. Default is False.\n",
        "\n",
        "        Returns:\n",
        "            list: The examples as ``InputExample``\\ s that have been added.\n",
        "        \"\"\"\n",
        "        assert texts  # not an empty array\n",
        "        assert labels is None or len(texts) == len(labels)\n",
        "        assert ids is None or len(texts) == len(ids)\n",
        "        assert not (labels and oracle_ids)\n",
        "        assert isinstance(texts, list)\n",
        "\n",
        "        if ids is None:\n",
        "            ids = [None] * len(texts)\n",
        "        if labels is None:\n",
        "            if oracle_ids:  # convert `oracle_ids` to `labels`\n",
        "                labels = []\n",
        "                for text_set, oracle_id in zip(texts, oracle_ids):\n",
        "                    text_label = [0] * len(text_set)\n",
        "                    for l in oracle_id:\n",
        "                        text_label[l] = 1\n",
        "                    labels.append(text_label)\n",
        "            else:\n",
        "                labels = [None] * len(texts)\n",
        "\n",
        "        examples = []\n",
        "        added_labels = []\n",
        "        for idx, (text_set, label_set, guid) in enumerate(zip(texts, labels, ids)):\n",
        "            if not text_set or not label_set:\n",
        "                continue  # input()\n",
        "            added_labels.append(label_set)\n",
        "            if targets:\n",
        "                example = InputExample(\n",
        "                    guid=guid, text=text_set, labels=label_set, target=targets[idx]\n",
        "                )\n",
        "            else:\n",
        "                example = InputExample(guid=guid, text=text_set, labels=label_set)\n",
        "            examples.append(example)\n",
        "\n",
        "        # Update examples\n",
        "        if overwrite_examples:\n",
        "            self.examples = examples\n",
        "        else:\n",
        "            self.examples.extend(examples)\n",
        "\n",
        "        # Update labels\n",
        "        if overwrite_labels:\n",
        "            self.labels = added_labels\n",
        "        else:\n",
        "            self.labels += added_labels\n",
        "\n",
        "        return self.examples\n",
        "\n",
        "    def get_features_process(\n",
        "        self,\n",
        "        input_information,\n",
        "        num_examples=0,\n",
        "        tokenizer=None,\n",
        "        bert_compatible_cls=True,\n",
        "        sep_token=None,\n",
        "        cls_token=None,\n",
        "        create_sent_rep_token_ids=True,\n",
        "        sent_rep_token_id=None,\n",
        "        create_sent_lengths=True,\n",
        "        create_segment_ids=\"binary\",\n",
        "        segment_token_id=None,\n",
        "        create_source=False,\n",
        "        max_length=None,\n",
        "        pad_on_left=False,\n",
        "        pad_token=0,\n",
        "        mask_padding_with_zero=True,\n",
        "        create_attention_mask=True,\n",
        "        pad_ids_and_attention=True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        The process that actually creates the features. :meth:`~data.SentencesProcessor.get_features`\n",
        "        is the driving function, look there for a description of how this function works. This\n",
        "        function only exists so that processing can easily be done in parallel using ``Pool.map``.\n",
        "        \"\"\"\n",
        "        ex_index, example, label = input_information\n",
        "        if ex_index % 1000 == 0:\n",
        "            logger.info(\"Generating features for example %s/%s\", ex_index, num_examples)\n",
        "        if bert_compatible_cls:\n",
        "            # convert `example.text` to array of sentences\n",
        "            src_txt = (\" \".join(sent) for sent in example.text)\n",
        "        else:\n",
        "            src_txt = example.text\n",
        "\n",
        "        input_ids = self.get_input_ids(\n",
        "            tokenizer, src_txt, bert_compatible_cls, sep_token, cls_token, max_length\n",
        "        )\n",
        "\n",
        "        # Segment (Token Type) IDs\n",
        "        segment_ids = None\n",
        "        if create_segment_ids == \"binary\":\n",
        "            current_segment_flag = True\n",
        "            segment_ids = []\n",
        "            for token in input_ids:\n",
        "                segment_ids += [0 if current_segment_flag else 1]\n",
        "                if token == segment_token_id:\n",
        "                    current_segment_flag = not current_segment_flag\n",
        "\n",
        "        if create_segment_ids == \"sequential\":\n",
        "            current_segment = 0\n",
        "            segment_ids = []\n",
        "            for token in input_ids:\n",
        "                segment_ids += [current_segment]\n",
        "                if token == segment_token_id:\n",
        "                    current_segment += 1\n",
        "\n",
        "        # Sentence Representation Token IDs and Sentence Lengths\n",
        "        sent_rep_ids = None\n",
        "        sent_lengths = None\n",
        "        if create_sent_rep_token_ids:\n",
        "            # create list of indexes for the `sent_rep` tokens\n",
        "            sent_rep_ids = [\n",
        "                i for i, t in enumerate(input_ids) if t == sent_rep_token_id\n",
        "            ]\n",
        "            # truncate `label` to the length of the `sent_rep_ids` aka the number of sentences\n",
        "            label = label[: len(sent_rep_ids)]\n",
        "\n",
        "            if create_sent_lengths:\n",
        "                # if there are 1 or 0 sentences then the length of the entire sequence will be\n",
        "                # the only value in `sent_lengths`\n",
        "                if len(sent_rep_ids) < 2:\n",
        "                    sent_lengths = [len(input_ids)]\n",
        "                else:\n",
        "                    sent_lengths = [\n",
        "                        sent_rep_ids[i] - sent_rep_ids[i - 1]\n",
        "                        for i in range(1, len(sent_rep_ids))\n",
        "                    ]\n",
        "                    # Add sentence length for the last sentence, if missing.\n",
        "                    # If the last sentence representation token position in `input_ids` is not\n",
        "                    # the last token in `input_ids` then add the length of the last sentence\n",
        "                    # to `sent_lengths` by subtracting the position of the last `sent_rep_token`\n",
        "                    # from the length of `input_ids`\n",
        "                    if sent_rep_ids[-1] != len(input_ids) - 1:\n",
        "                        sent_lengths.append(len(input_ids) - sent_rep_ids[-1])\n",
        "                    # Add sentence length for the first sentence, if missing.\n",
        "                    # If the first sentence representation token is not the first token in\n",
        "                    # `input_ids` then add the length of the first sentence by inserting\n",
        "                    # the first value in `sent_rep_ids` at the front of `sent_lengths`.\n",
        "                    if sent_rep_ids[0] != 0:\n",
        "                        sent_lengths.insert(0, sent_rep_ids[0] + 1)\n",
        "\n",
        "        # Attention\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        if create_attention_mask:\n",
        "            attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Padding\n",
        "        # Zero-pad up to the sequence length.\n",
        "        if pad_ids_and_attention:\n",
        "            padding_length = max_length - len(input_ids)\n",
        "            if pad_on_left:\n",
        "                input_ids = ([pad_token] * padding_length) + input_ids\n",
        "                attention_mask = (\n",
        "                    [0 if mask_padding_with_zero else 1] * padding_length\n",
        "                ) + attention_mask\n",
        "            else:\n",
        "                input_ids = input_ids + ([pad_token] * padding_length)\n",
        "                attention_mask = attention_mask + (\n",
        "                    [0 if mask_padding_with_zero else 1] * padding_length\n",
        "                )\n",
        "\n",
        "            assert (\n",
        "                len(input_ids) == max_length\n",
        "            ), \"Error with input length {} vs {}\".format(len(input_ids), max_length)\n",
        "            assert (\n",
        "                len(attention_mask) == max_length\n",
        "            ), \"Error with input length {} vs {}\".format(\n",
        "                len(attention_mask), max_length\n",
        "            )\n",
        "\n",
        "        if ex_index < 5 and self.verbose:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\", example.guid)\n",
        "            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
        "            if segment_ids is not None:\n",
        "                logger.info(\n",
        "                    \"token_type_ids: %s\", \" \".join([str(x) for x in segment_ids])\n",
        "                )\n",
        "            if sent_rep_ids is not None:\n",
        "                logger.info(\n",
        "                    \"sent_rep_token_ids: %s\", \" \".join([str(x) for x in sent_rep_ids])\n",
        "                )\n",
        "            if sent_lengths is not None:\n",
        "                logger.info(\n",
        "                    \"sent_lengths: %s\", \" \".join([str(x) for x in sent_lengths])\n",
        "                )\n",
        "            if create_attention_mask:\n",
        "                logger.info(\n",
        "                    \"attention_mask: %s\", \" \".join([str(x) for x in attention_mask])\n",
        "                )\n",
        "            logger.info(\"labels: %s (id = %s)\", example.labels, label)\n",
        "\n",
        "        # Return features\n",
        "        # if the attention mask was created then add the mask to the returned features\n",
        "        outputs = {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"labels\": label,\n",
        "            \"token_type_ids\": segment_ids,\n",
        "            \"sent_rep_token_ids\": sent_rep_ids,\n",
        "            \"sent_lengths\": sent_lengths,\n",
        "            \"target\": example.target,\n",
        "        }\n",
        "        if create_attention_mask:\n",
        "            outputs[\"attention_mask\"] = attention_mask\n",
        "        if create_source:\n",
        "            # convert form individual tokens to only individual sentences\n",
        "            source = [\" \".join(sentence) for sentence in example.text]\n",
        "            outputs[\"source\"] = source\n",
        "\n",
        "        return InputFeatures(**outputs)\n",
        "\n",
        "    def get_features(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        bert_compatible_cls=True,\n",
        "        create_sent_rep_token_ids=True,\n",
        "        sent_rep_token_id=None,\n",
        "        create_sent_lengths=True,\n",
        "        create_segment_ids=\"binary\",\n",
        "        segment_token_id=None,\n",
        "        create_source=False,\n",
        "        n_process=2,\n",
        "        max_length=None,\n",
        "        pad_on_left=False,\n",
        "        pad_token=0,\n",
        "        mask_padding_with_zero=True,\n",
        "        create_attention_mask=True,\n",
        "        pad_ids_and_attention=True,\n",
        "        return_type=None,\n",
        "        save_to_path=None,\n",
        "        save_to_name=None,\n",
        "        save_as_type=\"txt\",\n",
        "    ):\n",
        "        r\"\"\"Convert the examples stored by the ``SentencesProcessor`` to features that can be used by\n",
        "        a model. The following processes can be performed: tokenization, token type ids (to separate\n",
        "        sentences), sentence representation token ids (the locations of each sentence representation\n",
        "        token), sentence lengths, and the attention mask. Padding can be applied to the tokenized\n",
        "        examples and the attention masks but it is recommended to instead use the\n",
        "        :meth:`data.pad_batch_collate` function so each batch is padded individually for efficiency\n",
        "        (less zeros passed through model).\n",
        "\n",
        "        Arguments:\n",
        "            tokenizer (transformers.PreTrainedTokenizer): The tokenizer used to tokenize the examples.\n",
        "            bert_compatible_cls (bool, optional): Adds '[CLS]' tokens in front of each sentence. This is useful\n",
        "                so that the '[CLS]' token can be used to obtain sentence\n",
        "                embeddings. This only works if the chosen model has the '[CLS]'\n",
        "                token in its vocabulary. Default is True.\n",
        "            create_sent_rep_token_ids (bool, optional): Option to create sentence representation token ids. This\n",
        "                will store a list of the indexes of all the ``sent_rep_token_id``\\ s\n",
        "                in the tokenized example. Default is True.\n",
        "            sent_rep_token_id ([type], optional): The token id that should be captured for each sentence (should have\n",
        "                one per sentence and each should represent that sentence).\n",
        "                Default is ``'[CLS]' token if bert_compatible_cls else '[SEP]' token``.\n",
        "            create_sent_lengths (bool, optional): Option to create a list of sentence lengths where each index in\n",
        "                the list coresponds to the respective sentence in the example. Default is True.\n",
        "            create_segment_ids (str, optional): Option to create segment ids (aka token type ids).\n",
        "                See https://huggingface.co/transformers/glossary.html#token-type-ids for more info.\n",
        "                Set to either \"binary\", \"sequential\", or False.\n",
        "\n",
        "                * ``binary`` alternates between 0 and 1 for each sentence.\n",
        "                * ``sequential`` starts at 0 and increments by 1 for each sentence.\n",
        "                * ``False`` does not create any segment ids.\n",
        "\n",
        "                Note: Many pretrained models that accept token type ids use them\n",
        "                for question answering ans related tasks where the model receives\n",
        "                two inputs. Therefore, most models have a token type id vocabulary\n",
        "                size of 2, which means they only have learned 2 token type ids. The\n",
        "                \"binary\" mode exists so that these pretrained models can easily\n",
        "                be used.\n",
        "                Default is \"binary\".\n",
        "            segment_token_id (str, optional): The token id to be used when creating segment ids. Can be set to 'period'\n",
        "                to treat periods as sentence separation tokens, but this is a terrible\n",
        "                idea for obvious reasons. Default is '[SEP]' token id.\n",
        "            create_source (bool, optional): Option to save the source text (non-tokenized) as a string. Default is False.\n",
        "            n_process (int, optional): How many processes to use for multithreading for running get_features_process().\n",
        "                Set higher to run faster and set lower is you experience OOM issues. Default is 2.\n",
        "            max_length (int, optional): If ``pad_ids_and_attention`` is True then pad to this amount. Default is ``tokenizer.max_len``.\n",
        "            pad_on_left (bool, optional): Optionally, pad on the left instead of right. Default is False.\n",
        "            pad_token (int, optional): Which token to use for padding the ``input_ids``. Default is 0.\n",
        "            mask_padding_with_zero (bool, optional): Use zeros to pad the attention. Uses ones otherwise. Default is True.\n",
        "            create_attention_mask (bool, optional): Option to create the attention mask. It is recommended to use\n",
        "                the :meth:`data.pad_batch_collate` function, which will automatically create\n",
        "                attention masks and pad them on a per batch level. Default is ``False if return_type == \"lists\" else True``.\n",
        "            pad_ids_and_attention (bool, optional): Pad the ``input_ids`` with ``pad_token`` and attention masks\n",
        "                with 0s or 1s deneding on ``mask_padding_with_zero``. Pad both to\n",
        "                ``max_length``. Default is ``False if return_type == \"lists\" else True``\n",
        "            return_type (str, optional): Either \"tensors\", \"lists\", or None. See \"Returns\" section below. Default is None.\n",
        "            save_to_path (str, optional): The folder/directory to save the data to OR None to not save.\n",
        "                Will save the data specified by ``return_type`` to disk. Default is None.\n",
        "            save_to_name (str, optional): The name of the file to save. The extension '.pt' is automatically\n",
        "                appended. Default is ``'dataset_' + self.name + '.pt'``.\n",
        "            save_as_type (str, optional): The file extension of saved file if `save_to_path` is set. Supports \"pt\" (PyTorch)\n",
        "                and \"txt\" (Text). Saving as \"txt\" requires the ``return_type`` to be ``lists``. If ``return_type`` is\n",
        "                ``tensors`` the only ``save_as_type`` available is \"pt\". Defaults to \"txt\".\n",
        "\n",
        "        Returns:\n",
        "            list or torch.TensorDataset: If ``return_type is None`` return the list of calculated\n",
        "            features. If ``return_type == \"tensors\"`` return the features converted to tensors\n",
        "            and stacked such that features are grouped together into individual tensors. If\n",
        "            ``return_type == \"lists\"``, which is the recommended option then exports each\n",
        "            ``InputFeatures`` object in the exported ``features`` list as a dictionary and appends each\n",
        "            dictionary to a list. Returns that list.\n",
        "        \"\"\"\n",
        "        assert return_type in [\"tensors\", \"lists\"] or return_type is None\n",
        "        assert save_as_type in [\"txt\", \"pt\"] or save_to_path is None\n",
        "        if save_as_type == \"txt\":\n",
        "            assert return_type == \"lists\"\n",
        "        if return_type == \"tensors\":\n",
        "            assert save_as_type == \"pt\" or save_to_path is None\n",
        "        if return_type == \"lists\":\n",
        "            create_attention_mask = False\n",
        "            pad_ids_and_attention = False\n",
        "        else:  # if `return_type` is None  or \"tensors\"\n",
        "            create_attention_mask = True\n",
        "            pad_ids_and_attention = True\n",
        "\n",
        "        if max_length is None:\n",
        "            max_length = tokenizer.max_len\n",
        "\n",
        "        # batch_length = max(len(input_ids) for input_ids in all_input_ids)\n",
        "\n",
        "        if create_sent_rep_token_ids:\n",
        "            if sent_rep_token_id == \"sep\":  # get the sep token id\n",
        "                sent_rep_token_id = tokenizer.sep_token_id\n",
        "            elif sent_rep_token_id == \"cls\":  # get the cls token id\n",
        "                sent_rep_token_id = tokenizer.cls_token_id\n",
        "            elif not sent_rep_token_id:  # if the `sent_rep_token_id` is not set\n",
        "                # if using `bert_compatible_cls` then default to the `cls_token_id`\n",
        "                if bert_compatible_cls:\n",
        "                    sent_rep_token_id = tokenizer.cls_token_id\n",
        "                else:  # otherwise, get the `sep_token_id`\n",
        "                    sent_rep_token_id = tokenizer.sep_token_id\n",
        "\n",
        "        if create_segment_ids:\n",
        "            if segment_token_id == \"period\":  # get the token id for a \".\"\n",
        "                segment_token_id = tokenizer.convert_tokens_to_ids([\".\"])[0]\n",
        "            elif (\n",
        "                not segment_token_id\n",
        "            ):  # default to trying to get the `sep_token_id` if the `segment_token_id` is not set\n",
        "                segment_token_id = tokenizer.sep_token_id\n",
        "\n",
        "        features = []\n",
        "        pool = Pool(n_process)\n",
        "        _get_features_process = partial(\n",
        "            self.get_features_process,\n",
        "            num_examples=len(self.labels),\n",
        "            tokenizer=tokenizer,\n",
        "            bert_compatible_cls=bert_compatible_cls,\n",
        "            sep_token=tokenizer.sep_token,\n",
        "            cls_token=tokenizer.cls_token,\n",
        "            create_sent_rep_token_ids=create_sent_rep_token_ids,\n",
        "            sent_rep_token_id=sent_rep_token_id,\n",
        "            create_sent_lengths=create_sent_lengths,\n",
        "            create_segment_ids=create_segment_ids,\n",
        "            segment_token_id=segment_token_id,\n",
        "            create_source=create_source,\n",
        "            max_length=max_length,\n",
        "            pad_on_left=pad_on_left,\n",
        "            pad_token=pad_token,\n",
        "            mask_padding_with_zero=mask_padding_with_zero,\n",
        "            create_attention_mask=create_attention_mask,\n",
        "            pad_ids_and_attention=pad_ids_and_attention,\n",
        "        )\n",
        "\n",
        "        for rtn_features in pool.map(\n",
        "            _get_features_process,\n",
        "            zip(range(len(self.labels)), self.examples, self.labels),\n",
        "        ):\n",
        "            features.append(rtn_features)\n",
        "\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "        if not return_type:\n",
        "            return features\n",
        "        elif return_type == \"tensors\":\n",
        "            final_tensors = []\n",
        "\n",
        "            all_input_ids = torch.tensor(\n",
        "                [f.input_ids for f in features], dtype=torch.long\n",
        "            )\n",
        "            final_tensors.append(all_input_ids)\n",
        "            all_attention_masks = torch.tensor(\n",
        "                [f.attention_mask for f in features], dtype=torch.long\n",
        "            )\n",
        "            final_tensors.append(all_attention_masks)\n",
        "            all_labels = torch.tensor(\n",
        "                pad([f.labels for f in features], 0), dtype=torch.long\n",
        "            )\n",
        "            final_tensors.append(all_labels)\n",
        "\n",
        "            if create_segment_ids:\n",
        "                all_token_type_ids = torch.tensor(\n",
        "                    pad([f.token_type_ids for f in features], 0), dtype=torch.long\n",
        "                )\n",
        "                final_tensors.append(all_token_type_ids)\n",
        "            # Pad sentence representation token ids (`sent_rep_token_ids`)\n",
        "            if create_sent_rep_token_ids:\n",
        "                all_sent_rep_token_ids = torch.tensor(\n",
        "                    pad([f.sent_rep_token_ids for f in features], -1), dtype=torch.long\n",
        "                )\n",
        "                all_sent_rep_token_ids_masks = ~(all_sent_rep_token_ids == -1)\n",
        "                all_sent_rep_token_ids[all_sent_rep_token_ids == -1] = 0\n",
        "                final_tensors.append(all_sent_rep_token_ids)\n",
        "                final_tensors.append(all_sent_rep_token_ids_masks)\n",
        "\n",
        "                if create_sent_lengths:\n",
        "                    all_sent_lengths = torch.tensor(\n",
        "                        pad([f.sent_lengths for f in features], 0), dtype=torch.long\n",
        "                    )\n",
        "                    final_tensors.append(all_sent_lengths)\n",
        "\n",
        "            dataset = torch.utils.data.TensorDataset(*final_tensors)\n",
        "\n",
        "        elif return_type == \"lists\":\n",
        "            dataset = [example.to_dict() for example in features]\n",
        "\n",
        "        if save_to_path:\n",
        "            final_save_name = save_to_name if save_to_name else (\"dataset_\" + self.name)\n",
        "            dataset_path = os.path.join(\n",
        "                save_to_path, (final_save_name + \".\" + save_as_type),\n",
        "            )\n",
        "            logger.info(\"Saving dataset into cached file %s\", dataset_path)\n",
        "            if save_as_type == \"txt\":\n",
        "                with open(dataset_path, \"w+\") as file:\n",
        "                    # Need to replace single with double quotes so it can be loaded as JSON\n",
        "                    file.write(\n",
        "                        \"\\n\".join([json.dumps(x) for x in dataset]) + \"\\n\"\n",
        "                    )\n",
        "            elif save_as_type == \"pt\":\n",
        "                torch.save(dataset, dataset_path)\n",
        "            else:\n",
        "                logger.error(\"'%s' is an invalid save type.\", save_as_type)\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def load(self, load_from_path, dataset_name=None):\n",
        "        \"\"\"Attempts to load the dataset from storage. If that fails, will return None.\"\"\"\n",
        "        final_load_name = dataset_name if dataset_name else (\"dataset_\" + self.name)\n",
        "        dataset_path = os.path.join(load_from_path, (final_load_name + \".pt\"),)\n",
        "\n",
        "        if os.path.exists(dataset_path):\n",
        "            logger.info(\"Loading data from file %s\", dataset_path)\n",
        "            dataset = torch.load(dataset_path)\n",
        "            return dataset\n",
        "\n",
        "        return None\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing data.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UIbghXxYukY"
      },
      "source": [
        "!mv data.py transformersum/src/data.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuHv1Lhfx548"
      },
      "source": [
        "# EXPERIMENT WITH MODEL ONLY DEALING WITH MAX 512 TOKENS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lcY49e8Kndd",
        "outputId": "fc48cb9f-e2ac-4d9a-dffd-e687fc7d8bdb"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk \n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "slNp5BIOV-7u",
        "outputId": "185b77a0-316f-4141-be2b-10af37d0ae20"
      },
      "source": [
        "df= pd.read_csv('transformersum/src/889_final.csv')\n",
        "df.head()\n",
        "import nltk\n",
        "\n",
        "df['summary_sent_count']=df['summary'].apply(lambda k : len(nltk.sent_tokenize(k)))\n",
        "df['summary_sent_count'].value_counts()\n",
        "\n",
        "#shorter_df = df[(df['summary_sent_count']==3)]\n",
        "shorter_df = df[(df['summary_sent_count']>=3) & (df['summary_sent_count']<10)]\n",
        "shorter_df.to_csv('shorter_df_1.csv', index= False, header= True)\n",
        "!cp shorter_df_1.csv transformersum/src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ace1f0cf68da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformersum/src/889_final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_sent_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'transformersum/src/889_final.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZp3ksdAa7u4",
        "outputId": "7f5f08c4-df63-4d95-9475-6518e1fe0ab2"
      },
      "source": [
        "shorter_df['summary_sent_count'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    65\n",
              "4    30\n",
              "5    24\n",
              "6    17\n",
              "8    10\n",
              "7     7\n",
              "9     5\n",
              "Name: summary_sent_count, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXNOCKqYYuqt",
        "outputId": "5539d44e-810f-4302-f415-53de3609c2a7"
      },
      "source": [
        "%%writefile hannah_limit_512.py\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from extractive import ExtractiveSummarizer\n",
        "model = ExtractiveSummarizer.load_from_checkpoint(\"epoch=3.ckpt\")\n",
        "\n",
        "def dice_similarity (list1, list2):\n",
        "  numer= len(list(set(list1).intersection(list2)))\n",
        "  deno=len(list1)+len(list2)\n",
        "  return 2*float(numer)/ deno \n",
        "\n",
        "summaries, dice_results = [], []\n",
        "\n",
        "start = time.time()\n",
        "df = pd.read_csv('shorter_df_1.csv')\n",
        "for i, _ in enumerate(range(df.shape[0])):\n",
        "  ground_truth = df.iloc[i]['summary']\n",
        "  text_to_summarize =df.iloc[i]['abstract'] +' '+ df.iloc[i]['main_text']\n",
        "  summary = model.predict(text_to_summarize, num_summary_sentences=4)\n",
        "  # df.iloc[i]['generated_summary'] = summary\n",
        "  \n",
        "  summaries.append(summary)\n",
        "  dice_results.append(dice_similarity(nltk.word_tokenize(ground_truth.lower()), nltk.word_tokenize(summary.lower())))\n",
        "  # df.iloc[i]['dice_similarity'] = dice_similarity(nltk.word_tokenize(ground_truth.lower()), nltk.word_tokenize(summary.lower()))\n",
        "\n",
        "df['dice_sim_512'] = dice_results\n",
        "df['generated_summary_512'] = summaries\n",
        "df.to_csv('889_data_dice_512.csv', header=True, index=False)\n",
        "end = time.time()\n",
        "print (end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting hannah_limit_512.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OR-nUmyNYuu2"
      },
      "source": [
        "!mv hannah_limit_512.py transformersum/src\n",
        "#!ls transformersum/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvGXym8wizvP",
        "outputId": "76eeb73e-498f-49d5-cf9e-ca2642662d8c"
      },
      "source": [
        "%%shell \n",
        "\n",
        "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
        "conda activate transformersum\n",
        "\n",
        "cd transformersum/src\n",
        "python hannah_limit_512.py\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Warning: please export TSAN_OPTIONS='ignore_noninstrumented_modules=1' to avoid false positive reports from the OpenMP runtime!\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1226 > 512). Running this sequence through the model will result in indexing errors\n",
            "162.88649535179138\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFOIxjAxuHbD"
      },
      "source": [
        "!cp transformersum/src/889_data_dice_512.csv ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "6WqvMKtwyESH",
        "outputId": "b2758028-fc45-405e-acfb-806e9c7f913f"
      },
      "source": [
        "import pandas as pd\n",
        "df_out = pd.read_csv('889_data_dice_512.csv')\n",
        "df_out.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "      <th>sent_count</th>\n",
              "      <th>summary_sent_count</th>\n",
              "      <th>dice_sim_512</th>\n",
              "      <th>generated_summary_512</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COVID-19 Prevention and Control Strategies for...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\n\\n# Main text\\n\\nSince December 2019, the ou...</td>\n",
              "      <td>-1</td>\n",
              "      <td>38</td>\n",
              "      <td>3</td>\n",
              "      <td>0.177778</td>\n",
              "      <td>-1 \\n\\n # Main text \\n\\n Since December 2019 ,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clinical evidence based review and recommendat...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>3</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nIn the era of globalization, in...</td>\n",
              "      <td>BACKGROUND: Aerosol generating medical procedu...</td>\n",
              "      <td>173</td>\n",
              "      <td>3</td>\n",
              "      <td>0.309179</td>\n",
              "      <td>The aim of this literature review was to ident...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Emergence of COVID-19 Infection: What Is Known...</td>\n",
              "      <td>The purpose of this review is to focus on the ...</td>\n",
              "      <td>2</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nCoronaviruses are a large famil...</td>\n",
              "      <td>BACKGROUND: The discovery of the coronavirus d...</td>\n",
              "      <td>88</td>\n",
              "      <td>3</td>\n",
              "      <td>0.350649</td>\n",
              "      <td>METHODS : For this narrative review , more tha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>COVID-19 coronavirus: recommended personal pro...</td>\n",
              "      <td>The primary purpose of this study is to review...</td>\n",
              "      <td>6</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>9</td>\n",
              "      <td># Main text\\n\\nCOVID-19 coronavirus has spread...</td>\n",
              "      <td>PURPOSE: With the COVID-19 crisis, recommendat...</td>\n",
              "      <td>164</td>\n",
              "      <td>9</td>\n",
              "      <td>0.203822</td>\n",
              "      <td>METHODS : A systematic review of the available...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aerosol-generating otolaryngology procedures a...</td>\n",
              "      <td>The objective of the review was to provide evi...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nDuring the coronavirus disease ...</td>\n",
              "      <td>BACKGROUND: Adequate personal protective equip...</td>\n",
              "      <td>172</td>\n",
              "      <td>3</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>MAIN BODY : Health care workers in China who p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                              generated_summary_512\n",
              "0  COVID-19 Prevention and Control Strategies for...  ...  -1 \\n\\n # Main text \\n\\n Since December 2019 ,...\n",
              "1  Clinical evidence based review and recommendat...  ...  The aim of this literature review was to ident...\n",
              "2  Emergence of COVID-19 Infection: What Is Known...  ...  METHODS : For this narrative review , more tha...\n",
              "3  COVID-19 coronavirus: recommended personal pro...  ...  METHODS : A systematic review of the available...\n",
              "4  Aerosol-generating otolaryngology procedures a...  ...  MAIN BODY : Health care workers in China who p...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7bj04mUyJXr",
        "outputId": "0720123f-a8bf-4c03-fc8c-90b4e2768651"
      },
      "source": [
        "df_out['dice_sim_512'].mean()*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.142716082886796"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMiKwUS9CuDR"
      },
      "source": [
        "# EXPERIMENT WITH MODEL ONLY DEALING WITH 512 TOKENS AND POSTPROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL66S1SjBWm2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX67DH-sy8A9"
      },
      "source": [
        "def post_process(data):\n",
        "\n",
        "  data_tmp=[]\n",
        "\n",
        "  data = data.replace('# Main text', '')\n",
        "  data = re.sub(r'\\(\\s*[F|f]igure\\s*\\d+\\w?\\s*\\)','', data)\n",
        "\n",
        "  data = data.replace('\\n', ' ') #remove new lines\n",
        "  # return data\n",
        "  data = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)','', data) #remove urls\n",
        "\n",
        "  data = re.sub(r'\\(\\w+\\s*(et\\sal\\.\\s)?\\d+\\)', '', data)    #remove reference items (Migliaccio et al. 2013 )\n",
        "\n",
        "  data = re.sub(r'as shown in fig\\.?\\d+', '', data)  #remove figure mentions\n",
        "\n",
        "  data = data.replace('url to online version','').replace('#','') #remove custom markers\n",
        "\n",
        "  # data = re.sub(r'[^\\w\\s]','',data)\n",
        "\n",
        "  data_tmp = re.sub(r'\\s{2,}', ' ', data)  #replace consecutive >=2 white spaces to just 1\n",
        "  return data_tmp.strip()   # join the list of words to make one big string\n",
        "\n",
        "def dice_similarity (list1, list2):\n",
        "  numer= len(list(set(list1).intersection(list2)))\n",
        "  deno=len(list1)+len(list2)\n",
        "  return 2*float(numer)/ deno \n",
        "\n",
        "def get_dice(g, s):\n",
        "  return dice_similarity(nltk.word_tokenize(g.lower()), nltk.word_tokenize(s.lower()))\n",
        "\n",
        "df_out['generated_summary_512_postprocess'] = df_out['generated_summary_512'].apply(lambda k: post_process(k))\n",
        "df_out['summary_postprocess'] = df_out['summary'].apply(lambda k: post_process(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "Mno1JV1RQJfS",
        "outputId": "213a206f-b4f1-48b9-c6b5-fb642de3a31e"
      },
      "source": [
        "dice_scores=[]\n",
        "for i in range(df_out.shape[0]):\n",
        "  gen=df_out.iloc[i]['generated_summary_512_postprocess']\n",
        "  summ=df_out.iloc[i]['summary_postprocess']\n",
        "  dice_scores.append(get_dice(gen, summ))\n",
        "\n",
        "df_out['899_dice_sim_512_postprocess.csv']=dice_scores\n",
        "df_out.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "      <th>sent_count</th>\n",
              "      <th>summary_sent_count</th>\n",
              "      <th>dice_sim_512</th>\n",
              "      <th>generated_summary_512</th>\n",
              "      <th>generated_summary_512_postprocess</th>\n",
              "      <th>summary_postprocess</th>\n",
              "      <th>899_dice_sim_512_postprocess.csv</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COVID-19 Prevention and Control Strategies for...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\n\\n# Main text\\n\\nSince December 2019, the ou...</td>\n",
              "      <td>-1</td>\n",
              "      <td>38</td>\n",
              "      <td>3</td>\n",
              "      <td>0.177778</td>\n",
              "      <td>-1 \\n\\n # Main text \\n\\n Since December 2019 ,...</td>\n",
              "      <td>-1 Since December 2019 , the outbreak of a res...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>0.180791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clinical evidence based review and recommendat...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>3</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nIn the era of globalization, in...</td>\n",
              "      <td>BACKGROUND: Aerosol generating medical procedu...</td>\n",
              "      <td>173</td>\n",
              "      <td>3</td>\n",
              "      <td>0.309179</td>\n",
              "      <td>The aim of this literature review was to ident...</td>\n",
              "      <td>The aim of this literature review was to ident...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>0.309179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Emergence of COVID-19 Infection: What Is Known...</td>\n",
              "      <td>The purpose of this review is to focus on the ...</td>\n",
              "      <td>2</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nCoronaviruses are a large famil...</td>\n",
              "      <td>BACKGROUND: The discovery of the coronavirus d...</td>\n",
              "      <td>88</td>\n",
              "      <td>3</td>\n",
              "      <td>0.350649</td>\n",
              "      <td>METHODS : For this narrative review , more tha...</td>\n",
              "      <td>METHODS : For this narrative review , more tha...</td>\n",
              "      <td>The purpose of this review is to focus on the ...</td>\n",
              "      <td>0.350649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Aerosol-generating otolaryngology procedures a...</td>\n",
              "      <td>The objective of the review was to provide evi...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nDuring the coronavirus disease ...</td>\n",
              "      <td>BACKGROUND: Adequate personal protective equip...</td>\n",
              "      <td>172</td>\n",
              "      <td>3</td>\n",
              "      <td>0.370370</td>\n",
              "      <td>MAIN BODY : Health care workers in China who p...</td>\n",
              "      <td>MAIN BODY : Health care workers in China who p...</td>\n",
              "      <td>The objective of the review was to provide evi...</td>\n",
              "      <td>0.370370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Drivers of airborne human-to-human pathogen tr...</td>\n",
              "      <td>Here, we synthesize insights from microbiologi...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://api.elsevie...</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nThe horror of airborne infectio...</td>\n",
              "      <td>Airborne pathogens — either transmitted via ae...</td>\n",
              "      <td>101</td>\n",
              "      <td>3</td>\n",
              "      <td>0.507042</td>\n",
              "      <td>Here , we synthesize insights from microbiolog...</td>\n",
              "      <td>Here , we synthesize insights from microbiolog...</td>\n",
              "      <td>Here, we synthesize insights from microbiologi...</td>\n",
              "      <td>0.507042</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... 899_dice_sim_512_postprocess.csv\n",
              "0  COVID-19 Prevention and Control Strategies for...  ...                         0.180791\n",
              "1  Clinical evidence based review and recommendat...  ...                         0.309179\n",
              "2  Emergence of COVID-19 Infection: What Is Known...  ...                         0.350649\n",
              "3  Aerosol-generating otolaryngology procedures a...  ...                         0.370370\n",
              "4  Drivers of airborne human-to-human pathogen tr...  ...                         0.507042\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzrXEQuNReo6",
        "outputId": "23f5866c-9a1b-40be-c8a3-2c11a88c3c55"
      },
      "source": [
        "df_out['899_dice_sim_512_postprocess.csv'].mean()*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26.687538407561718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4Y_2lONbHwJ",
        "outputId": "1a586849-3ea5-4d59-9e77-e4faf1f84ece"
      },
      "source": [
        "!pip install rouge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.6/site-packages (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from rouge) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSzMlHCCer0h"
      },
      "source": [
        "from rouge import Rouge \n",
        "rouge = Rouge()\n",
        "\n",
        "def get_rouge(h, r):\n",
        "  return rouge.get_scores(h, r)\n",
        "  \n",
        "\n",
        "rouge_scores=[]\n",
        "for i in range(df_out.shape[0]):\n",
        "  gen=df_out.iloc[i]['generated_summary_512_postprocess']\n",
        "  summ=df_out.iloc[i]['summary_postprocess']\n",
        "  rouge_scores.append(get_rouge(gen, summ))\n",
        "  \n",
        "# df_out['rouge_sim_gt_512_postprocess']=dice_scores\n",
        "# df_out.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRVJh4zwe84j"
      },
      "source": [
        "r1, r2, rL= [], [], []\n",
        "for i in rouge_scores:\n",
        "  r1.append(i[0]['rouge-1']['f'])\n",
        "  r2.append(i[0]['rouge-2']['f'])\n",
        "  rL.append(i[0]['rouge-l']['f'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4TvLojLe95v",
        "outputId": "e1a35d58-c97a-4491-ac5c-e10aa3515ad3"
      },
      "source": [
        "import numpy as np\n",
        "#np.std(r1)\n",
        "\n",
        "np.mean(r1), np.mean(r2), np.mean(rL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.34410564001043525, 0.1823867022855405, 0.337721246660522)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgwygqeuRwkB"
      },
      "source": [
        "df_out.to_csv('889_data_dice_512_processed.csv', header=True, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkaMMN7CUTyE"
      },
      "source": [
        "!cp 889_data_dice_512_processed.csv transformersum/src\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGTxMW5Sy8NU"
      },
      "source": [
        "# EXPERIMENT WITH MODEL DEALING WITH > 512 TOKENS - Global Max Pooling\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWJkYQ0DBmKz",
        "outputId": "3ed7a905-5aa4-4511-d7db-eb207b43102c"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdROsIv4vtK9",
        "outputId": "73befa23-a0b7-4194-e472-13120ee38741"
      },
      "source": [
        "df= pd.read_csv('transformersum/src/889_final.csv')\n",
        "df.head()\n",
        "import nltk\n",
        "\n",
        "df['summary_sent_count']=df['summary'].apply(lambda k : len(nltk.sent_tokenize(k)))\n",
        "df['summary_sent_count'].value_counts()\n",
        "shorter_df = df[(df['summary_sent_count']>=3) & (df['summary_sent_count']<10)]\n",
        "#shorter_df = df[df['summary_sent_count']==4]\n",
        "shorter_df.to_csv('shorter_gt_df_1.csv', index= False, header= True)\n",
        "!cp shorter_df_1.csv transformersum/src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'shorter_df_1.csv': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BCKOX3RCAnS",
        "outputId": "e842bff3-4f25-447d-e559-150c9056c778"
      },
      "source": [
        "shorter_df['summary_sent_count'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    65\n",
              "4    30\n",
              "5    24\n",
              "6    17\n",
              "8    10\n",
              "7     7\n",
              "9     5\n",
              "Name: summary_sent_count, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek39d_iAzCi4",
        "outputId": "b3115d6e-f791-4b49-a8a2-8fc8615ee143"
      },
      "source": [
        "%%writefile hannah_limit_gt_512.py\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from extractive import ExtractiveSummarizer\n",
        "model = ExtractiveSummarizer.load_from_checkpoint(\"epoch=3.ckpt\")\n",
        "\n",
        "\n",
        "\n",
        "def dice_similarity (list1, list2):\n",
        "  numer= len(list(set(list1).intersection(list2)))\n",
        "  deno=len(list1)+len(list2)\n",
        "  return 2*float(numer)/ deno \n",
        "\n",
        "df = pd.read_csv('shorter_gt_df_1.csv')\n",
        "\n",
        "summaries, dice_results = [-1]*df.shape[0], [-1]*df.shape[0]\n",
        "\n",
        "break_point=16\n",
        "start = time.time()\n",
        "\n",
        "for i, _ in enumerate(range(df.shape[0])):\n",
        "  SUMMARY = {}\n",
        "  ground_truth = df.iloc[i]['summary']\n",
        "  sents_tokenized = df.iloc[i]['abstract'] +' '+ df.iloc[i]['main_text']\n",
        "\n",
        "  start_idx=0\n",
        "  for _ in range(6):\n",
        "\n",
        "    text_to_summarize = ' '.join(sents_tokenized[start_idx:start_idx+break_point])\n",
        "    \n",
        "    if len(text_to_summarize)==0:\n",
        "      break\n",
        "      \n",
        "    model_summary = model.predict(text_to_summarize, raw_scores=True, num_summary_sentences=4)\n",
        "\n",
        "    for s in model_summary:\n",
        "      SUMMARY[s[0]] = s[1]\n",
        "    start_idx += break_point\n",
        "  \n",
        "  sorted_sents = sorted(SUMMARY.items(), key=lambda item: item[1], reverse=True)\n",
        "  summary = ' '.join([sentences[0] for sentences in sorted_sents[:4]])\n",
        "  \n",
        "  summaries[i] = summary\n",
        "  dice_results[i] = dice_similarity(nltk.word_tokenize(ground_truth.lower()), nltk.word_tokenize(summary.lower()))\n",
        "\n",
        "\n",
        "df['dice_sim_gt_512'] = dice_results\n",
        "df['generated_summary_gt_512'] = summaries\n",
        "df.to_csv('889_data_dice_gt_512.csv', header=True, index=False)\n",
        "end = time.time()\n",
        "print (end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing hannah_limit_gt_512.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkwQI8x45Wuz"
      },
      "source": [
        "!mv hannah_limit_gt_512.py transformersum/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn1Zijtw4eiA",
        "outputId": "6f4eb196-9bb1-4bf7-8335-e30dcc2bec2a"
      },
      "source": [
        "%%shell \n",
        "\n",
        "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
        "conda activate transformersum\n",
        "\n",
        "cd transformersum/src\n",
        "python hannah_limit_gt_512.py\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Warning: please export TSAN_OPTIONS='ignore_noninstrumented_modules=1' to avoid false positive reports from the OpenMP runtime!\n",
            "165.5842125415802\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "DekLv2N_z7BR",
        "outputId": "1da86e31-cf92-4ccb-d5cc-34906ccc965a"
      },
      "source": [
        "!cp transformersum/src/889_data_dice_gt_512.csv .\n",
        "\n",
        "import pandas as pd\n",
        "df_out = pd.read_csv('889_data_dice_gt_512.csv')\n",
        "df_out.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "      <th>sent_count</th>\n",
              "      <th>summary_sent_count</th>\n",
              "      <th>dice_sim_gt_512</th>\n",
              "      <th>generated_summary_gt_512</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COVID-19 Prevention and Control Strategies for...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\n\\n# Main text\\n\\nSince December 2019, the ou...</td>\n",
              "      <td>-1</td>\n",
              "      <td>38</td>\n",
              "      <td>3</td>\n",
              "      <td>0.045802</td>\n",
              "      <td>- 1   \\n \\n  #    M a i n    t e x t.    2 0 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clinical evidence based review and recommendat...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>3</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nIn the era of globalization, in...</td>\n",
              "      <td>BACKGROUND: Aerosol generating medical procedu...</td>\n",
              "      <td>173</td>\n",
              "      <td>3</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>s    ( A G M P s )    p r e s e n. B A C K G R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Emergence of COVID-19 Infection: What Is Known...</td>\n",
              "      <td>The purpose of this review is to focus on the ...</td>\n",
              "      <td>2</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nCoronaviruses are a large famil...</td>\n",
              "      <td>BACKGROUND: The discovery of the coronavirus d...</td>\n",
              "      <td>88</td>\n",
              "      <td>3</td>\n",
              "      <td>0.022099</td>\n",
              "      <td>- 1 9 )    d u r i n g    a    p n. e a s e   ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>COVID-19 coronavirus: recommended personal pro...</td>\n",
              "      <td>The primary purpose of this study is to review...</td>\n",
              "      <td>6</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>9</td>\n",
              "      <td># Main text\\n\\nCOVID-19 coronavirus has spread...</td>\n",
              "      <td>PURPOSE: With the COVID-19 crisis, recommendat...</td>\n",
              "      <td>164</td>\n",
              "      <td>9</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>e    C O V I D - 1 9    c r i s i. s ,    r e ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aerosol-generating otolaryngology procedures a...</td>\n",
              "      <td>The objective of the review was to provide evi...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nDuring the coronavirus disease ...</td>\n",
              "      <td>BACKGROUND: Adequate personal protective equip...</td>\n",
              "      <td>172</td>\n",
              "      <td>3</td>\n",
              "      <td>0.061856</td>\n",
              "      <td>o t e c t i v e    e q u i p m e. B A C K G R ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                           generated_summary_gt_512\n",
              "0  COVID-19 Prevention and Control Strategies for...  ...  - 1   \\n \\n  #    M a i n    t e x t.    2 0 1...\n",
              "1  Clinical evidence based review and recommendat...  ...  s    ( A G M P s )    p r e s e n. B A C K G R...\n",
              "2  Emergence of COVID-19 Infection: What Is Known...  ...  - 1 9 )    d u r i n g    a    p n. e a s e   ...\n",
              "3  COVID-19 coronavirus: recommended personal pro...  ...  e    C O V I D - 1 9    c r i s i. s ,    r e ...\n",
              "4  Aerosol-generating otolaryngology procedures a...  ...  o t e c t i v e    e q u i p m e. B A C K G R ...\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydAxvEb6YUN9",
        "outputId": "3286398c-5a12-4642-f2fc-a3b820d2443a"
      },
      "source": [
        "\n",
        "df_out['dice_sim_gt_512'].mean()*100, df_out['dice_sim_gt_512'].std()*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.035791985652321, 1.8150785062801877)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNpLZuJgZXg1"
      },
      "source": [
        "# EXPERIMENT WITH MODEL DEALING WITH > 512 TOKENS AND POSTPROCESSING\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7qCwJh2ZhM4"
      },
      "source": [
        "def post_process(data):\n",
        "\n",
        "  data_tmp=[]\n",
        "\n",
        "  data = data.replace('# Main text', '')\n",
        "  data = re.sub(r'\\(\\s*[F|f]igure\\s*\\d+\\w?\\s*\\)','', data)\n",
        "\n",
        "  data = data.replace('\\n', ' ') #remove new lines\n",
        "  # return data\n",
        "  data = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)','', data) #remove urls\n",
        "\n",
        "  data = re.sub(r'\\(\\w+\\s*(et\\sal\\.\\s)?\\d+\\)', '', data)    #remove reference items (Migliaccio et al. 2013 )\n",
        "\n",
        "  data = re.sub(r'as shown in fig\\.?\\d+', '', data)  #remove figure mentions\n",
        "\n",
        "  data = data.replace('url to online version','').replace('#','') #remove custom markers\n",
        "\n",
        "  # data = re.sub(r'[^\\w\\s]','',data)\n",
        "\n",
        "  data_tmp = re.sub(r'\\s{2,}', ' ', data)  #replace consecutive >=2 white spaces to just 1\n",
        "  return data_tmp.strip()   # join the list of words to make one big string\n",
        "\n",
        "def dice_similarity (list1, list2):\n",
        "  numer= len(list(set(list1).intersection(list2)))\n",
        "  deno=len(list1)+len(list2)\n",
        "  return 2*float(numer)/ deno \n",
        "\n",
        "def get_dice(g, s):\n",
        "  return dice_similarity(nltk.word_tokenize(g.lower()), nltk.word_tokenize(s.lower()))\n",
        "\n",
        "df_out['generated_summary_gt_512_postprocess'] = df_out['generated_summary_gt_512'].apply(lambda k: post_process(k))\n",
        "df_out['summary_postprocess'] = df_out['summary'].apply(lambda k: post_process(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "eli8PeJ3Zl0v",
        "outputId": "772e519e-7e75-4ee2-ee05-3938972f8058"
      },
      "source": [
        "dice_scores=[]\n",
        "for i in range(df_out.shape[0]):\n",
        "  gen=df_out.iloc[i]['generated_summary_gt_512_postprocess']\n",
        "  summ=df_out.iloc[i]['summary_postprocess']\n",
        "  dice_scores.append(get_dice(gen, summ))\n",
        "df_out['dice_sim_gt_512_postprocess']=dice_scores\n",
        "df_out.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "      <th>sent_count</th>\n",
              "      <th>summary_sent_count</th>\n",
              "      <th>dice_sim_gt_512</th>\n",
              "      <th>generated_summary_gt_512</th>\n",
              "      <th>generated_summary_gt_512_postprocess</th>\n",
              "      <th>summary_postprocess</th>\n",
              "      <th>dice_sim_gt_512_postprocess</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COVID-19 Prevention and Control Strategies for...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\n\\n# Main text\\n\\nSince December 2019, the ou...</td>\n",
              "      <td>-1</td>\n",
              "      <td>38</td>\n",
              "      <td>3</td>\n",
              "      <td>0.045802</td>\n",
              "      <td>- 1   \\n \\n  #    M a i n    t e x t.    2 0 1...</td>\n",
              "      <td>- 1 M a i n t e x t. 2 0 1 9 , t h e o u t b r...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>0.046154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clinical evidence based review and recommendat...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>3</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nIn the era of globalization, in...</td>\n",
              "      <td>BACKGROUND: Aerosol generating medical procedu...</td>\n",
              "      <td>173</td>\n",
              "      <td>3</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>s    ( A G M P s )    p r e s e n. B A C K G R...</td>\n",
              "      <td>s ( A G M P s ) p r e s e n. B A C K G R O U N...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>0.012500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Emergence of COVID-19 Infection: What Is Known...</td>\n",
              "      <td>The purpose of this review is to focus on the ...</td>\n",
              "      <td>2</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nCoronaviruses are a large famil...</td>\n",
              "      <td>BACKGROUND: The discovery of the coronavirus d...</td>\n",
              "      <td>88</td>\n",
              "      <td>3</td>\n",
              "      <td>0.022099</td>\n",
              "      <td>- 1 9 )    d u r i n g    a    p n. e a s e   ...</td>\n",
              "      <td>- 1 9 ) d u r i n g a p n. e a s e 2 0 1 9 ( C...</td>\n",
              "      <td>The purpose of this review is to focus on the ...</td>\n",
              "      <td>0.022099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>COVID-19 coronavirus: recommended personal pro...</td>\n",
              "      <td>The primary purpose of this study is to review...</td>\n",
              "      <td>6</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>9</td>\n",
              "      <td># Main text\\n\\nCOVID-19 coronavirus has spread...</td>\n",
              "      <td>PURPOSE: With the COVID-19 crisis, recommendat...</td>\n",
              "      <td>164</td>\n",
              "      <td>9</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>e    C O V I D - 1 9    c r i s i. s ,    r e ...</td>\n",
              "      <td>e C O V I D - 1 9 c r i s i. s , r e c o m m e...</td>\n",
              "      <td>The primary purpose of this study is to review...</td>\n",
              "      <td>0.016304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Aerosol-generating otolaryngology procedures a...</td>\n",
              "      <td>The objective of the review was to provide evi...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nDuring the coronavirus disease ...</td>\n",
              "      <td>BACKGROUND: Adequate personal protective equip...</td>\n",
              "      <td>172</td>\n",
              "      <td>3</td>\n",
              "      <td>0.061856</td>\n",
              "      <td>o t e c t i v e    e q u i p m e. B A C K G R ...</td>\n",
              "      <td>o t e c t i v e e q u i p m e. B A C K G R O U...</td>\n",
              "      <td>The objective of the review was to provide evi...</td>\n",
              "      <td>0.061856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... dice_sim_gt_512_postprocess\n",
              "0  COVID-19 Prevention and Control Strategies for...  ...                    0.046154\n",
              "1  Clinical evidence based review and recommendat...  ...                    0.012500\n",
              "2  Emergence of COVID-19 Infection: What Is Known...  ...                    0.022099\n",
              "3  COVID-19 coronavirus: recommended personal pro...  ...                    0.016304\n",
              "4  Aerosol-generating otolaryngology procedures a...  ...                    0.061856\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e21I96TKZl8D",
        "outputId": "98dabb96-2127-429d-82be-b7991a3fa07e"
      },
      "source": [
        "df_out['dice_sim_gt_512_postprocess'].mean()*100, df_out['dice_sim_gt_512_postprocess'].std()*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3.001393687979078, 1.8034080350082218)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv4A5uQ4ZsA3"
      },
      "source": [
        "df_out.to_csv('889_data_dice_gt_512_processed.csv', header=True, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wbj8FprZsDB"
      },
      "source": [
        "from rouge import Rouge \n",
        "rouge = Rouge()\n",
        "\n",
        "def get_rouge(h, r):\n",
        "  return rouge.get_scores(h, r)\n",
        "\n",
        "rouge_scores=[]\n",
        "for i in range(df_out.shape[0]):\n",
        "  gen=df_out.iloc[i]['generated_summary_gt_512_postprocess']\n",
        "  summ=df_out.iloc[i]['summary_postprocess']\n",
        "  rouge_scores.append(get_rouge(gen, summ))\n",
        "  \n",
        "# df_out['rouge_sim_gt_512_postprocess']=dice_scores\n",
        "# df_out.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEqyE1Qedq3D"
      },
      "source": [
        "r1, r2, rL= [], [], []\n",
        "for i in rouge_scores:\n",
        "  r1.append(i[0]['rouge-1']['f'])\n",
        "  r2.append(i[0]['rouge-2']['f'])\n",
        "  rL.append(i[0]['rouge-l']['f'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJBOcmO_eI1n",
        "outputId": "bd8eacfc-d7a1-413a-c53c-42778d313122"
      },
      "source": [
        "np.mean(r1), np.mean(r2) ,np.mean(rL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.23809660801525553, 0.09620180855459474, 0.2233451033280843)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtPcYq5xgc3R"
      },
      "source": [
        "# EXPERIMENT WITH MODEL DEALING WITH > 512 TOKENS - Weighted by TextRank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWm0e_agkY-H",
        "outputId": "116544d8-3155-4f83-b88f-60d127b9a330"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk \n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "0odHDdGdky6W",
        "outputId": "09a959f8-6687-4647-ac20-d82f424b33c9"
      },
      "source": [
        "df= pd.read_csv('transformersum/src/889_final.csv')\n",
        "df.head()\n",
        "import nltk\n",
        "\n",
        "df['summary_sent_count']=df['summary'].apply(lambda k : len(nltk.sent_tokenize(k)))\n",
        "df['summary_sent_count'].value_counts()\n",
        "\n",
        "shorter_df = df[df['summary_sent_count']==4]\n",
        "shorter_df.to_csv('shorter_gt_df_1.csv', index= False, header= True)\n",
        "!cp shorter_df_1.csv transformersum/src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b735ff9a0b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformersum/src/889_final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_sent_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "aJC2qfVclYfl",
        "outputId": "f6f03479-e37b-4252-f0ae-d1a6a42b4111"
      },
      "source": [
        "shorter_df['summary_sent_count'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-20455d368b81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshorter_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_sent_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'shorter_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCD7tOz8gmG1",
        "outputId": "7f846afa-55f6-46bd-d6d1-a00812b86c71"
      },
      "source": [
        "%%writefile hannah_limit_gt_512_tr.py\n",
        "\n",
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "import time\n",
        "import nltk\n",
        "from summa.summarizer import summarize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from extractive import ExtractiveSummarizer\n",
        "model = ExtractiveSummarizer.load_from_checkpoint(\"epoch=3.ckpt\")\n",
        "\n",
        "def dice_similarity (list1, list2):\n",
        "  numer= len(list(set(list1).intersection(list2)))\n",
        "  deno=len(list1)+len(list2)\n",
        "  return 2*float(numer)/ deno \n",
        "\n",
        "df = pd.read_csv('shorter_gt_df_1.csv')\n",
        "\n",
        "\n",
        "summaries, dice_results = [-1]*df.shape[0], [-1]*df.shape[0]\n",
        "break_point=16\n",
        "start = time.time()\n",
        "\n",
        "for i, _ in enumerate(range(df.shape[0])):\n",
        "  print (i)\n",
        "  SUMMARY, NEW_SUMMARY = {}, {}\n",
        "  ground_truth = df.iloc[i]['summary']\n",
        "  sents_tokenized = nltk.sent_tokenize(df.iloc[i]['abstract']+' '+ df.iloc[i]['main_text'])\n",
        "\n",
        "\n",
        "  text=df.iloc[i]['abstract']+' '+ df.iloc[i]['main_text']\n",
        "  sent_scores = dict(sorted(dict(summarize(text, ratio=1, scores=True)).items(), key=lambda k: k[1], reverse=True))\n",
        "\n",
        "\n",
        "  start_idx=0\n",
        "  for _ in range(6):\n",
        "\n",
        "    text_to_summarize = ' '.join(sents_tokenized[start_idx:start_idx+break_point])\n",
        "    \n",
        "    if len(text_to_summarize)==0:\n",
        "      break\n",
        "      \n",
        "    model_summary = model.predict(text_to_summarize, raw_scores=True, num_summary_sentences=4)\n",
        "\n",
        "    for s in model_summary:\n",
        "      SUMMARY[s[0]] = s[1]\n",
        "    start_idx += break_point\n",
        "  \n",
        "  sorted_sents = sorted(SUMMARY.items(), key=lambda item: item[1], reverse=True)\n",
        "  \n",
        "  for sentence in sorted_sents:\n",
        "    maxx, sel_sent=-1, None\n",
        "    for k, _ in sent_scores.items():\n",
        "      token_sort_ratio = fuzz.token_sort_ratio(sentence, k)\n",
        "      if token_sort_ratio>=maxx:\n",
        "        maxx=token_sort_ratio\n",
        "        sel_sent=k\n",
        "    try:\n",
        "      NEW_SUMMARY[sentence[0]] = sent_scores[sel_sent]*sentence[1] # adding the scores here gave a better value than x them (we need a way of combinding the scores)\n",
        "    except:\n",
        "      NEW_SUMMARY[sentence[0]] = sentence[1]\n",
        "\n",
        "  # print (NEW_SUMMARY)\n",
        "  sorted_sents_new = sorted(NEW_SUMMARY.items(), key=lambda item: item[1], reverse=True)\n",
        "  # print (sorted_sents_new)\n",
        "  summary = ' '.join([sentences[0] for sentences in sorted_sents_new[:4]])\n",
        "  \n",
        "  summaries[i] = summary\n",
        "  dice_results[i] = dice_similarity(nltk.word_tokenize(ground_truth.lower()), nltk.word_tokenize(summary.lower()))\n",
        "  \n",
        "\n",
        "df['dice_sim_gt_512_tr'] = dice_results\n",
        "df['generated_summary_gt_512_tr'] = summaries\n",
        "df.to_csv('889_data_dice_gt_512_tr.csv', header=True, index=False)\n",
        "end = time.time()\n",
        "print (end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing hannah_limit_gt_512_tr.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6ekUlB1loS5"
      },
      "source": [
        "!mv hannah_limit_gt_512_tr.py transformersum/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "qzcUtED7n8v2",
        "outputId": "9f51d062-3ec1-4c3a-fd7e-b228b262b833"
      },
      "source": [
        "%%shell \n",
        "\n",
        "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
        "conda activate transformersum\n",
        "pip install summa\n",
        "pip install fuzzywuzzy\n",
        "\n",
        "cd transformersum/src\n",
        "python hannah_limit_gt_512_tr.py\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: summa in /usr/local/envs/transformersum/lib/python3.9/site-packages (1.2.0)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from summa) (1.6.0)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from scipy>=0.19->summa) (1.20.1)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/envs/transformersum/lib/python3.9/site-packages (0.18.0)\n",
            "/usr/local/envs/transformersum/lib/python3.9/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
            "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Warning: please export TSAN_OPTIONS='ignore_noninstrumented_modules=1' to avoid false positive reports from the OpenMP runtime!\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformersum/src/hannah_limit_gt_512_tr.py\", line 17, in <module>\n",
            "    df = pd.read_csv('889_data_dice_512_processed.csv')\n",
            "  File \"/usr/local/envs/transformersum/lib/python3.9/site-packages/pandas/io/parsers.py\", line 610, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "  File \"/usr/local/envs/transformersum/lib/python3.9/site-packages/pandas/io/parsers.py\", line 468, in _read\n",
            "    return parser.read(nrows)\n",
            "  File \"/usr/local/envs/transformersum/lib/python3.9/site-packages/pandas/io/parsers.py\", line 1057, in read\n",
            "    index, columns, col_dict = self._engine.read(nrows)\n",
            "  File \"/usr/local/envs/transformersum/lib/python3.9/site-packages/pandas/io/parsers.py\", line 2061, in read\n",
            "    data = self._reader.read(nrows)\n",
            "  File \"pandas/_libs/parsers.pyx\", line 756, in pandas._libs.parsers.TextReader.read\n",
            "  File \"pandas/_libs/parsers.pyx\", line 771, in pandas._libs.parsers.TextReader._read_low_memory\n",
            "  File \"pandas/_libs/parsers.pyx\", line 827, in pandas._libs.parsers.TextReader._read_rows\n",
            "  File \"pandas/_libs/parsers.pyx\", line 814, in pandas._libs.parsers.TextReader._tokenize_rows\n",
            "  File \"pandas/_libs/parsers.pyx\", line 1951, in pandas._libs.parsers.raise_parser_error\n",
            "pandas.errors.ParserError: Error tokenizing data. C error: EOF inside string starting at row 397\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8b30b5e9e669>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\neval \"$(conda shell.bash hook)\" # copy conda command to shell\\nconda activate transformersum\\npip install summa\\npip install fuzzywuzzy\\n\\ncd transformersum/src\\npython hannah_limit_gt_512_tr.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command '\neval \"$(conda shell.bash hook)\" # copy conda command to shell\nconda activate transformersum\npip install summa\npip install fuzzywuzzy\n\ncd transformersum/src\npython hannah_limit_gt_512_tr.py' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "g_iPbLXXNFMV",
        "outputId": "c43b68b5-635e-4d6a-b0b0-167364333d16"
      },
      "source": [
        "!cp transformersum/src/889_data_dice_gt_512_tr.csv .\n",
        "\n",
        "import pandas as pd\n",
        "df_out = pd.read_csv('889_data_dice_gt_512_tr.csv')\n",
        "df_out.head() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'transformersum/src/889_data_dice_gt_512_tr.csv': No such file or directory\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-64e142097f31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'889_data_dice_gt_512_tr.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '889_data_dice_gt_512_tr.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXOtJYVhNHje",
        "outputId": "9f661dde-459e-4338-b2f6-f1c78a175519"
      },
      "source": [
        "df_out['dice_sim_gt_512_tr'].mean()*100\n",
        "# (0.16885502178698258, 0.17283493824202076, 0.21602604870884606)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.5748293716507"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWb39NylkCDB"
      },
      "source": [
        "# EXPERIMENT WITH MODEL DEALING WITH > 512 TOKENS - Weighted by TextRank with POST PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKv8_3XT00-l"
      },
      "source": [
        "def post_process(data):\n",
        "\n",
        "  data_tmp=[]\n",
        "\n",
        "  data = data.replace('# Main text', '')\n",
        "  data = re.sub(r'\\(\\s*[F|f]igure\\s*\\d+\\w?\\s*\\)','', data)\n",
        "\n",
        "  data = data.replace('\\n', ' ') #remove new lines\n",
        "  # return data\n",
        "  data = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)','', data) #remove urls\n",
        "\n",
        "  data = re.sub(r'\\(\\w+\\s*(et\\sal\\.\\s)?\\d+\\)', '', data)    #remove reference items (Migliaccio et al. 2013 )\n",
        "\n",
        "  data = re.sub(r'as shown in fig\\.?\\d+', '', data)  #remove figure mentions\n",
        "\n",
        "  data = data.replace('url to online version','').replace('#','') #remove custom markers\n",
        "\n",
        "  # data = re.sub(r'[^\\w\\s]','',data)\n",
        "\n",
        "  data_tmp = re.sub(r'\\s{2,}', ' ', data)  #replace consecutive >=2 white spaces to just 1\n",
        "  return data_tmp.strip()   # join the list of words to make one big string\n",
        "\n",
        "def dice_similarity (list1, list2):\n",
        "  numer= len(list(set(list1).intersection(list2)))\n",
        "  deno=len(list1)+len(list2)\n",
        "  return 2*float(numer)/ deno \n",
        "\n",
        "def get_dice(g, s):\n",
        "  return dice_similarity(nltk.word_tokenize(g.lower()), nltk.word_tokenize(s.lower()))\n",
        "\n",
        "df_out['generated_summary_gt_512_tr_postprocess'] = df_out['generated_summary_gt_512_tr'].apply(lambda k: post_process(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "Ftea-ROV1T-i",
        "outputId": "fcc83d06-d254-4c46-fb00-614fd866b51d"
      },
      "source": [
        "dice_scores=[]\n",
        "for i in range(df_out.shape[0]):\n",
        "  gen=df_out.iloc[i]['generated_summary_gt_512_tr_postprocess']\n",
        "  summ=df_out.iloc[i]['summary_postprocess']\n",
        "  dice_scores.append(get_dice(gen, summ))\n",
        "df_out['dice_sim_gt_512_tr_postprocess']=dice_scores\n",
        "df_out.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "      <th>summary_sent_count</th>\n",
              "      <th>main_text_len</th>\n",
              "      <th>words_per_sentence</th>\n",
              "      <th>sent_count</th>\n",
              "      <th>dice_sim_512</th>\n",
              "      <th>generated_summary_512</th>\n",
              "      <th>generated_summary_512_postprocess</th>\n",
              "      <th>summary_postprocess</th>\n",
              "      <th>899_dice_sim_512_postprocess.csv</th>\n",
              "      <th>dice_sim_gt_512_tr</th>\n",
              "      <th>generated_summary_gt_512_tr</th>\n",
              "      <th>generated_summary_gt_512_tr_postprocess</th>\n",
              "      <th>dice_sim_gt_512_tr_postprocess</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>An effect assessment of Airborne particulate m...</td>\n",
              "      <td>This study aims to examine the effects of airb...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>1</td>\n",
              "      <td># Main text\\n\\n. It is confirmed that influenz...</td>\n",
              "      <td>Objective: Coronavirus disease 2019 (COVID-19)...</td>\n",
              "      <td>1</td>\n",
              "      <td>2566</td>\n",
              "      <td>18.729927</td>\n",
              "      <td>137</td>\n",
              "      <td>0.100629</td>\n",
              "      <td>It is reported that aerosols from highly virul...</td>\n",
              "      <td>It is reported that aerosols from highly virul...</td>\n",
              "      <td>This study aims to examine the effects of airb...</td>\n",
              "      <td>0.100629</td>\n",
              "      <td>0.135021</td>\n",
              "      <td>Concerning about that , we collected populatio...</td>\n",
              "      <td>Concerning about that , we collected populatio...</td>\n",
              "      <td>0.135021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aerosolized Particle Reduction: A Novel Cadave...</td>\n",
              "      <td>This study aimed to identify escape of small-p...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>2</td>\n",
              "      <td># Main text\\n\\nInstitutional review board appr...</td>\n",
              "      <td>OBJECTIVES: This study aimed to identify escap...</td>\n",
              "      <td>2</td>\n",
              "      <td>1727</td>\n",
              "      <td>28.783333</td>\n",
              "      <td>60</td>\n",
              "      <td>0.089686</td>\n",
              "      <td>This method was used to test several different...</td>\n",
              "      <td>This method was used to test several different...</td>\n",
              "      <td>This study aimed to identify escape of small-p...</td>\n",
              "      <td>0.098522</td>\n",
              "      <td>0.131980</td>\n",
              "      <td>We present a new and effective method for the ...</td>\n",
              "      <td>We present a new and effective method for the ...</td>\n",
              "      <td>0.131980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COVID-19 Prevention and Control Strategies for...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\n\\n# Main text\\n\\nSince December 2019, the ou...</td>\n",
              "      <td>-1</td>\n",
              "      <td>3</td>\n",
              "      <td>1058</td>\n",
              "      <td>27.842105</td>\n",
              "      <td>38</td>\n",
              "      <td>0.166667</td>\n",
              "      <td># Main text \\n\\n Since December 2019 , the out...</td>\n",
              "      <td>Since December 2019 , the outbreak of a respir...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>0.169492</td>\n",
              "      <td>0.178404</td>\n",
              "      <td>Third , psychiatric hospitals should assist th...</td>\n",
              "      <td>Third , psychiatric hospitals should assist th...</td>\n",
              "      <td>0.178404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Will COVID-19 pandemic diminish by summer-mons...</td>\n",
              "      <td>Therefore the study to investigate the meteoro...</td>\n",
              "      <td>11</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>21</td>\n",
              "      <td># Main text\\n\\nr=0.56) between temperature and...</td>\n",
              "      <td>The novel Coronavirus (2019-nCoV) was identifi...</td>\n",
              "      <td>21</td>\n",
              "      <td>6634</td>\n",
              "      <td>25.713178</td>\n",
              "      <td>258</td>\n",
              "      <td>0.123198</td>\n",
              "      <td>For this , we have investigated the associatio...</td>\n",
              "      <td>For this , we have investigated the associatio...</td>\n",
              "      <td>Therefore the study to investigate the meteoro...</td>\n",
              "      <td>0.123198</td>\n",
              "      <td>0.148148</td>\n",
              "      <td>https://doi.org/10.1101 Highlights : \\n\\n  Fi...</td>\n",
              "      <td>Highlights :  First study on the effects of m...</td>\n",
              "      <td>0.148738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Clinical evidence based review and recommendat...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>3</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nIn the era of globalization, in...</td>\n",
              "      <td>BACKGROUND: Aerosol generating medical procedu...</td>\n",
              "      <td>3</td>\n",
              "      <td>4137</td>\n",
              "      <td>23.913295</td>\n",
              "      <td>173</td>\n",
              "      <td>0.112450</td>\n",
              "      <td>The objective of this literature review is to ...</td>\n",
              "      <td>The objective of this literature review is to ...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>0.112450</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>This first review of papers led to the followi...</td>\n",
              "      <td>This first review of papers led to the followi...</td>\n",
              "      <td>0.200000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... dice_sim_gt_512_tr_postprocess\n",
              "0  An effect assessment of Airborne particulate m...  ...                       0.135021\n",
              "1  Aerosolized Particle Reduction: A Novel Cadave...  ...                       0.131980\n",
              "2  COVID-19 Prevention and Control Strategies for...  ...                       0.178404\n",
              "3  Will COVID-19 pandemic diminish by summer-mons...  ...                       0.148738\n",
              "4  Clinical evidence based review and recommendat...  ...                       0.200000\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b84TG7Ui1khR",
        "outputId": "5bbaeef8-e070-44d7-ed26-025689cb4f40"
      },
      "source": [
        "df_out['dice_sim_gt_512_tr_postprocess'].mean()*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.657805287579485"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4TzS4Na1zik"
      },
      "source": [
        "from rouge import Rouge \n",
        "rouge = Rouge()\n",
        "\n",
        "def get_rouge(h, r):\n",
        "  return rouge.get_scores(h, r)\n",
        "\n",
        "rouge_scores=[]\n",
        "for i in range(df_out.shape[0]):\n",
        "  gen=df_out.iloc[i]['generated_summary_gt_512_tr_postprocess']\n",
        "  summ=df_out.iloc[i]['summary_postprocess']\n",
        "  rouge_scores.append(get_rouge(gen, summ))\n",
        "# df_out['rouge_sim_gt_512_postprocess']=dice_scores\n",
        "# df_out.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNjauExW1-Pl"
      },
      "source": [
        "r1, r2, rL= [], [], []\n",
        "for i in rouge_scores:\n",
        "  r1.append(i[0]['rouge-1']['f'])\n",
        "  r2.append(i[0]['rouge-2']['f'])\n",
        "  rL.append(i[0]['rouge-l']['f'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjiENg3j1_3f",
        "outputId": "bfa6d401-dd69-49c6-8e9f-0cc2515785a6"
      },
      "source": [
        "np.mean(r1), np.mean(r2), np.mean(rL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.2696628445709898, 0.1345371751902299, 0.26545419819178484)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_IFO9hvRPWo"
      },
      "source": [
        "# EXPERIMENT WITH MODEL DEALING WITH > 512 TOKENS - Weighted by SemSim using BioBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMLq-0CDRI5-",
        "outputId": "b68a67c8-70b8-4646-fefe-76ea63573714"
      },
      "source": [
        "%%writefile hannah_limit_gt_512_bio.py\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from scipy.spatial import distance\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model_bio = SentenceTransformer('dmis-lab/biobert-v1.1')\n",
        "\n",
        "\n",
        "\n",
        "from extractive import ExtractiveSummarizer\n",
        "model = ExtractiveSummarizer.load_from_checkpoint(\"epoch=3.ckpt\")\n",
        "\n",
        "def dice_similarity (list1, list2):\n",
        "  numer= len(list(set(list1).intersection(list2)))\n",
        "  deno=len(list1)+len(list2)\n",
        "  return 2*float(numer)/ deno \n",
        "\n",
        "df = pd.read_csv('889_A_data_dice_512_processed.csv')\n",
        "\n",
        "\n",
        "summaries, dice_results = [-1]*df.shape[0], [-1]*df.shape[0]\n",
        "break_point=16\n",
        "start = time.time()\n",
        "\n",
        "for i, _ in enumerate(range(df.shape[0])):\n",
        "  print (i)\n",
        "  SUMMARY, NEW_SUMMARY = {}, {}\n",
        "  ground_truth =  df.iloc[i]['summary']\n",
        "  sents_tokenized = nltk.sent_tokenize(df.iloc[i]['abstract'] +' '+ df.iloc[i]['main_text'])\n",
        "\n",
        "\n",
        "  text=df.iloc[i]['abstract'] +' '+df.iloc[i]['main_text']\n",
        "  sentence_vector2 = model_bio.encode(text)\n",
        "\n",
        "  start_idx=0\n",
        "  for _ in range(6):\n",
        "\n",
        "    text_to_summarize = ' '.join(sents_tokenized[start_idx:start_idx+break_point])\n",
        "    \n",
        "    if len(text_to_summarize)==0:\n",
        "      break\n",
        "      \n",
        "    model_summary = model.predict(text_to_summarize, raw_scores=True, num_summary_sentences=4)\n",
        "\n",
        "    for s in model_summary:\n",
        "      SUMMARY[s[0]] = s[1]\n",
        "    start_idx += break_point\n",
        "  \n",
        "  sorted_sents = sorted(SUMMARY.items(), key=lambda item: item[1], reverse=True)\n",
        "  \n",
        "  for sentence in sorted_sents:\n",
        "    sentence_vector1 = model_bio.encode(sentence[0])\n",
        "    sim = 1 - distance.cosine(sentence_vector1, sentence_vector2)\n",
        "\n",
        "    try:\n",
        "      NEW_SUMMARY[sentence[0]] = sim*sentence[1]\n",
        "    except:\n",
        "      NEW_SUMMARY[sentence[0]] = sentence[1]\n",
        "\n",
        "  # print (NEW_SUMMARY)\n",
        "  sorted_sents_new = sorted(NEW_SUMMARY.items(), key=lambda item: item[1], reverse=True)\n",
        "  # print (sorted_sents_new)\n",
        "  summary = ' '.join([sentences[0] for sentences in sorted_sents_new[:4]])\n",
        "  \n",
        "  summaries[i] = summary\n",
        "  dice_results[i] = dice_similarity(nltk.word_tokenize(ground_truth.lower()), nltk.word_tokenize(summary.lower()))\n",
        "  \n",
        "\n",
        "df['dice_sim_gt_512_bio'] = dice_results\n",
        "df['generated_summary_gt_512_bio'] = summaries\n",
        "df.to_csv('889_data_dice_gt_512_bio.csv', header=True, index=False)\n",
        "end = time.time()\n",
        "print (end-start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing hannah_limit_gt_512_bio.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6JIqxDCVOYe"
      },
      "source": [
        "!mv hannah_limit_gt_512_bio.py transformersum/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_-jaIdRVQRc",
        "outputId": "a6871695-c469-4c45-b1b3-cc9561b3be87"
      },
      "source": [
        "%%shell \n",
        "\n",
        "eval \"$(conda shell.bash hook)\" # copy conda command to shell\n",
        "conda activate transformersum\n",
        "pip install -U sentence-transformers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cd transformersum/src\n",
        "python hannah_limit_gt_512_bio.py\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/envs/transformersum/lib/python3.9/site-packages (0.4.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/envs/transformersum/lib/python3.9/site-packages (from sentence-transformers) (0.24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/envs/transformersum/lib/python3.9/site-packages (from sentence-transformers) (1.20.1)\n",
            "Requirement already satisfied: nltk in /usr/local/envs/transformersum/lib/python3.9/site-packages (from sentence-transformers) (3.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/envs/transformersum/lib/python3.9/site-packages (from sentence-transformers) (0.1.95)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from sentence-transformers) (1.7.1.post2)\n",
            "Requirement already satisfied: tqdm in /usr/local/envs/transformersum/lib/python3.9/site-packages (from sentence-transformers) (4.49.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from sentence-transformers) (4.3.3)\n",
            "Requirement already satisfied: scipy in /usr/local/envs/transformersum/lib/python3.9/site-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/envs/transformersum/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/envs/transformersum/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.11.13)\n",
            "Requirement already satisfied: filelock in /usr/local/envs/transformersum/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/envs/transformersum/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/envs/transformersum/lib/python3.9/site-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied: joblib in /usr/local/envs/transformersum/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/envs/transformersum/lib/python3.9/site-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.26.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: six in /usr/local/envs/transformersum/lib/python3.9/site-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/envs/transformersum/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Warning: please export TSAN_OPTIONS='ignore_noninstrumented_modules=1' to avoid false positive reports from the OpenMP runtime!\n",
            "Exception when trying to download https://sbert.net/models/dmis-lab/biobert-v1.1.zip. Response 404\n",
            "SentenceTransformer-Model https://sbert.net/models/dmis-lab/biobert-v1.1.zip not found. Try to create it from scratch\n",
            "Try to create Transformer Model dmis-lab/biobert-v1.1 with mean pooling\n",
            "0\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "8168.129747390747\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "SmjwBM-Zs5DV",
        "outputId": "1a95d512-4fb2-4de5-dd45-677cd0a02d44"
      },
      "source": [
        "!cp transformersum/src/889_data_dice_gt_512_bio.csv .\n",
        "\n",
        "import pandas as pd\n",
        "df_out = pd.read_csv('889_data_dice_gt_512_bio.csv')\n",
        "df_out.head() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "      <th>summary_sent_count</th>\n",
              "      <th>main_text_len</th>\n",
              "      <th>words_per_sentence</th>\n",
              "      <th>sent_count</th>\n",
              "      <th>dice_sim_512</th>\n",
              "      <th>generated_summary_512</th>\n",
              "      <th>generated_summary_512_postprocess</th>\n",
              "      <th>summary_postprocess</th>\n",
              "      <th>dice_A_sim_512_postprocess.csv</th>\n",
              "      <th>dice_sim_gt_512_bio</th>\n",
              "      <th>generated_summary_gt_512_bio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>An effect assessment of Airborne particulate m...</td>\n",
              "      <td>This study aims to examine the effects of airb...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>1</td>\n",
              "      <td># Main text\\n\\n. It is confirmed that influenz...</td>\n",
              "      <td>Objective: Coronavirus disease 2019 (COVID-19)...</td>\n",
              "      <td>1</td>\n",
              "      <td>2566</td>\n",
              "      <td>18.729927</td>\n",
              "      <td>137</td>\n",
              "      <td>0.175439</td>\n",
              "      <td>Methods : In this study , we obtained confirme...</td>\n",
              "      <td>Methods : In this study , we obtained confirme...</td>\n",
              "      <td>This study aims to examine the effects of airb...</td>\n",
              "      <td>0.175439</td>\n",
              "      <td>0.131148</td>\n",
              "      <td>Methods : In this study , we obtained confirme...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aerosolized Particle Reduction: A Novel Cadave...</td>\n",
              "      <td>This study aimed to identify escape of small-p...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>2</td>\n",
              "      <td># Main text\\n\\nInstitutional review board appr...</td>\n",
              "      <td>OBJECTIVES: This study aimed to identify escap...</td>\n",
              "      <td>2</td>\n",
              "      <td>1727</td>\n",
              "      <td>28.783333</td>\n",
              "      <td>60</td>\n",
              "      <td>0.343195</td>\n",
              "      <td>This study also aimed to evaluate the efficacy...</td>\n",
              "      <td>This study also aimed to evaluate the efficacy...</td>\n",
              "      <td>This study aimed to identify escape of small-p...</td>\n",
              "      <td>0.343195</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>We present a new and effective method for the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COVID-19 Prevention and Control Strategies for...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\n\\n# Main text\\n\\nSince December 2019, the ou...</td>\n",
              "      <td>-1</td>\n",
              "      <td>3</td>\n",
              "      <td>1058</td>\n",
              "      <td>27.842105</td>\n",
              "      <td>38</td>\n",
              "      <td>0.177778</td>\n",
              "      <td>-1 \\n\\n # Main text \\n\\n Since December 2019 ,...</td>\n",
              "      <td>-1 Since December 2019 , the outbreak of a res...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>0.180791</td>\n",
              "      <td>0.192893</td>\n",
              "      <td>Third , psychiatric hospitals should assist th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Will COVID-19 pandemic diminish by summer-mons...</td>\n",
              "      <td>Therefore the study to investigate the meteoro...</td>\n",
              "      <td>11</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>21</td>\n",
              "      <td># Main text\\n\\nr=0.56) between temperature and...</td>\n",
              "      <td>The novel Coronavirus (2019-nCoV) was identifi...</td>\n",
              "      <td>21</td>\n",
              "      <td>6634</td>\n",
              "      <td>25.713178</td>\n",
              "      <td>258</td>\n",
              "      <td>0.138562</td>\n",
              "      <td>We have investigated the effect of meteorologi...</td>\n",
              "      <td>We have investigated the effect of meteorologi...</td>\n",
              "      <td>Therefore the study to investigate the meteoro...</td>\n",
              "      <td>0.138562</td>\n",
              "      <td>0.124675</td>\n",
              "      <td>https://doi.org/10.1101 Highlights : \\n\\n  Fi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Clinical evidence based review and recommendat...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>3</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nIn the era of globalization, in...</td>\n",
              "      <td>BACKGROUND: Aerosol generating medical procedu...</td>\n",
              "      <td>3</td>\n",
              "      <td>4137</td>\n",
              "      <td>23.913295</td>\n",
              "      <td>173</td>\n",
              "      <td>0.309179</td>\n",
              "      <td>The aim of this literature review was to ident...</td>\n",
              "      <td>The aim of this literature review was to ident...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>0.309179</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>This first review of papers led to the followi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                       generated_summary_gt_512_bio\n",
              "0  An effect assessment of Airborne particulate m...  ...  Methods : In this study , we obtained confirme...\n",
              "1  Aerosolized Particle Reduction: A Novel Cadave...  ...  We present a new and effective method for the ...\n",
              "2  COVID-19 Prevention and Control Strategies for...  ...  Third , psychiatric hospitals should assist th...\n",
              "3  Will COVID-19 pandemic diminish by summer-mons...  ...  https://doi.org/10.1101 Highlights : \\n\\n  Fi...\n",
              "4  Clinical evidence based review and recommendat...  ...  This first review of papers led to the followi...\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXivO494tILK",
        "outputId": "c365aa01-bfe6-4983-cbac-880054f20945"
      },
      "source": [
        "df_out['dice_sim_gt_512_bio'].mean()*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.71990950139814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkVTvxKDp0M1"
      },
      "source": [
        "#EXPERIMENT WITH MODEL DEALING WITH > 512 TOKENS - Weighted by SemSim using BioBERT- POSTPROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eOZMQn6JSmb"
      },
      "source": [
        "import re\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3dN6FiEIua-"
      },
      "source": [
        "def post_process(data):\n",
        "\n",
        "  data_tmp=[]\n",
        "\n",
        "  data = data.replace('# Main text', '')\n",
        "  data = re.sub(r'\\(\\s*[F|f]igure\\s*\\d+\\w?\\s*\\)','', data)\n",
        "\n",
        "  data = data.replace('\\n', ' ') #remove new lines\n",
        "  # return data\n",
        "  data = re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)','', data) #remove urls\n",
        "\n",
        "  data = re.sub(r'\\(\\w+\\s*(et\\sal\\.\\s)?\\d+\\)', '', data)    #remove reference items (Migliaccio et al. 2013 )\n",
        "\n",
        "  data = re.sub(r'as shown in fig\\.?\\d+', '', data)  #remove figure mentions\n",
        "\n",
        "  data = data.replace('url to online version','').replace('#','') #remove custom markers\n",
        "\n",
        "  # data = re.sub(r'[^\\w\\s]','',data)\n",
        "\n",
        "  data_tmp = re.sub(r'\\s{2,}', ' ', data)  #replace consecutive >=2 white spaces to just 1\n",
        "  return data_tmp.strip()   # join the list of words to make one big string\n",
        "\n",
        "def dice_similarity (list1, list2):\n",
        "  numer= len(list(set(list1).intersection(list2)))\n",
        "  deno=len(list1)+len(list2)\n",
        "  return 2*float(numer)/ deno \n",
        "\n",
        "def get_dice(g, s):\n",
        "  return dice_similarity(nltk.word_tokenize(g.lower()), nltk.word_tokenize(s.lower()))\n",
        "\n",
        "df_out['generated_summary_gt_512_bio_postprocess'] = df_out['generated_summary_gt_512_bio'].apply(lambda k: post_process(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "IltoiYbOIubF",
        "outputId": "c75932ae-b10f-4429-a9b1-b88e8cc762bf"
      },
      "source": [
        "dice_scores=[]\n",
        "for i in range(df_out.shape[0]):\n",
        "  gen=df_out.iloc[i]['generated_summary_gt_512_bio_postprocess']\n",
        "  summ=df_out.iloc[i]['summary_postprocess']\n",
        "  dice_scores.append(get_dice(gen, summ))\n",
        "df_out['dice_sim_gt_512_bio_postprocess']=dice_scores\n",
        "\n",
        "df_out.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-77d9fd2fcea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdice_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msumm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dice_sim_gt_512_bio_postprocess'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdice_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'889_data_dice_gt_512_bio.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdf_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTqoRFJMIubG",
        "outputId": "6688499b-707e-4b44-dc9c-37852be6bbe7"
      },
      "source": [
        "df_out['dice_sim_gt_512_bio_postprocess'].mean()*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.810165953630758"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5dZZqAYJvBw"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAsHZKabJvQp"
      },
      "source": [
        "from rouge import Rouge \n",
        "rouge = Rouge()\n",
        "\n",
        "def get_rouge(h, r):\n",
        "  return rouge.get_scores(h, r)\n",
        "\n",
        "rouge_scores=[]\n",
        "for i in range(df_out.shape[0]):\n",
        "  gen=df_out.iloc[i]['generated_summary_gt_512_bio_postprocess']\n",
        "  summ=df_out.iloc[i]['summary_postprocess']\n",
        "  rouge_scores.append(get_rouge(gen, summ))\n",
        "# df_out['rouge_sim_gt_512_postprocess']=dice_scores\n",
        "# df_out.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un9nBsCrJvQp"
      },
      "source": [
        "r1, r2, rL= [], [], []\n",
        "for i in rouge_scores:\n",
        "  r1.append(i[0]['rouge-1']['f'])\n",
        "  r2.append(i[0]['rouge-2']['f'])\n",
        "  rL.append(i[0]['rouge-l']['f'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izBizMPzJvQq",
        "outputId": "881b7b5d-0617-4e2e-97bb-a56049a54a98"
      },
      "source": [
        "np.mean(r1), np.mean(r2), np.mean(rL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.26814551763411854, 0.13520069146859548, 0.2654070495712493)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROpFLrb26wmZ"
      },
      "source": [
        "\n",
        "\n",
        "# AWS DATA EXTRACTION AND MANIPULATION\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsJwzxYdyMEq"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['889_data.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8_2x9LSEftl",
        "outputId": "4c8fce38-e742-4fd0-fb96-34c489a1a4a3"
      },
      "source": [
        "!pip install boto3\n",
        "# !pip install bert-embedding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.17.20)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.3.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.20 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.20.20)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.20->boto3) (1.26.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.20->boto3) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.20->boto3) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjQbCn8l-hhU"
      },
      "source": [
        "import json\n",
        "import boto3\n",
        "\n",
        "# Let's use Amazon S3\n",
        "s3_resource = boto3.resource('s3', aws_access_key_id ='AKIAY44QSVLZKNSS4ELO', aws_secret_access_key ='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')\n",
        "s3_client = boto3.client('s3', aws_access_key_id ='AKIAY44QSVLZKNSS4ELO', aws_secret_access_key ='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')\n",
        "\n",
        "json_bucket = s3_resource.Bucket('pansurg-curation-workflo-groundtruthlabelingresul-i0mmv3x80jt1') #this is where the json files are kept \n",
        "fulltext_bucket = 'pansurg-curation-workflo-kendraqueryresults50d0eb-vhc009b4k983' #this is where the full txt files are kept\n",
        "\n",
        "\n",
        "\n",
        "# result = s3.list_objects(Bucket=your_bucket, Prefix='AllCurations', Delimiter='/')\n",
        "# for o in result.get('CommonPrefixes'):\n",
        "#     print ('sub folder : ', o.get('Prefix'))\n",
        "\n",
        "# Upload a new file\n",
        "# data = open('pansurg-curation-workflo-groundtruthlabelingresul-i0mmv3x80jt1/AllCurations/REDASAq10s2A/') #2B is the other curator \n",
        "# s3.Bucket('pansurg-curation-workflo-groundtruthlabelingresul-i0mmv3x80jt1').put_object(Key='pansurg-curation-workflo-groundtruthlabelingresul-i0mmv3x80jt1', Body=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXqiJkFijDHB"
      },
      "source": [
        "   endidx= highlight['endOffset'] # this is the end off set of each chunk \n",
        "        startidx= highlight['startOffset']\n",
        "        mididx= (startidx+endidx)/2 \n",
        "\n",
        "        if mididx>=0 and mididx<segment_length:\n",
        "          section1.append(1)\n",
        "        elif mididx>=segment_length and mididx<segment_length*2:\n",
        "          section2.append(1)\n",
        "        elif mididx>=segment_length*2 and mididx<segment_length*3:\n",
        "          section3.append(1)\n",
        "        else:\n",
        "          section4.append(1)\n",
        "          # the code above is determining which section the summary highlight is in \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVj6AtGlIxWO",
        "outputId": "6c17dc34-d6fd-4af9-e82e-179635bbe6e1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n",
        "all_data = []\n",
        "section1, section2, section3, section4= [], [], [], []\n",
        "MIN_CHAR_TO_KEEP=5 \n",
        "MAX_CHAR_TO_KEEP= MIN_CHAR_TO_KEEP*100\n",
        "segment_length=5151.740816326531\n",
        "\n",
        "\n",
        "for idx, s3_file in enumerate(json_bucket.objects.filter(Prefix='AllCurations/REDA')):  # creating a for loop to identify all objects with that directary #your_bucket.objects.all():\n",
        "    body = json.loads(s3_file.get()['Body'].read()) # the body key is found in the json file, we want to read it so we load it as a json\n",
        "    #print (body) #body of the file\n",
        "\n",
        "    if 'errorreport' in body: #if error then skips this paper\n",
        "      # print (f\"Error Files {body['dataObject']}\")\n",
        "      continue # if the error exists we move to the next file and continue the loop\n",
        "    \n",
        "    summary_buffer = \"\" # if we dont have the error we create two variables \n",
        "    count=0\n",
        "    for highlight in body['crowd-entity-annotation']: # we loop though the list and look for crowd entity annotation \n",
        "      relevant_file = '/'.join(body['dataObject'].split('/')[3:]) # we also want to open the dataObject which is also part of the body, this will give us the entire path name (post bucket name)\n",
        "\n",
        "      full_text_file_object = s3_client.get_object(Bucket=fulltext_bucket, Key=relevant_file) #get the relivent files from the bucket in specififc place i.e key \n",
        "      full_text_file_text = full_text_file_object['Body'].read() # we read its body and store it \n",
        "      content = full_text_file_text.decode('utf-8') # we decode it in utf-8 as there could be some special characters in the file which might create a problem later, the entire text is saved under content\n",
        "\n",
        "      if highlight['label']=='summary': #check if the lable is summary or not\n",
        "        content_data=content[highlight['startOffset']:highlight['endOffset']]\n",
        "\n",
        "        if len(content_data) > MIN_CHAR_TO_KEEP and len(content_data) < MAX_CHAR_TO_KEEP: # the conten t has to be within the threshehold length to be counted \n",
        "          count += 1 #how many summary segements people highlighted \n",
        "        summary_buffer += content_data # the characters which relate to the labes are as a string and stored in the summary buffer \n",
        "        # keep going through the file and add to the sumamry till there is no summary highlights left\n",
        "\n",
        "        endidx= highlight['endOffset'] # this is the end off set of each chunk \n",
        "        startidx= highlight['startOffset']\n",
        "        mididx= (startidx+endidx)/2 \n",
        "\n",
        "        if mididx>=0 and mididx<segment_length:\n",
        "          section1.append(1)\n",
        "        elif mididx>=segment_length and mididx<segment_length*2:\n",
        "          section2.append(1)\n",
        "        elif mididx>=segment_length*2 and mididx<segment_length*3:\n",
        "          section3.append(1)\n",
        "        else:\n",
        "          section4.append(1)\n",
        "          # the code above is determining which section the summary highlight is in \n",
        "        \n",
        "\n",
        "    all_sentences = len(nltk.sent_tokenize(summary_buffer))\n",
        "    all_data.append(['dummy title', summary_buffer, count, content, startidx, endidx, all_sentences, 'main_text', 'abstract']) #append values to dictonary in that order\n",
        "    # print ('\\n\\n') #spaces inbetween \n",
        "\n",
        "    # remove these lines\n",
        "    if idx>1000:\n",
        "     break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFs6Vcxa68ET"
      },
      "source": [
        "LEN_FULL_TEXT=[len(i) for i in df2['full_text']] # the length of the full text for each document \n",
        "DOC_AVG_LEN= sum(LEN_FULL_TEXT)/ len(LEN_FULL_TEXT) # the avrage document length \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WX9sOIT42UuC",
        "outputId": "43ca30d3-1a61-4577-c625-ef64fb393cdd"
      },
      "source": [
        "sections_required= 4.0\n",
        "segemnt_length= DOC_AVG_LEN/sections_required\n",
        "print (segemnt_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5151.740816326531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "hOYwsTESDJYT",
        "outputId": "21d9f46e-ca7e-4ba2-8047-a7f7cac44c84"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "x = ['section1', 'section2', 'section3', 'section4']\n",
        "y = [len(section1), len(section2), len(section3), len(section4)]\n",
        "ax.bar(x, y)\n",
        "\n",
        "plt.xlabel ('Section of paper')\n",
        "plt.ylabel ('No. of segments')\n",
        "plt.title ('Distribution of summary highlights')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja8pCmbz-Eot"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "Xb7pNSR0GSVK",
        "outputId": "e1e5eea5-bb2e-42b8-e536-c06fe5efa4b8"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.DataFrame(all_data, columns = ['title', 'summary', 'segment_count', 'full_text', 'all_sentences','main_text','abstract'])  \n",
        "\n",
        "df.head(20000)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_or_indexify_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    688\u001b[0m             raise AssertionError(\n\u001b[0;32m--> 689\u001b[0;31m                 \u001b[0;34mf\"{len(columns)} columns passed, passed data had \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m                 \u001b[0;34mf\"{len(content)} columns\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: 7 columns passed, passed data had 9 columns",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-c9a299498b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'summary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'segment_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'full_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'all_sentences'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'main_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    507\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_named_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# columns if columns is not None else []\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         return _list_of_dict_to_arrays(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_to_arrays\u001b[0;34m(data, columns, coerce_float, dtype)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_object_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoerce_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoerce_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 7 columns passed, passed data had 9 columns"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmCPmOTkMwC8"
      },
      "source": [
        "import re\n",
        "\n",
        "def extract_title(text):\n",
        "  data = re.findall(r'Title\\n\\n+.+?\\n\\n+', text, flags=re.IGNORECASE) #this is where the title is in the text, ignoring upper case\n",
        "  if len(data):\n",
        "    return data[0].replace('Title\\n\\n', '') # if the length is more than one character return the data\n",
        "  else: \n",
        "    return '-1' # if not put -1\n",
        "\n",
        "def extract_abstract(text):\n",
        "  data = re.findall(r'Abstract\\n\\n+.+?\\n\\n\\n+', text, flags=re.IGNORECASE)\n",
        "  if len(data):\n",
        "    return data[0].replace('Abstract\\n\\n', '')\n",
        "  else:\n",
        "    return '-1'\n",
        "  \n",
        "def extract_maintext(text):\n",
        "  try:\n",
        "    text = text.split('\\n\\n\\n')\n",
        "    return text[-2].replace('Main text\\n\\n','')\n",
        "  except:\n",
        "    return '-1'\n",
        "  # text = text.replace('\\n', '||')\n",
        "  # data = re.findall(r'Main text||||+.+', text, flags=re.MULTILINE)\n",
        "  # print (data)\n",
        "  # if len(data):\n",
        "  #   return data[0].replace('Main text||||', '')\n",
        "  # else:\n",
        "  #   return '-1'\n",
        "\n",
        "df['abstract']= df['full_text'].map(lambda x: extract_abstract(x)) # map runs every line in the full text i.e in x which is the entire content of the paper, pass it to extract_ function\n",
        "df['title']= df['full_text'].map(lambda x: extract_title(x)) # override the title to give the proper title name\n",
        "df['main_text']= df['full_text'].map(lambda x: extract_maintext(x)) #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "r0McsUcINcsh",
        "outputId": "02cf8600-e858-4db4-c301-b9d22baee078"
      },
      "source": [
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>An effect assessment of Airborne particulate m...</td>\n",
              "      <td>This study aims to examine the effects of airb...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>1</td>\n",
              "      <td># . It is confirmed that influenza is a season...</td>\n",
              "      <td>Objective: Coronavirus disease 2019 (COVID-19)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aerosolized Particle Reduction: A Novel Cadave...</td>\n",
              "      <td>This study aimed to identify escape of small-p...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>2</td>\n",
              "      <td># Institutional review board approval was obta...</td>\n",
              "      <td>OBJECTIVES: This study aimed to identify escap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>This version updates the 27 March publication ...</td>\n",
              "      <td>0</td>\n",
              "      <td>This version updates the 27 March publication ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td>This version updates the 27 March publication ...</td>\n",
              "      <td>0</td>\n",
              "      <td>This version updates the 27 March publication ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>COVID-19 may transmit through aerosol\\n\\n\\n</td>\n",
              "      <td></td>\n",
              "      <td>0</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>0</td>\n",
              "      <td>\\n\\n# Dear Editor,\\n\\nOn Feb 18, the National ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                           abstract\n",
              "0  An effect assessment of Airborne particulate m...  ...  Objective: Coronavirus disease 2019 (COVID-19)...\n",
              "1  Aerosolized Particle Reduction: A Novel Cadave...  ...  OBJECTIVES: This study aimed to identify escap...\n",
              "2                                                 -1  ...                                                 -1\n",
              "3                                                 -1  ...                                                 -1\n",
              "4        COVID-19 may transmit through aerosol\\n\\n\\n  ...                                                 -1\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "9WQrQHTGNgeS",
        "outputId": "c2ac9368-911d-46ca-d022-4b47d4ccc228"
      },
      "source": [
        "df_filter = df[df['segment_count']!=0]\n",
        "df_filter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>An effect assessment of Airborne particulate m...</td>\n",
              "      <td>This study aims to examine the effects of airb...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>1</td>\n",
              "      <td># . It is confirmed that influenza is a season...</td>\n",
              "      <td>Objective: Coronavirus disease 2019 (COVID-19)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aerosolized Particle Reduction: A Novel Cadave...</td>\n",
              "      <td>This study aimed to identify escape of small-p...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>2</td>\n",
              "      <td># Institutional review board approval was obta...</td>\n",
              "      <td>OBJECTIVES: This study aimed to identify escap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>COVID-19 Prevention and Control Strategies for...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\n\\n# Since December 2019, the outbreak of a r...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Will COVID-19 pandemic diminish by summer-mons...</td>\n",
              "      <td>Therefore the study to investigate the meteoro...</td>\n",
              "      <td>11</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>21</td>\n",
              "      <td># r=0.56) between temperature and daily COVID-...</td>\n",
              "      <td>The novel Coronavirus (2019-nCoV) was identifi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Clinical evidence based review and recommendat...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>3</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td>During the COVID-19 pandemic, tracheotomy, end...</td>\n",
              "      <td>BACKGROUND: Aerosol generating medical procedu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>The Architecture of SARS-CoV-2 Transcriptome\\n...</td>\n",
              "      <td>In this study, we combined two complementary s...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>15</td>\n",
              "      <td>Further information and requests for resources...</td>\n",
              "      <td>Summary SARS-CoV-2 is a betacoronavirus respon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>885</th>\n",
              "      <td>SARS-CoV-2 SPIKE PROTEIN: an optimal immunolog...</td>\n",
              "      <td>. To date, no therapeutics or vaccines against...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># The outbreak of the novel coronavirus diseas...</td>\n",
              "      <td>COVID-19 has rapidly spread all over the world...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>SARS-CoV-2: diagnostic and design conundrums, ...</td>\n",
              "      <td>In males, ACE2 receptor sites have been report...</td>\n",
              "      <td>2</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>8</td>\n",
              "      <td># As the global death toll from the COVID-19 [...</td>\n",
              "      <td>The question on whether SARS-CoV-2 (Severe acu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>False positives in reverse transcription PCR t...</td>\n",
              "      <td>Review of external quality assessments reveale...</td>\n",
              "      <td>2</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>2</td>\n",
              "      <td># Large-scale testing for SARS-CoV-2 is a key ...</td>\n",
              "      <td>Background: Large-scale testing for SARS-CoV-2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>SARS-CoV-2 seroconversion in health care worke...</td>\n",
              "      <td>asymptomatic seroconversion occurs, however pr...</td>\n",
              "      <td>2</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>4</td>\n",
              "      <td># The correlates of protection against SARS-Co...</td>\n",
              "      <td>Background The correlates of protection agains...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>490 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 title  ...                                           abstract\n",
              "0    An effect assessment of Airborne particulate m...  ...  Objective: Coronavirus disease 2019 (COVID-19)...\n",
              "1    Aerosolized Particle Reduction: A Novel Cadave...  ...  OBJECTIVES: This study aimed to identify escap...\n",
              "5    COVID-19 Prevention and Control Strategies for...  ...                                                 -1\n",
              "8    Will COVID-19 pandemic diminish by summer-mons...  ...  The novel Coronavirus (2019-nCoV) was identifi...\n",
              "9    Clinical evidence based review and recommendat...  ...  BACKGROUND: Aerosol generating medical procedu...\n",
              "..                                                 ...  ...                                                ...\n",
              "883  The Architecture of SARS-CoV-2 Transcriptome\\n...  ...  Summary SARS-CoV-2 is a betacoronavirus respon...\n",
              "885  SARS-CoV-2 SPIKE PROTEIN: an optimal immunolog...  ...  COVID-19 has rapidly spread all over the world...\n",
              "886  SARS-CoV-2: diagnostic and design conundrums, ...  ...  The question on whether SARS-CoV-2 (Severe acu...\n",
              "887  False positives in reverse transcription PCR t...  ...  Background: Large-scale testing for SARS-CoV-2...\n",
              "888  SARS-CoV-2 seroconversion in health care worke...  ...  Background The correlates of protection agains...\n",
              "\n",
              "[490 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "wmyEbhYjNmcd",
        "outputId": "dbda9553-5875-47da-e1d1-72878518d2cb"
      },
      "source": [
        "df_filter= df_filter.reset_index(drop=True)\n",
        "df_filter.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>An effect assessment of Airborne particulate m...</td>\n",
              "      <td>This study aims to examine the effects of airb...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>1</td>\n",
              "      <td># . It is confirmed that influenza is a season...</td>\n",
              "      <td>Objective: Coronavirus disease 2019 (COVID-19)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aerosolized Particle Reduction: A Novel Cadave...</td>\n",
              "      <td>This study aimed to identify escape of small-p...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>2</td>\n",
              "      <td># Institutional review board approval was obta...</td>\n",
              "      <td>OBJECTIVES: This study aimed to identify escap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COVID-19 Prevention and Control Strategies for...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\n\\n# Since December 2019, the outbreak of a r...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Will COVID-19 pandemic diminish by summer-mons...</td>\n",
              "      <td>Therefore the study to investigate the meteoro...</td>\n",
              "      <td>11</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>21</td>\n",
              "      <td># r=0.56) between temperature and daily COVID-...</td>\n",
              "      <td>The novel Coronavirus (2019-nCoV) was identifi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Clinical evidence based review and recommendat...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>3</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td>During the COVID-19 pandemic, tracheotomy, end...</td>\n",
              "      <td>BACKGROUND: Aerosol generating medical procedu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                           abstract\n",
              "0  An effect assessment of Airborne particulate m...  ...  Objective: Coronavirus disease 2019 (COVID-19)...\n",
              "1  Aerosolized Particle Reduction: A Novel Cadave...  ...  OBJECTIVES: This study aimed to identify escap...\n",
              "2  COVID-19 Prevention and Control Strategies for...  ...                                                 -1\n",
              "3  Will COVID-19 pandemic diminish by summer-mons...  ...  The novel Coronavirus (2019-nCoV) was identifi...\n",
              "4  Clinical evidence based review and recommendat...  ...  BACKGROUND: Aerosol generating medical procedu...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1uimtY9wN7Kt",
        "outputId": "308cb34b-c8ba-45d1-8028-ed16fa603767"
      },
      "source": [
        "df_filter.iloc[1]['full_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# URL to online version\\n\\nhttps://www.ncbi.nlm.nih.gov/pubmed/32423338/\\nhttps://doi.org/10.1177/0194599820929275\\n\\n\\n# Title\\n\\nAerosolized Particle Reduction: A Novel Cadaveric Model and a Negative Airway Pressure Respirator (NAPR) System to Protect Health Care Workers From COVID-19\\n\\n\\n# Abstract\\n\\nOBJECTIVES: This study aimed to identify escape of small-particle aerosols from a variety of masks using simulated breathing conditions. This study also aimed to evaluate the efficacy of a negative-pressure environment around the face in preventing the escape of small aerosolized particles. STUDY DESIGN: This study is an evaluation study with specific methodology described below. SETTING: This study was performed in our institution’s fresh tissue laboratory. SUBJECTS AND METHODS: A fixed cadaver head was placed in a controlled environment with a black background, and small-particle aerosols were created using joss incense sticks (mass-median aerosol diameter of 0.28 µ). Smoke was passed through the cadaver head, and images were taken with a high-resolution camera in a standardized manner. Digital image processing was used to calculate relative amounts of small-particle escape from a variety of masks, including a standard surgical mask, a modified Ambu mask, and our negative airway pressure respirator (NAPR). RESULTS: Significant amounts of aerosolized particles escaped during the trials with no mask, a standard surgical mask, and the NAPR without suction. When suction was applied to the NAPR, creating a negative-pressure system, no particle escape was noted. CONCLUSION: We present a new and effective method for the study of small-particle aerosols as a step toward better understanding the spread of these particles and the transmission of coronavirus disease 2019. We also present the concept of an NAPR to better protect health care workers from aerosols generated from the upper and lower airways.\\n\\n\\n# Main text\\n\\nInstitutional review board approval was obtained from the Thomas Jefferson University institutional review board. To generate a visible small-particle aerosol, joss incense sticks were used. The smoke generated from this type of incense has been shown to have a mass-median aerosol diameter of 0.28 µ and is white colored.7 Incense was burned in a collection vessel until it was filled with smoke. Smoke was then siphoned from a valve at the top of the collection vessel using a modified Ambu bag, and the process was repeated for each trial.\\n\\nA fixed cadaver head was placed in a controlled environment with a black background. Care was taken to minimize any extraneous light contamination. A size 8.0 endotracheal tube was then inserted into the trachea from below and the cuff was inflated. The smoke-filled modified Ambu bag was attached to the endotracheal tube and the contents of the bag were emptied over 3 seconds. This method was used to test several different scenarios: a cadaver with no mask (Figure 1), with a standard surgical mask (Figure 2), with a modified Ambu anesthesia mask (Figure 3), and with our novel mask—an Ambu mask fitted with suction tubing attached to a HEPA filtration system—which we have named a negative airway pressure respirator or “NAPR” (Figure 4). A digital camera with 18-megapixel resolution was used to capture the smoke escape from the cadaver, generating 36 images over each 3-second run. Using Adobe Photoshop v.20 (Adobe, Inc), a thresholding filter was applied to the first image in each run down to the noise floor (Figure 1B). A thresholding filter was then applied at the same level to the photo two-thirds of the way through each run (Figure 1C). Subtraction images were then generated by subtracting the processed first picture of each run to the processed picture two-thirds of the way through the run. A white pixel count was then performed to attempt to quantify the amount of smoke present in the field.\\n\\nThe pictures generated by this method contained 1.8 × 107 pixels. Given the size of the field of view, each pixel represented an area on the order of 1 × 10−5 cm2 or 1000 µ2. Using thresholding, an environment with carefully controlled light, a rapid frame rate, and a stable setup, we were able to generate subtraction images that could identify exposure value difference ratios down to 1.08, which correlates to a 0.8% light emittance difference at the camera’s detector. Assuming the particles block light, this translates to the ability to detect particles down to a size of approximately 8 µ2.\\n\\nThe specialized NAPR was designed by taking a standard Ambu mask, drilling a 9-mm hole in the plastic near the bottom of the mask, and inserting 10-mm diameter suction tubing through the new aperture (Figure 5). This mask was used to test the effect of a negative-pressure environment on the spread of aerosols using a pressure of −120 mm Hg.\\n\\nThe first trial was performed without any mask on the cadaver. It was clear that without a mask, a large amount of aerosolized particles were released (Figure 1). After thresholding and subtracting the image two-thirds of the way through the run with the first image of the run, 27,486 white pixels were detected (Figure 1D). When a standard surgical mask was added, the number of white pixels detected with this method was reduced to 21,379, but there were still aerosolized particles being released mainly from the top and sides of the mask (Figure 2). A trial was performed using the NAPR without any suction attached, and 3835 white pixels were detected (Figure 3). A trial performed after applying suction to the NAPR revealed 88 white pixels, which was due to background noise from a small amount of movement between the first photo and the photo used for analysis (Figure 4). This tiny amount of noise was present in all of the runs but was calculated to be <1% of the overall pixel count in each analysis. Trials were also performed in which suction was initiated halfway through the run, and it was noted that once suction was initiated, no additional particles escaped and particles seemed to regress from beyond the mask back into the negative-pressure environment. The amount of regression and escape was difficult to quantify due to motion artifact from connecting the suction tubing midway through the run and not having a consistent benchmark image with which to compare the first image. Trials were also performed using a drill and endoscope in the nose; while no detectable particle escape was noted, it was somewhat difficult to quantify given the motion artifact on the photos from the endoscope and drilling (Figure 5).\\n\\nHere we present a method to test for the escape of small aerosolized particles from a patient’s airway as well as a novel negative-pressure mask concept. Based on our model, the NAPR mask seems to be protective against aerosol spread even in the scenario in which a patient is forcefully exhaling while an airway procedure is being performed. There have been multiple recent reports describing the risks of endonasal procedures with regard to transmission of COVID-19.8 Several interesting models have been recently published. Workman et al4 published a method using atomized fluorescein introduced into the nasal cavity through a defect in the cribriform. This interesting and timely article was primarily aimed at detecting large particles on the floor after endonasal procedures. Our work complements this by evaluating small- and medium-sized particles in an environment close to the patient’s face. In Workman et al,4 a mask was also proposed that limited the spread of large, fluorescein-coated tissue particles. In our study, we show that even with a surgical mask, small aerosolized particles can still escape if a patient is exhaling. Clearly, this would not be the case if the patient was intubated for surgery unless there was a cuff leak or endotracheal tube migration. However, when performing procedures on the upper and lower airways on an awake patient or when generating turbulent airflow in the airways of an intubated patient, small-particle aerosols can potentially be generated, and these were not able to be examined by methodology in Workman et al.4 For these reasons, we believe our findings to be germane to their results.\\n\\nSimilar to Workman et al,4 Anfinrud et al5 showed the patterns of droplet particle spread during speech in a recently published letter with accompanying video. By using a laser sheet and an iPhone, they were able to show that a surgical mask is able to contain most large droplet particles during speech. This is a useful experiment and certainly is applicable to everyday community transmission of COVID-19. However, Meselson’s comments9 on the article mentioned that this model may not be able to adequately capture smaller droplets and particles such as are present with COVID-19. While we are still learning more about this virus and the different methods by which it can transmitted, the WHO has put out guidelines cautioning that although COVID-19 is generally assumed to be transmitted via droplets, it can become airborne in circumstances including airway manipulation, and smaller particles need to be considered infective.6 There are also now multiple independent reports postulating that COVID-19 can indeed be spread through an airborne route.10,11\\n\\nIt is our belief that a local negative-pressure environment around the patient’s nose and mouth will be instrumental in minimizing the risk associated with procedures of the upper and lower airways. Based on our model, the NAPR was extremely effective at eliminating escape of small aerosolized particles even with simulated forced exhalation. We were able to adequately work in the nose given the constraints of the mask, but it could easily be modified further to facilitate other oral, laryngeal, or endonasal procedures, and other mask modifications are forthcoming (Figure 5). Given the nature of the current pandemic, using this mask on a patient while performing bronchoscopy could be a timely and helpful innovation.\\n\\nThis study has some limitations. Movements of the setup during the 3-second exhalation created some noise, which could affect analyzing the images; analyzing a 2-dimensional picture of a 3-dimensional reality always has certain limitations; and it is somewhat difficult to get each run perfect so that this method becomes very accurate quantitatively. Despite some inherent drawbacks, however, we feel that this is an excellent first step in measuring small-particle escape. Our model also represents a useful and effective method to test protective equipment. We also feel that the concept of an NAPR, irrespective of design, could potentially have far-reaching applications in the medical field. Future directions include refining our image-gathering and image-processing techniques, testing additional mask prototypes, and trials in live subjects.\\n\\nWe present a new and effective method for the study of small-particle aerosols as a step toward better understanding the spread of these particles and the transmission of COVID-19. We also present the concept of a novel NAPR to better protect health care workers.\\n\\nTawfiq Khoury, running experiments, preparing the manuscript; Pascal Lavergne, running experiments, preparing the manuscript; Chandala Chitguppi, running experiments, preparing the manuscript; Mindy Rabinowitz, project design, manuscript preparation; Gurston Nyquist, project design, manuscript preparation; Marc Rosen, project design, manuscript preparation; James Evans, running experiments, preparing the manuscript.\\n\\nCompeting interests: None.\\n\\nSponsorships: None.\\n\\nFunding source: None.\\n\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WWG6QAoJN-I_",
        "outputId": "6a92038c-b8cd-4927-ef31-567f53af6c8c"
      },
      "source": [
        "df_filter.iloc[1]['main_text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Institutional review board approval was obtained from the Thomas Jefferson University institutional review board. To generate a visible small-particle aerosol, joss incense sticks were used. The smoke generated from this type of incense has been shown to have a mass-median aerosol diameter of 0.28 µ and is white colored.7 Incense was burned in a collection vessel until it was filled with smoke. Smoke was then siphoned from a valve at the top of the collection vessel using a modified Ambu bag, and the process was repeated for each trial.\\n\\nA fixed cadaver head was placed in a controlled environment with a black background. Care was taken to minimize any extraneous light contamination. A size 8.0 endotracheal tube was then inserted into the trachea from below and the cuff was inflated. The smoke-filled modified Ambu bag was attached to the endotracheal tube and the contents of the bag were emptied over 3 seconds. This method was used to test several different scenarios: a cadaver with no mask (Figure 1), with a standard surgical mask (Figure 2), with a modified Ambu anesthesia mask (Figure 3), and with our novel mask—an Ambu mask fitted with suction tubing attached to a HEPA filtration system—which we have named a negative airway pressure respirator or “NAPR” (Figure 4). A digital camera with 18-megapixel resolution was used to capture the smoke escape from the cadaver, generating 36 images over each 3-second run. Using Adobe Photoshop v.20 (Adobe, Inc), a thresholding filter was applied to the first image in each run down to the noise floor (Figure 1B). A thresholding filter was then applied at the same level to the photo two-thirds of the way through each run (Figure 1C). Subtraction images were then generated by subtracting the processed first picture of each run to the processed picture two-thirds of the way through the run. A white pixel count was then performed to attempt to quantify the amount of smoke present in the field.\\n\\nThe pictures generated by this method contained 1.8 × 107 pixels. Given the size of the field of view, each pixel represented an area on the order of 1 × 10−5 cm2 or 1000 µ2. Using thresholding, an environment with carefully controlled light, a rapid frame rate, and a stable setup, we were able to generate subtraction images that could identify exposure value difference ratios down to 1.08, which correlates to a 0.8% light emittance difference at the camera’s detector. Assuming the particles block light, this translates to the ability to detect particles down to a size of approximately 8 µ2.\\n\\nThe specialized NAPR was designed by taking a standard Ambu mask, drilling a 9-mm hole in the plastic near the bottom of the mask, and inserting 10-mm diameter suction tubing through the new aperture (Figure 5). This mask was used to test the effect of a negative-pressure environment on the spread of aerosols using a pressure of −120 mm Hg.\\n\\nThe first trial was performed without any mask on the cadaver. It was clear that without a mask, a large amount of aerosolized particles were released (Figure 1). After thresholding and subtracting the image two-thirds of the way through the run with the first image of the run, 27,486 white pixels were detected (Figure 1D). When a standard surgical mask was added, the number of white pixels detected with this method was reduced to 21,379, but there were still aerosolized particles being released mainly from the top and sides of the mask (Figure 2). A trial was performed using the NAPR without any suction attached, and 3835 white pixels were detected (Figure 3). A trial performed after applying suction to the NAPR revealed 88 white pixels, which was due to background noise from a small amount of movement between the first photo and the photo used for analysis (Figure 4). This tiny amount of noise was present in all of the runs but was calculated to be <1% of the overall pixel count in each analysis. Trials were also performed in which suction was initiated halfway through the run, and it was noted that once suction was initiated, no additional particles escaped and particles seemed to regress from beyond the mask back into the negative-pressure environment. The amount of regression and escape was difficult to quantify due to motion artifact from connecting the suction tubing midway through the run and not having a consistent benchmark image with which to compare the first image. Trials were also performed using a drill and endoscope in the nose; while no detectable particle escape was noted, it was somewhat difficult to quantify given the motion artifact on the photos from the endoscope and drilling (Figure 5).\\n\\nHere we present a method to test for the escape of small aerosolized particles from a patient’s airway as well as a novel negative-pressure mask concept. Based on our model, the NAPR mask seems to be protective against aerosol spread even in the scenario in which a patient is forcefully exhaling while an airway procedure is being performed. There have been multiple recent reports describing the risks of endonasal procedures with regard to transmission of COVID-19.8 Several interesting models have been recently published. Workman et al4 published a method using atomized fluorescein introduced into the nasal cavity through a defect in the cribriform. This interesting and timely article was primarily aimed at detecting large particles on the floor after endonasal procedures. Our work complements this by evaluating small- and medium-sized particles in an environment close to the patient’s face. In Workman et al,4 a mask was also proposed that limited the spread of large, fluorescein-coated tissue particles. In our study, we show that even with a surgical mask, small aerosolized particles can still escape if a patient is exhaling. Clearly, this would not be the case if the patient was intubated for surgery unless there was a cuff leak or endotracheal tube migration. However, when performing procedures on the upper and lower airways on an awake patient or when generating turbulent airflow in the airways of an intubated patient, small-particle aerosols can potentially be generated, and these were not able to be examined by methodology in Workman et al.4 For these reasons, we believe our findings to be germane to their results.\\n\\nSimilar to Workman et al,4 Anfinrud et al5 showed the patterns of droplet particle spread during speech in a recently published letter with accompanying video. By using a laser sheet and an iPhone, they were able to show that a surgical mask is able to contain most large droplet particles during speech. This is a useful experiment and certainly is applicable to everyday community transmission of COVID-19. However, Meselson’s comments9 on the article mentioned that this model may not be able to adequately capture smaller droplets and particles such as are present with COVID-19. While we are still learning more about this virus and the different methods by which it can transmitted, the WHO has put out guidelines cautioning that although COVID-19 is generally assumed to be transmitted via droplets, it can become airborne in circumstances including airway manipulation, and smaller particles need to be considered infective.6 There are also now multiple independent reports postulating that COVID-19 can indeed be spread through an airborne route.10,11\\n\\nIt is our belief that a local negative-pressure environment around the patient’s nose and mouth will be instrumental in minimizing the risk associated with procedures of the upper and lower airways. Based on our model, the NAPR was extremely effective at eliminating escape of small aerosolized particles even with simulated forced exhalation. We were able to adequately work in the nose given the constraints of the mask, but it could easily be modified further to facilitate other oral, laryngeal, or endonasal procedures, and other mask modifications are forthcoming (Figure 5). Given the nature of the current pandemic, using this mask on a patient while performing bronchoscopy could be a timely and helpful innovation.\\n\\nThis study has some limitations. Movements of the setup during the 3-second exhalation created some noise, which could affect analyzing the images; analyzing a 2-dimensional picture of a 3-dimensional reality always has certain limitations; and it is somewhat difficult to get each run perfect so that this method becomes very accurate quantitatively. Despite some inherent drawbacks, however, we feel that this is an excellent first step in measuring small-particle escape. Our model also represents a useful and effective method to test protective equipment. We also feel that the concept of an NAPR, irrespective of design, could potentially have far-reaching applications in the medical field. Future directions include refining our image-gathering and image-processing techniques, testing additional mask prototypes, and trials in live subjects.\\n\\nWe present a new and effective method for the study of small-particle aerosols as a step toward better understanding the spread of these particles and the transmission of COVID-19. We also present the concept of a novel NAPR to better protect health care workers.\\n\\nTawfiq Khoury, running experiments, preparing the manuscript; Pascal Lavergne, running experiments, preparing the manuscript; Chandala Chitguppi, running experiments, preparing the manuscript; Mindy Rabinowitz, project design, manuscript preparation; Gurston Nyquist, project design, manuscript preparation; Marc Rosen, project design, manuscript preparation; James Evans, running experiments, preparing the manuscript.\\n\\nCompeting interests: None.\\n\\nSponsorships: None.\\n\\nFunding source: None.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx7JqOidOxuW"
      },
      "source": [
        "df_filter.to_csv('889_data.csv', header= True, index= False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz6oDXN7fp89"
      },
      "source": [
        "# DATA ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY0xygcfrgBk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-c4JXm7PgFS"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuNnInyVPhcL"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "df = pd.read_csv('889_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOf_jh-XPwv6",
        "outputId": "4faed5e7-ea16-49d6-ff51-8db6b9b5e5c9"
      },
      "source": [
        "df = df[df['segment_count']!=0]\n",
        "df.head(), df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                               title  ...                                           abstract\n",
              " 0  An effect assessment of Airborne particulate m...  ...  Objective: Coronavirus disease 2019 (COVID-19)...\n",
              " 1  Aerosolized Particle Reduction: A Novel Cadave...  ...  OBJECTIVES: This study aimed to identify escap...\n",
              " 2  COVID-19 Prevention and Control Strategies for...  ...                                                 -1\n",
              " 3  Will COVID-19 pandemic diminish by summer-mons...  ...  The novel Coronavirus (2019-nCoV) was identifi...\n",
              " 4  Clinical evidence based review and recommendat...  ...  BACKGROUND: Aerosol generating medical procedu...\n",
              " \n",
              " [5 rows x 7 columns], (490, 7))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7nadfl3jSoe"
      },
      "source": [
        "import re\n",
        "\n",
        "def extract_title(text):\n",
        "  data = re.findall(r'Title\\n\\n+.+?\\n\\n+', text, flags=re.IGNORECASE) #this is where the title is in the text, ignoring upper case\n",
        "  if len(data):\n",
        "    return data[0].replace('Title\\n\\n', '') # if the length is more than one character return the data\n",
        "  else: \n",
        "    return '-1' # if not put -1\n",
        "\n",
        "def extract_abstract(text):\n",
        "  data = re.findall(r'Abstract\\n\\n+.+?\\n\\n\\n+', text, flags=re.IGNORECASE)\n",
        "  if len(data):\n",
        "    return data[0].replace('Abstract\\n\\n', '')\n",
        "  else:\n",
        "    return '-1'\n",
        "  \n",
        "def extract_maintext(text):\n",
        "  try:\n",
        "    text = text.split('\\n\\n\\n')\n",
        "    for idx, t in enumerate(text):\n",
        "      if 'main text' in t.lower()[:100]:\n",
        "        break    \n",
        "    return ' '.join(text[idx:])\n",
        "  except:\n",
        "    return '-1'\n",
        "  # text = text.replace('\\n', '||')\n",
        "  # data = re.findall(r'Main text||||+.+', text, flags=re.MULTILINE)\n",
        "  # print (data)\n",
        "  # if len(data):\n",
        "  #   return data[0].replace('Main text||||', '')\n",
        "  # else:\n",
        "  #   return '-1'\n",
        "\n",
        "df['abstract']= df['full_text'].map(lambda x: extract_abstract(x)) # map runs every line in the full text i.e in x which is the entire content of the paper, pass it to extract_ function\n",
        "df['title']= df['full_text'].map(lambda x: extract_title(x)) # override the title to give the proper title name\n",
        "df['main_text']= df['full_text'].map(lambda x: extract_maintext(x)) #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "I7eZdX5Wjghd",
        "outputId": "09cb56c0-d5c8-4710-98ac-1df872894ba2"
      },
      "source": [
        "df = df[df['main_text']!='-1']\n",
        "df=df.reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>An effect assessment of Airborne particulate m...</td>\n",
              "      <td>This study aims to examine the effects of airb...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>1</td>\n",
              "      <td># Main text\\n\\n. It is confirmed that influenz...</td>\n",
              "      <td>Objective: Coronavirus disease 2019 (COVID-19)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aerosolized Particle Reduction: A Novel Cadave...</td>\n",
              "      <td>This study aimed to identify escape of small-p...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>2</td>\n",
              "      <td># Main text\\n\\nInstitutional review board appr...</td>\n",
              "      <td>OBJECTIVES: This study aimed to identify escap...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COVID-19 Prevention and Control Strategies for...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\n\\n# Main text\\n\\nSince December 2019, the ou...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Will COVID-19 pandemic diminish by summer-mons...</td>\n",
              "      <td>Therefore the study to investigate the meteoro...</td>\n",
              "      <td>11</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>21</td>\n",
              "      <td># Main text\\n\\nr=0.56) between temperature and...</td>\n",
              "      <td>The novel Coronavirus (2019-nCoV) was identifi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Clinical evidence based review and recommendat...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>3</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nIn the era of globalization, in...</td>\n",
              "      <td>BACKGROUND: Aerosol generating medical procedu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ...                                           abstract\n",
              "0  An effect assessment of Airborne particulate m...  ...  Objective: Coronavirus disease 2019 (COVID-19)...\n",
              "1  Aerosolized Particle Reduction: A Novel Cadave...  ...  OBJECTIVES: This study aimed to identify escap...\n",
              "2  COVID-19 Prevention and Control Strategies for...  ...                                                 -1\n",
              "3  Will COVID-19 pandemic diminish by summer-mons...  ...  The novel Coronavirus (2019-nCoV) was identifi...\n",
              "4  Clinical evidence based review and recommendat...  ...  BACKGROUND: Aerosol generating medical procedu...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLqhlv7a7T5_",
        "outputId": "f8578322-4b30-4be0-8523-7c5b523d2f90"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(490, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8lT3M0Zjuew"
      },
      "source": [
        "# SEGMENT COUNT DISTRIBUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "RuDzZjQkjtP8",
        "outputId": "a7700156-f97d-4600-e02f-63fd6b9f3c4a"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "seg_cnt_dict = dict(df['segment_count'].value_counts())\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.bar(seg_cnt_dict.keys(), seg_cnt_dict.values(), width=.5, color='g')\n",
        "plt.xticks(np.arange(min(seg_cnt_dict.keys()), max(seg_cnt_dict.keys())+1, 1.0))\n",
        "plt.title('Segment count distribution')\n",
        "plt.ylabel('segment count')\n",
        "plt.xlabel('number of segments')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2bYxC7Sl4Xo"
      },
      "source": [
        "# SENTENCE HIGHLIGHT DISTRIBUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJkwUc-NmBDO",
        "outputId": "90f9ec66-9a29-433d-e38c-2ad5070dc203"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def count_sentences(x):\n",
        "  return len(nltk.sent_tokenize(x))\n",
        "\n",
        "df['summary_sent_count'] = df['summary'].apply(lambda x: count_sentences(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "JZF9poUamais",
        "outputId": "0d4abeb1-21d8-43ab-a541-2ac842bfd998"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "sent_cnt_dict = dict(df['summary_sent_count'].value_counts())\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(sent_cnt_dict.keys(), sent_cnt_dict.values(), width=.5, color='g')\n",
        "# plt.xticks(sent_cnt_dict.keys())\n",
        "plt.xticks(np.arange(min(sent_cnt_dict.keys()), max(sent_cnt_dict.keys())+1, 1.0))\n",
        "plt.title('Sentance highlight distribution')\n",
        "plt.ylabel('highlight count')\n",
        "plt.xlabel('number of highlights')\n",
        "plt.show()\n",
        "\n",
        "print (f\"On average they highlight {math.ceil(df['summary_sent_count'].mean())} sentences\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "On average they highlight 4 sentences\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_HZ3oF_nnHe"
      },
      "source": [
        "#DOCUMENT LENGTH DISTRIBUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSKf-FLnepcP"
      },
      "source": [
        "def document_length_word_level(x):\n",
        "  return len(nltk.word_tokenize(x))\n",
        "\n",
        "df['main_text_len'] = df['main_text'].apply(lambda x: document_length_word_level(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLbbLdQAqakw",
        "outputId": "bf9c37c6-4510-4da0-c1b7-496ab0db3bcc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# main_text_w_cnt_dict = dict(df['main_text_len'].value_counts())\n",
        "# plt.figure(figsize=(20,5))\n",
        "# plt.bar(main_text_w_cnt_dict.keys(), main_text_w_cnt_dict.values(), width=.5, color='g')\n",
        "# plt.xticks(np.arange(min(main_text_w_cnt_dict.keys()), max(main_text_w_cnt_dict.keys())+1, 1.0))\n",
        "# plt.show()\n",
        "\n",
        "print (f\"On average document length is {math.ceil(df['main_text_len'].mean())} words\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "On average document length is 3335 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV09gov4vpEg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgydICM3o9Pq"
      },
      "source": [
        "# SEGMENT POPULARITY DISTRIBUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD4UDBBApTHo"
      },
      "source": [
        "# LEN_FULL_TEXT=[len(i) for i in df['main_text']] # the length of the full text for each document \n",
        "# DOC_AVG_LEN= sum(LEN_FULL_TEXT)/ len(LEN_FULL_TEXT) # the avrage document length at char level\n",
        "\n",
        "# sections_required= 4.0\n",
        "# segemnt_length= DOC_AVG_LEN/sections_required\n",
        "\n",
        "# print (segemnt_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0RQaEud7aR8"
      },
      "source": [
        "# SENTENCE LENGTH AT WORD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaGp4hJpt-Qq"
      },
      "source": [
        "def words_in_sent(x):\n",
        "  sentences = nltk.sent_tokenize(x)\n",
        "  t = [len(nltk.word_tokenize(sentence)) for sentence in sentences]\n",
        "  return sum(t)/len(t)\n",
        "\n",
        "df['words_per_sentence'] = df['main_text'].apply(lambda k: words_in_sent(k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ket4VbJ8JVv",
        "outputId": "0c830851-c824-445a-eaa2-3e726dc6216b"
      },
      "source": [
        "print ('Average words per sentence {}'.format(df['words_per_sentence'].mean()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average words per sentence 28.109357063261562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vA0utyj9dfn"
      },
      "source": [
        " # SENTENCES PER PAPER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxTWcG8N9cT6",
        "outputId": "dcca1df3-1888-4667-b3b3-6babaeaa24fc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "def sent_count(x):\n",
        "  sentences = nltk.sent_tokenize(x)\n",
        "  return len(sentences)\n",
        "\n",
        "df['sent_count'] = df['main_text'].apply(lambda k: sent_count(k))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8kq0el09u0o",
        "outputId": "b053ba55-cdef-4b54-9858-a19ad3e5b595"
      },
      "source": [
        "df['sent_count'].mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "126.21224489795918"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXFz5LvrO483"
      },
      "source": [
        "df.to_csv('889_final.csv', header=True, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rRoCKAgPHJZ"
      },
      "source": [
        "!mv 889_final.csv transformersum/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33hYe2NYQNsY"
      },
      "source": [
        "# Dice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u_WGQe0cO7s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9mBdTopcPJV"
      },
      "source": [
        "# bert/distill/roberta - 512\n",
        "\n",
        "# 3000 - avg\n",
        "# 512---\n",
        "\n",
        "16 -16 -16 -16 - paper length\n",
        "# BERT - BERT - BERT .. BERT\n",
        "# 3 - 3 - 3 - 3 -  3\n",
        "# \\               /\n",
        "#  \\             /\n",
        "#   \\ ----------/\n",
        "#         3\n",
        "#         |\n",
        "#      evaluate\n",
        "\n",
        "\n",
        "# sent1, sent2, sent3 [1, 0, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REJpMl9w3xkq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEF6cn8QquVY"
      },
      "source": [
        "#PEGASUS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6DuZbd6krBpP",
        "outputId": "4fe5404e-87b8-4f76-d266-34d1be661c5e"
      },
      "source": [
        "!pip install SentencePiece\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting SentencePiece\n",
            "  Downloading sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 11.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: SentencePiece\n",
            "Successfully installed SentencePiece-0.1.95\n",
            "Collecting transformers\n",
            "  Using cached transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "Collecting filelock\n",
            "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting packaging\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 4.7 MB/s \n",
            "\u001b[?25hCollecting dataclasses\n",
            "  Downloading dataclasses-0.8-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/site-packages (from transformers) (4.56.0)\n",
            "Collecting numpy>=1.17\n",
            "  Downloading numpy-1.19.5-cp36-cp36m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 163 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/site-packages (from transformers) (2.25.1)\n",
            "Collecting regex!=2019.12.17\n",
            "  Downloading regex-2020.11.13-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
            "\u001b[K     |████████████████████████████████| 723 kB 47.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.1-cp36-cp36m-manylinux2010_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 37.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Using cached sacremoses-0.0.43.tar.gz (883 kB)\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-3.7.0-py3-none-any.whl (11 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
            "Collecting typing-extensions>=3.6.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting pyparsing>=2.0.2\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests->transformers) (1.26.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Collecting click\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 969 kB/s \n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 47.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=e03bb0cb3478fad356f06ba0f18a059fac1e65c29bf24b0a93bc32276aa8fda4\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: zipp, typing-extensions, regex, pyparsing, joblib, click, tokenizers, sacremoses, packaging, numpy, importlib-metadata, filelock, dataclasses, transformers\n",
            "Successfully installed click-7.1.2 dataclasses-0.8 filelock-3.0.12 importlib-metadata-3.7.0 joblib-1.0.1 numpy-1.19.5 packaging-20.9 pyparsing-2.4.7 regex-2020.11.13 sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3 typing-extensions-3.7.4.3 zipp-3.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "joblib",
                  "numpy",
                  "pyparsing",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "jo_w083jrHQz",
        "outputId": "0520a9a9-023b-48c4-bf9a-8767dada4255"
      },
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n",
        "# src_text = [\n",
        "#     \"\"\" PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\"\"\n",
        "# ]\n",
        "\n",
        "model_name = 'google/pegasus-pubmed'\n",
        "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
        "model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DistributionNotFound",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDistributionNotFound\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/site-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mgot_ver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \"\"\"\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    771\u001b[0m                         \u001b[0mrequirers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequirers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m                 \u001b[0mto_activate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDistributionNotFound\u001b[0m: The 'sacremoses' distribution was not found and is required by the application",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDistributionNotFound\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-31a6c40bc244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPegasusForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPegasusTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# src_text = [\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     \"\"\" PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/site-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m from .file_utils import (\n\u001b[1;32m     45\u001b[0m     \u001b[0m_BaseLazyModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/site-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# not required, check version only if installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"can't find {pkg} in {deps.keys()}, check dependency_versions_table.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/site-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m\"\"\" require_version wrapper which emits a core-specific hint on failure \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mhint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git master\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/site-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mgot_ver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"this application\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# check that the right version is installed if version number was provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDistributionNotFound\u001b[0m: The 'sacremoses' distribution was not found and is required by this application, \nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git master"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwj8SigwygJR"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "new_df = pd.read_csv('889_A_data_dice_512_processed.csv')\n",
        "#new_df[['generated_summary_512_postprocess', 'summary_postprocess']]\n",
        "#print [0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10nC8PZMBra0"
      },
      "source": [
        "new_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OZbJFdeqvCI",
        "outputId": "1eb0e7a5-3a03-4a35-d344-a96b9cbcfef9"
      },
      "source": [
        "# import json\n",
        "\n",
        "\n",
        "\n",
        "# data = \"\"\"This study aims to examine the effects of airborne particulate matter (PM) pollution on COVID-19 across China. Dry and cold environment favors SARS-Cov-2 to survive and transmit in droplet or in the form of aerosol (Araujo and Naimi 2020) . Previous studies show that aerosol particles emitting from coughing by influenza patients contain high level of influenza virus and these particles are within the respirable size range . These aerosols having virus are easy to transmit among individuals. It is reported that aerosols from highly virulent pathogens like severe acute respiratory syndrome-coronavirus (SARS-CoV) could travel more than six feet (Kutter et al. 2018) . Also, these aerosols are likely composed of airborne pollution particles and attached virus droplets, which promote the spread of pathogen like influenza viruses (Mori et al. 2017 ). An ecologic analysis found that there were positive relationship between PM 2.5 concentration and influenza-like-illness risk in\"\"\"\n",
        "# library = [('dummy', data)]\n",
        "\n",
        "# for item in library:\n",
        "#     (title, abstract) = item\n",
        "#     inputs = tokenizer([abstract], max_length=512, return_tensors='pt')\n",
        "    \n",
        "#     summary_ids = model.generate(inputs['input_ids'].cuda(),  max_length=256, top_k=10, temperature=0.2, top_p=0.95)\n",
        "    \n",
        "#     response=[tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
        "#     print (response)\n",
        "#     break  \n",
        "#     #response = openai.Completion.create(engine=\"curie\", prompt=p, max_tokens=100, temperature = 0.2)\n",
        "#     #clean up response for printing to screen\n",
        "#     responselist = json.dumps(response)\n",
        "#     print(response)\n",
        "#     #data = json.loads(responselist)\n",
        "#     #model list comprehension using this framework new_list = [i[\"a\"] for i in t]\n",
        "#     #new_data=[i[\"text\"] for i in data['choices']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['the aim of this study was to examine the effects of airborne particulate matter ( aerosol matter ) pollution on cold environment and transmit influenza virus . <n> the study was carried out in the university of sarajevo , bosnia and herzegovina . <n> it was conducted in the department of civil engineering , university of sarajevo , bosnia and herzegovina . <n> the study was approved by the institutional review board of the university of sarajevo , bosnia and herzegovina and the institutional review board of the university of sarajevo , bosnia and herzegovina .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBJpujfe0COJ"
      },
      "source": [
        "def dice_similarity (list1, list2):\n",
        "  numer= len(list(set(list1).intersection(list2)))\n",
        "  deno=len(list1)+len(list2)\n",
        "  return 2*float(numer)/ deno "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhxCZlAGuauM",
        "outputId": "2bae92a3-24c8-47a4-e3e3-cb5341659235"
      },
      "source": [
        "from tqdm import tqdm \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "dice_sim, summaries = [], []\n",
        "for item in tqdm(range(new_df.shape[0])):\n",
        "  gt_truth = new_df.iloc[item]['abstract']\n",
        "  gen_truth = new_df.iloc[item]['generated_summary_512_postprocess']\n",
        "  inputs = tokenizer([gen_truth], max_length=512, return_tensors='pt')\n",
        "  summary_ids = model.generate(inputs['input_ids'].cuda(),  max_length=256, top_k=10, top_p=0.95)\n",
        "  response=[tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n",
        "  dice_sim.append(dice_similarity(nltk.word_tokenize(response[0].lower()), nltk.word_tokenize(gt_truth.lower())))\n",
        "  summaries.append(response[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/490 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 490/490 [41:02<00:00,  5.03s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "3GGC7MvC1Thg",
        "outputId": "48dfc9d3-bdd6-453d-f8ee-310082d9e9f9"
      },
      "source": [
        "new_df['pegasus_inp512_token_model'] = summaries\n",
        "new_df['dice_pegasus_inp512_token_model'] = dice_sim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-4e38869dfc99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pegasus_inp512_token_model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummaries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dice_pegasus_inp512_token_model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdice_sim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'summaries' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "0bCnqAJ122vw",
        "outputId": "4bc9dc63-7afb-4693-a5fb-1ecbeb9f915e"
      },
      "source": [
        "new_df['dice_pegasus_inp512_token_model'].mean()*100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-74fc117011d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dice_pegasus_inp512_token_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "TdbywfNGzQuu",
        "outputId": "422dff98-8d23-40f7-8060-3a42583408b6"
      },
      "source": [
        "!pip install Rouge\n",
        "from rouge import Rouge \n",
        "rouge = Rouge()\n",
        "\n",
        "def get_rouge(h, r):\n",
        "  return rouge.get_scores(h, r)\n",
        "\n",
        "rouge_scores=[]\n",
        "for i in range(new_df.shape[0]):\n",
        "  gen=new_df.iloc[i]['pegasus_inp512_token_model']\n",
        "  summ=new_df.iloc[i]['summary_postprocess']\n",
        "  rouge_scores.append(get_rouge(gen, summ))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Rouge\n",
            "  Using cached rouge-1.0.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from Rouge) (1.15.0)\n",
            "Installing collected packages: Rouge\n",
            "Successfully installed Rouge-1.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-518afdacf41e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrouge_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mgen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pegasus_inp512_token_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0msumm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary_postprocess'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'new_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nsovzi42LTM"
      },
      "source": [
        "r1, r2, rL= [], [], []\n",
        "for i in rouge_scores:\n",
        "  r1.append(i[0]['rouge-1']['f'])\n",
        "  r2.append(i[0]['rouge-2']['f'])\n",
        "  rL.append(i[0]['rouge-l']['f'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdJ_tBzoro2u"
      },
      "source": [
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xoapi1RD26Rg",
        "outputId": "10a3df66-0137-4908-825e-0fed5f22e23e"
      },
      "source": [
        "np.mean(r1), np.mean(r2), np.mean(rL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.15503979884887764, 0.03028320383773577, 0.15808410608076648)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "ONSuF53g3nBa",
        "outputId": "d18781e2-3a32-4c45-9f12-092b68029a77"
      },
      "source": [
        "new_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>summary</th>\n",
              "      <th>segment_count</th>\n",
              "      <th>full_text</th>\n",
              "      <th>all_sentences</th>\n",
              "      <th>main_text</th>\n",
              "      <th>abstract</th>\n",
              "      <th>summary_sent_count</th>\n",
              "      <th>main_text_len</th>\n",
              "      <th>words_per_sentence</th>\n",
              "      <th>sent_count</th>\n",
              "      <th>dice_sim_512</th>\n",
              "      <th>generated_summary_512</th>\n",
              "      <th>generated_summary_512_postprocess</th>\n",
              "      <th>summary_postprocess</th>\n",
              "      <th>dice_A_sim_512_postprocess.csv</th>\n",
              "      <th>pegasus_inp512_token_model</th>\n",
              "      <th>dice_pegasus_inp512_token_model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>An effect assessment of Airborne particulate m...</td>\n",
              "      <td>This study aims to examine the effects of airb...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>1</td>\n",
              "      <td># Main text\\n\\n. It is confirmed that influenz...</td>\n",
              "      <td>Objective: Coronavirus disease 2019 (COVID-19)...</td>\n",
              "      <td>1</td>\n",
              "      <td>2566</td>\n",
              "      <td>18.729927</td>\n",
              "      <td>137</td>\n",
              "      <td>0.175439</td>\n",
              "      <td>Methods : In this study , we obtained confirme...</td>\n",
              "      <td>Methods : In this study , we obtained confirme...</td>\n",
              "      <td>This study aims to examine the effects of airb...</td>\n",
              "      <td>0.175439</td>\n",
              "      <td>background : cardiovascular diseases ( cvds ) ...</td>\n",
              "      <td>0.102564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aerosolized Particle Reduction: A Novel Cadave...</td>\n",
              "      <td>This study aimed to identify escape of small-p...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>2</td>\n",
              "      <td># Main text\\n\\nInstitutional review board appr...</td>\n",
              "      <td>OBJECTIVES: This study aimed to identify escap...</td>\n",
              "      <td>2</td>\n",
              "      <td>1727</td>\n",
              "      <td>28.783333</td>\n",
              "      <td>60</td>\n",
              "      <td>0.343195</td>\n",
              "      <td>This study also aimed to evaluate the efficacy...</td>\n",
              "      <td>This study also aimed to evaluate the efficacy...</td>\n",
              "      <td>This study aimed to identify escape of small-p...</td>\n",
              "      <td>0.343195</td>\n",
              "      <td>abstractthe aim of this study was to evaluate ...</td>\n",
              "      <td>0.320513</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COVID-19 Prevention and Control Strategies for...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>1</td>\n",
              "      <td># URL to online version\\n\\nhttps://www.ncbi.nl...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\n\\n# Main text\\n\\nSince December 2019, the ou...</td>\n",
              "      <td>-1</td>\n",
              "      <td>3</td>\n",
              "      <td>1058</td>\n",
              "      <td>27.842105</td>\n",
              "      <td>38</td>\n",
              "      <td>0.177778</td>\n",
              "      <td>-1 \\n\\n # Main text \\n\\n Since December 2019 ,...</td>\n",
              "      <td>-1 Since December 2019 , the outbreak of a res...</td>\n",
              "      <td>In summary, staff in psychiatric hospitals are...</td>\n",
              "      <td>0.180791</td>\n",
              "      <td>an outbreak of severe acute respiratory syndro...</td>\n",
              "      <td>0.081505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Will COVID-19 pandemic diminish by summer-mons...</td>\n",
              "      <td>Therefore the study to investigate the meteoro...</td>\n",
              "      <td>11</td>\n",
              "      <td># URL to online version\\n\\nhttp://medrxiv.org/...</td>\n",
              "      <td>21</td>\n",
              "      <td># Main text\\n\\nr=0.56) between temperature and...</td>\n",
              "      <td>The novel Coronavirus (2019-nCoV) was identifi...</td>\n",
              "      <td>21</td>\n",
              "      <td>6634</td>\n",
              "      <td>25.713178</td>\n",
              "      <td>258</td>\n",
              "      <td>0.138562</td>\n",
              "      <td>We have investigated the effect of meteorologi...</td>\n",
              "      <td>We have investigated the effect of meteorologi...</td>\n",
              "      <td>Therefore the study to investigate the meteoro...</td>\n",
              "      <td>0.138562</td>\n",
              "      <td>abstractthe aim of this study was to investiga...</td>\n",
              "      <td>0.074941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Clinical evidence based review and recommendat...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>3</td>\n",
              "      <td># URL to online version\\n\\nhttps://doi.org/10....</td>\n",
              "      <td>3</td>\n",
              "      <td># Main text\\n\\nIn the era of globalization, in...</td>\n",
              "      <td>BACKGROUND: Aerosol generating medical procedu...</td>\n",
              "      <td>3</td>\n",
              "      <td>4137</td>\n",
              "      <td>23.913295</td>\n",
              "      <td>173</td>\n",
              "      <td>0.309179</td>\n",
              "      <td>The aim of this literature review was to ident...</td>\n",
              "      <td>The aim of this literature review was to ident...</td>\n",
              "      <td>Direct evidence indicates that CO2 laser ablat...</td>\n",
              "      <td>0.309179</td>\n",
              "      <td>pulmonary tuberculosis ( tb ) is an important ...</td>\n",
              "      <td>0.095238</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  ... dice_pegasus_inp512_token_model\n",
              "0  An effect assessment of Airborne particulate m...  ...                        0.102564\n",
              "1  Aerosolized Particle Reduction: A Novel Cadave...  ...                        0.320513\n",
              "2  COVID-19 Prevention and Control Strategies for...  ...                        0.081505\n",
              "3  Will COVID-19 pandemic diminish by summer-mons...  ...                        0.074941\n",
              "4  Clinical evidence based review and recommendat...  ...                        0.095238\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "5nGEO_AW45QO",
        "outputId": "e2baff2e-2b38-42fd-e11e-3bcce49126b6"
      },
      "source": [
        "new_df[['dice_sim_gt_512_postprocessed', 'summary_postprocess']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-69a1cc9be3ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dice_sim_gt_512_postprocessed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'summary_postprocess'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1302\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;31m# we skip the warning on Categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['dice_sim_gt_512_postprocessed'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suVHXTxtsNda"
      },
      "source": [
        "new_df.to_csv('pegasus_inp512_token_model.csv', header=True, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvZdw5as5UAl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}